{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training DCN model on Taobao CTR dataset","provenance":[{"file_id":"1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a","timestamp":1640843499957}],"collapsed_sections":[],"mount_file_id":"1ItuHwiTRaMzZhLqxWFfWXO-il6FnF_sI","authorship_tag":"ABX9TyM1qjs0kaNncVXOSvrUnmwV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"90c8d6ca4eab4a18826d9f17e6c0636a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0ec7f324b42548408e92d5e46941457a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b035f3678cc1405fbd6f7564cff1ed11","IPY_MODEL_7bd2575559c3495eb007f0f0cb977fe0","IPY_MODEL_54e7a8030d7249f5b17f49c5f8a3fbbc"]}},"0ec7f324b42548408e92d5e46941457a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"b035f3678cc1405fbd6f7564cff1ed11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e872ca6772ac425d93cac83b155c5fc7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch 4: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f8eab32990684d6d958ab7d0d33245be"}},"7bd2575559c3495eb007f0f0cb977fe0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9053c6b2a56b4cd186e75fc9b3c5062d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f3374782cd14df7af446143297b001f"}},"54e7a8030d7249f5b17f49c5f8a3fbbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d32f1e4393d9438a9945bd212b171116","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/4 [00:03&lt;00:00,  1.24it/s, loss=0.432, v_num=0, Val Metrics={&#x27;logloss&#x27;: 0.16331680119037628, &#x27;AUC&#x27;: 1.0}]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b1af7ef3e7fa4319bbdf0df5edd916aa"}},"e872ca6772ac425d93cac83b155c5fc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f8eab32990684d6d958ab7d0d33245be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9053c6b2a56b4cd186e75fc9b3c5062d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4f3374782cd14df7af446143297b001f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d32f1e4393d9438a9945bd212b171116":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b1af7ef3e7fa4319bbdf0df5edd916aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"399ba0f7457b40049d8799902fe84a91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bbf347d0912447aa82a179a4afb84fe1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ab51107fe2c2487296db1c604995a759","IPY_MODEL_0066aea305524b8ab1e49e75ea706bc3","IPY_MODEL_da32380ac57e472cb9e23be86d72ef5d"]}},"bbf347d0912447aa82a179a4afb84fe1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"ab51107fe2c2487296db1c604995a759":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0b15b2a71dc49d9ad3f33866efb64c9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Validating:   0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef5473426dbb4d508d6c98fef517d5ab"}},"0066aea305524b8ab1e49e75ea706bc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f7d4c60a11f141bd99c2be2477f7545d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b00a382d630a47bd99d43f9aa522061c"}},"da32380ac57e472cb9e23be86d72ef5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_21d0bf55dbc6452d8a8941a729f36901","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/2 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6f91cad0cbd446b98b75d7eab4410549"}},"b0b15b2a71dc49d9ad3f33866efb64c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef5473426dbb4d508d6c98fef517d5ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7d4c60a11f141bd99c2be2477f7545d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b00a382d630a47bd99d43f9aa522061c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"21d0bf55dbc6452d8a8941a729f36901":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6f91cad0cbd446b98b75d7eab4410549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae0c3dbb7dca40428364891d2254562c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a916ae44a904a7898a052075fbcf171","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9803d4cbd47141eaa472448302796be4","IPY_MODEL_a120902b27c04d88a3e93e43eb418f92","IPY_MODEL_d8ba096c094142568e5685850227ca01"]}},"7a916ae44a904a7898a052075fbcf171":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"9803d4cbd47141eaa472448302796be4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f4dcc40365c7488e8a0e3e4d3aa06826","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Testing: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5901fa87e7b24fefb813c0b4657a1786"}},"a120902b27c04d88a3e93e43eb418f92":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bebf2c2da8cf48eea3a72820045f0937","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0a77af7b06d4842bde904047902b68c"}},"d8ba096c094142568e5685850227ca01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a350823ebf0940c9a217e25ac9e2e96d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00, 25.79it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14da0a7882d44739a9632db85ed19385"}},"f4dcc40365c7488e8a0e3e4d3aa06826":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5901fa87e7b24fefb813c0b4657a1786":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bebf2c2da8cf48eea3a72820045f0937":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b0a77af7b06d4842bde904047902b68c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a350823ebf0940c9a217e25ac9e2e96d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14da0a7882d44739a9632db85ed19385":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","source":["!wget -q --show-progress https://github.com/RecoHut-Stanzas/S516304/raw/main/data/tiny_data/train_sample.csv\n","!wget -q --show-progress https://github.com/RecoHut-Stanzas/S516304/raw/main/data/tiny_data/valid_sample.csv\n","!wget -q --show-progress https://github.com/RecoHut-Stanzas/S516304/raw/main/data/tiny_data/test_sample.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BlhAa6WyUZKj","executionInfo":{"status":"ok","timestamp":1641897135496,"user_tz":-330,"elapsed":1303,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d2f6f6b8-60a3-4969-f96e-74d392909db3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_sample.csv    100%[===================>]   9.31K  --.-KB/s    in 0s      \n","valid_sample.csv    100%[===================>]   9.31K  --.-KB/s    in 0s      \n","test_sample.csv     100%[===================>]   9.31K  --.-KB/s    in 0s      \n"]}]},{"cell_type":"code","source":["!pip install -q pytorch-lightning\n","\n","!git clone --branch US632593 https://github.com/RecoHut-Projects/recohut.git\n","!pip install -U ./recohut"],"metadata":{"id":"-BVin3TuxI9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DihrHtzg7SP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","import numpy as np\n","import pandas as pd\n","import h5py\n","import six\n","import pickle\n","import sklearn.preprocessing as sklearn_preprocess\n","from collections import Counter, OrderedDict, defaultdict\n","import io\n","import os\n","import logging\n","import json\n","from datetime import datetime, date\n","\n","import torch\n","\n","from pytorch_lightning import LightningDataModule\n","\n","from recohut.datasets.bases.common import Dataset as BaseDataset\n","from recohut.utils.common_utils import download_url"],"metadata":{"id":"fJptZHOChOrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Tokenizer(object):\n","    def __init__(self, topk_words=None, na_value=None, min_freq=1, splitter=None, \n","                 lower=False, oov_token=0, max_len=0, padding_type=\"pre\"):\n","        self._topk_words = topk_words\n","        self._na_value = na_value\n","        self._min_freq = min_freq\n","        self._lower = lower\n","        self._splitter = splitter\n","        self.oov_token = oov_token # use 0 for __OOV__\n","        self.word_counts = Counter()\n","        self.vocab = dict()\n","        self.vocab_size = 0 # include oov and padding\n","        self.max_len = max_len\n","        self.padding_type = padding_type\n","\n","    def fit_on_texts(self, texts, use_padding=True):\n","        tokens = list(texts)\n","        if self._splitter is not None: # for sequence\n","            text_splits = [text.split(self._splitter) for text in texts if not pd.isnull(text)]\n","            if self.max_len == 0:\n","                self.max_len = max(len(x) for x in text_splits)\n","            tokens = list(itertools.chain(*text_splits))\n","        if self._lower:\n","            tokens = [tk.lower() for tk in tokens]\n","        if self._na_value is not None:\n","            tokens = [tk for tk in tokens if tk != self._na_value]\n","        self.word_counts = Counter(tokens)\n","        words = [token for token, count in self.word_counts.most_common() if count >= self._min_freq]\n","        self.word_counts.clear() # empty the dict to save memory\n","        if self._topk_words:\n","            words = words[0:self._topk_words]\n","        self.vocab = dict((token, idx) for idx, token in enumerate(words, 1 + self.oov_token))\n","        self.vocab[\"__OOV__\"] = self.oov_token\n","        if use_padding:\n","            self.vocab[\"__PAD__\"] = len(words) + self.oov_token + 1 # use the last index for __PAD__\n","        self.vocab_size = len(self.vocab) + self.oov_token\n","\n","    def encode_category(self, categories):\n","        category_indices = [self.vocab.get(x, self.oov_token) for x in categories]\n","        return np.array(category_indices)\n","\n","    def encode_sequence(self, texts):\n","        sequence_list = []\n","        for text in texts:\n","            if pd.isnull(text) or text == '':\n","                sequence_list.append([])\n","            else:\n","                sequence_list.append([self.vocab.get(x, self.oov_token) for x in text.split(self._splitter)])\n","        sequence_list = self.padding(sequence_list, maxlen=self.max_len, value=self.vocab_size - 1,\n","                                padding=self.padding_type, truncating=self.padding_type)\n","        return np.array(sequence_list)\n","    \n","    def load_pretrained_embedding(self, feature_name, pretrain_path, embedding_dim, output_path):\n","        with h5py.File(pretrain_path, 'r') as hf:\n","            keys = hf[\"key\"][:]\n","            pretrained_vocab = dict(zip(keys, range(len(keys))))\n","            pretrained_emb = hf[\"value\"][:]\n","        embedding_matrix = np.random.normal(loc=0, scale=1.e-4, size=(self.vocab_size, embedding_dim))\n","        for word, idx in self.vocab.items():\n","            if word in pretrained_vocab:\n","                embedding_matrix[idx] = pretrained_emb[pretrained_vocab[word]]\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with h5py.File(output_path, 'a') as hf:\n","            hf.create_dataset(feature_name, data=embedding_matrix)\n","\n","    def set_vocab(self, vocab):\n","        self.vocab = vocab\n","        self.vocab_size = len(self.vocab) + self.oov_token\n","\n","    @staticmethod\n","    def padding(sequences, maxlen=None, dtype='int32',\n","                padding='pre', truncating='pre', value=0.):\n","        \"\"\" Pads sequences (list of list) to the ndarray of same length \"\"\"\n","        assert padding in [\"pre\", \"post\"], \"Invalid padding={}.\".format(padding)\n","        assert truncating in [\"pre\", \"post\"], \"Invalid truncating={}.\".format(truncating)\n","        \n","        if maxlen is None:\n","            maxlen = max(len(x) for x in sequences)\n","        arr = np.full((len(sequences), maxlen), value, dtype=dtype)\n","        for idx, x in enumerate(sequences):\n","            if len(x) == 0:\n","                continue  # empty list\n","            if truncating == 'pre':\n","                trunc = x[-maxlen:]\n","            else:\n","                trunc = x[:maxlen]\n","            trunc = np.asarray(trunc, dtype=dtype)\n","\n","            if padding == 'pre':\n","                arr[idx, -len(trunc):] = trunc\n","            else:\n","                arr[idx, :len(trunc)] = trunc\n","        return arr"],"metadata":{"id":"Ea_20yl9U1dH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Normalizer(object):\n","    def __init__(self, normalizer):\n","        if not callable(normalizer):\n","            self.callable = False\n","            if normalizer in ['StandardScaler', 'MinMaxScaler']:\n","                self.normalizer = getattr(sklearn_preprocess, normalizer)()\n","            else:\n","                raise NotImplementedError('normalizer={}'.format(normalizer))\n","        else:\n","            # normalizer is a method\n","            self.normalizer = normalizer\n","            self.callable = True\n","\n","    def fit(self, X):\n","        if not self.callable:\n","            null_index = np.isnan(X)\n","            self.normalizer.fit(X[~null_index].reshape(-1, 1))\n","\n","    def normalize(self, X):\n","        if self.callable:\n","            return self.normalizer(X)\n","        else:\n","            return self.normalizer.transform(X.reshape(-1, 1)).flatten()"],"metadata":{"id":"xXhv2rEkcbb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeatureMap(object):\n","    def __init__(self, dataset_id='ctr'):\n","        self.dataset_id = dataset_id\n","        self.num_fields = 0\n","        self.num_features = 0\n","        self.feature_len = 0\n","        self.feature_specs = OrderedDict()\n","        \n","    def set_feature_index(self):\n","        logging.info(\"Set feature index...\")\n","        idx = 0\n","        for feature, feature_spec in self.feature_specs.items():\n","            if feature_spec[\"type\"] != \"sequence\":\n","                self.feature_specs[feature][\"index\"] = idx\n","                idx += 1\n","            else:\n","                seq_indexes = [i + idx for i in range(feature_spec[\"max_len\"])]\n","                self.feature_specs[feature][\"index\"] = seq_indexes\n","                idx += feature_spec[\"max_len\"]\n","        self.feature_len = idx\n","\n","    def get_feature_index(self, feature_type=None):\n","        feature_indexes = []\n","        if feature_type is not None:\n","            if not isinstance(feature_type, list):\n","                feature_type = [feature_type]\n","            feature_indexes = [feature_spec[\"index\"] for feature, feature_spec in self.feature_specs.items()\n","                               if feature_spec[\"type\"] in feature_type]\n","        return feature_indexes\n","\n","    def load(self, json_file):\n","        logging.info(\"Load feature_map from json: \" + json_file)\n","        with io.open(json_file, \"r\", encoding=\"utf-8\") as fd:\n","            feature_map = json.load(fd, object_pairs_hook=OrderedDict)\n","        if feature_map[\"dataset_id\"] != self.dataset_id:\n","            raise RuntimeError(\"dataset_id={} does not match to feature_map!\".format(self.dataset_id))\n","        self.num_fields = feature_map[\"num_fields\"]\n","        self.num_features = feature_map.get(\"num_features\", None)\n","        self.feature_len = feature_map.get(\"feature_len\", None)\n","        self.feature_specs = OrderedDict(feature_map[\"feature_specs\"])\n","\n","    def save(self, json_file):\n","        logging.info(\"Save feature_map to json: \" + json_file)\n","        if not os.path.exists(os.path.dirname(json_file)):\n","            os.makedirs(os.path.dirname(json_file))\n","        feature_map = OrderedDict()\n","        feature_map[\"dataset_id\"] = self.dataset_id\n","        feature_map[\"num_fields\"] = self.num_fields\n","        feature_map[\"num_features\"] = self.num_features\n","        feature_map[\"feature_len\"] = self.feature_len\n","        feature_map[\"feature_specs\"] = self.feature_specs\n","        with open(json_file, \"w\") as fd:\n","            json.dump(feature_map, fd, indent=4)"],"metadata":{"id":"8W9veKeAUwBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CTRDataset(torch.utils.data.Dataset, BaseDataset):\n","    def __init__(self,\n","                 data_dir,\n","                 data_type=None,\n","                 *args,\n","                 **kwargs):\n","        super().__init__(data_dir)\n","        self.data_type = data_type\n","        self.pickle_file = os.path.join(self.processed_dir, \"feature_encoder.pkl\")\n","        self.json_file = os.path.join(self.processed_dir, \"feature_map.json\")\n","        self.feature_cols = self._complete_feature_cols(self.feature_cols)\n","        self.feature_map = FeatureMap()\n","        self.encoders = dict()\n","\n","        if self.data_type == 'train':\n","            self.darray =  self.load_data(self.raw_paths[0])\n","            self.num_samples = len(self.darray)\n","        elif self.data_type == 'valid':\n","            self.darray = self.load_data(self.raw_paths[1])\n","            self.validation_samples = len(self.darray)\n","        elif self.data_type == 'test':\n","            self.darray = self.load_data(self.raw_paths[2])\n","            self.test_samples = len(self.darray)\n","        elif self.data_type is None:\n","            self._process()\n","\n","    @property\n","    def raw_file_names(self):\n","        raise NotImplementedError\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['feature_encoder.pkl',\n","                'feature_map.json',\n","                'train_sample.h5',\n","                'valid_sample.h5',\n","                'test_sample.h5']\n","\n","    def download(self):\n","        raise NotImplementedError\n","\n","    def process(self):\n","        self.fit(self.raw_paths[0])\n","\n","    @staticmethod\n","    def _complete_feature_cols(feature_cols):\n","        full_feature_cols = []\n","        for col in feature_cols:\n","            name_or_namelist = col[\"name\"]\n","            if isinstance(name_or_namelist, list):\n","                for _name in name_or_namelist:\n","                    _col = col.copy()\n","                    _col[\"name\"] = _name\n","                    full_feature_cols.append(_col)\n","            else:\n","                full_feature_cols.append(col)\n","        return full_feature_cols\n","\n","    def read_csv(self, data_path):\n","        all_cols = self.feature_cols + [self.label_col]\n","        dtype_dict = dict((x[\"name\"], eval(x[\"dtype\"]) if isinstance(x[\"dtype\"], str) else x[\"dtype\"]) \n","                          for x in all_cols)\n","        ddf = pd.read_csv(data_path, dtype=dtype_dict, memory_map=True) \n","        return ddf\n","\n","    def _preprocess(self, ddf):\n","        all_cols = [self.label_col] + self.feature_cols[::-1]\n","        for col in all_cols:\n","            name = col[\"name\"]\n","            if name in ddf.columns and ddf[name].isnull().values.any():\n","                ddf[name] = self._fill_na(col, ddf[name])\n","            if \"preprocess\" in col and col[\"preprocess\"] != \"\":\n","                preprocess_fn = getattr(self, col[\"preprocess\"])\n","                ddf[name] = preprocess_fn(ddf, name)\n","        active_cols = [self.label_col[\"name\"]] + [col[\"name\"] for col in self.feature_cols if col[\"active\"]]\n","        ddf = ddf.loc[:, active_cols]\n","        return ddf\n","\n","    def _fill_na(self, col, series):\n","        na_value = col.get(\"na_value\")\n","        if na_value is not None:\n","            return series.fillna(na_value)\n","        elif col[\"dtype\"] == \"str\":\n","            return series.fillna(\"\")\n","        else:\n","            raise RuntimeError(\"Feature column={} requires to assign na_value!\".format(col[\"name\"]))\n","\n","    def fit(self, train_data, min_categr_count=1, num_buckets=10):           \n","        ddf = self.read_csv(train_data)\n","        ddf = self._preprocess(ddf)\n","        self.feature_map.num_fields = 0\n","        for col in self.feature_cols:\n","            if col[\"active\"]:\n","                name = col[\"name\"]\n","                self.fit_feature_col(col, ddf, \n","                                     min_categr_count=min_categr_count,\n","                                     num_buckets=num_buckets)\n","                self.feature_map.num_fields += 1\n","        self.feature_map.set_feature_index()\n","        self.save_pickle(self.pickle_file)\n","        self.feature_map.save(self.json_file)\n","        \n","    def fit_feature_col(self, feature_column, ddf, min_categr_count=1, num_buckets=10):\n","        name = feature_column[\"name\"]\n","        feature_type = feature_column[\"type\"]\n","        feature_source = feature_column.get(\"source\", \"\")\n","        self.feature_map.feature_specs[name] = {\"source\": feature_source,\n","                                                \"type\": feature_type}\n","        if \"min_categr_count\" in feature_column:\n","            min_categr_count = feature_column[\"min_categr_count\"]\n","        self.feature_map.feature_specs[name][\"min_categr_count\"] = min_categr_count\n","        if \"embedding_dim\" in feature_column:\n","            self.feature_map.feature_specs[name][\"embedding_dim\"] = feature_column[\"embedding_dim\"]\n","        feature_values = ddf[name].values\n","        if feature_type == \"numeric\":\n","            normalizer_name = feature_column.get(\"normalizer\", None)\n","            if normalizer_name is not None:\n","                normalizer = Normalizer(normalizer_name)\n","                normalizer.fit(feature_values)\n","                self.encoders[name + \"_normalizer\"] = normalizer\n","            self.feature_map.num_features += 1\n","        elif feature_type == \"categorical\":\n","            encoder = feature_column.get(\"encoder\", \"\")\n","            if encoder != \"\":\n","                self.feature_map.feature_specs[name][\"encoder\"] = encoder\n","            if encoder == \"\":\n","                tokenizer = Tokenizer(min_freq=min_categr_count, \n","                                      na_value=feature_column.get(\"na_value\", \"\"))\n","                if \"share_embedding\" in feature_column:\n","                    self.feature_map.feature_specs[name][\"share_embedding\"] = feature_column[\"share_embedding\"]\n","                    tokenizer.set_vocab(self.encoders[\"{}_tokenizer\".format(feature_column[\"share_embedding\"])].vocab)\n","                else:\n","                    if self.is_share_embedding_with_sequence(name):\n","                        tokenizer.fit_on_texts(feature_values, use_padding=True)\n","                        self.feature_map.feature_specs[name][\"padding_idx\"] = tokenizer.vocab_size - 1\n","                    else:\n","                        tokenizer.fit_on_texts(feature_values, use_padding=False)\n","                self.encoders[name + \"_tokenizer\"] = tokenizer\n","                self.feature_map.num_features += tokenizer.vocab_size\n","                self.feature_map.feature_specs[name][\"vocab_size\"] = tokenizer.vocab_size\n","                if \"pretrained_emb\" in feature_column:\n","                    self.feature_map.feature_specs[name][\"pretrained_emb\"] = \"pretrained_embedding.h5\"\n","                    self.feature_map.feature_specs[name][\"freeze_emb\"] = feature_column.get(\"freeze_emb\", True)\n","                    tokenizer.load_pretrained_embedding(name,\n","                                                        feature_column[\"pretrained_emb\"], \n","                                                        feature_column[\"embedding_dim\"],\n","                                                        os.path.join(self.processed_dir, \"pretrained_embedding.h5\"))\n","            elif encoder == \"numeric_bucket\":\n","                num_buckets = feature_column.get(\"num_buckets\", num_buckets)\n","                qtf = sklearn_preprocess.QuantileTransformer(n_quantiles=num_buckets + 1)\n","                qtf.fit(feature_values)\n","                boundaries = qtf.quantiles_[1:-1]\n","                self.feature_map.feature_specs[name][\"vocab_size\"] = num_buckets\n","                self.feature_map.num_features += num_buckets\n","                self.encoders[name + \"_boundaries\"] = boundaries\n","            elif encoder == \"hash_bucket\":\n","                num_buckets = feature_column.get(\"num_buckets\", num_buckets)\n","                uniques = Counter(feature_values)\n","                num_buckets = min(num_buckets, len(uniques))\n","                self.feature_map.feature_specs[name][\"vocab_size\"] = num_buckets\n","                self.feature_map.num_features += num_buckets\n","                self.encoders[name + \"_num_buckets\"] = num_buckets\n","        elif feature_type == \"sequence\":\n","            encoder = feature_column.get(\"encoder\", \"MaskedAveragePooling\")\n","            splitter = feature_column.get(\"splitter\", \" \")\n","            na_value = feature_column.get(\"na_value\", \"\")\n","            max_len = feature_column.get(\"max_len\", 0)\n","            padding = feature_column.get(\"padding\", \"post\")\n","            tokenizer = Tokenizer(min_freq=min_categr_count, splitter=splitter, \n","                                  na_value=na_value, max_len=max_len, padding=padding)\n","            if \"share_embedding\" in feature_column:\n","                self.feature_map.feature_specs[name][\"share_embedding\"] = feature_column[\"share_embedding\"]\n","                tokenizer.set_vocab(self.encoders[\"{}_tokenizer\".format(feature_column[\"share_embedding\"])].vocab)\n","            else:\n","                tokenizer.fit_on_texts(feature_values, use_padding=True)\n","            self.encoders[name + \"_tokenizer\"] = tokenizer\n","            self.feature_map.num_features += tokenizer.vocab_size\n","            self.feature_map.feature_specs[name].update({\"encoder\": encoder,\n","                                                         \"padding_idx\": tokenizer.vocab_size - 1,\n","                                                         \"vocab_size\": tokenizer.vocab_size,\n","                                                         \"max_len\": tokenizer.max_len})\n","            if \"pretrained_emb\" in feature_column:\n","                self.feature_map.feature_specs[name][\"pretrained_emb\"] = \"pretrained_embedding.h5\"\n","                self.feature_map.feature_specs[name][\"freeze_emb\"] = feature_column.get(\"freeze_emb\", True)\n","                tokenizer.load_pretrained_embedding(name,\n","                                                    feature_column[\"pretrained_emb\"], \n","                                                    feature_column[\"embedding_dim\"],\n","                                                    os.path.join(self.processed_dir, \"pretrained_embedding.h5\"))\n","        else:\n","            raise NotImplementedError(\"feature_col={}\".format(feature_column))\n","\n","    def transform(self, ddf):\n","        ddf = self._preprocess(ddf)\n","        data_arrays = []\n","        for feature, feature_spec in self.feature_map.feature_specs.items():\n","            feature_type = feature_spec[\"type\"]\n","            if feature_type == \"numeric\":\n","                numeric_array = ddf.loc[:, feature].fillna(0).apply(lambda x: float(x)).values\n","                normalizer = self.encoders.get(feature + \"_normalizer\")\n","                if normalizer:\n","                     numeric_array = normalizer.normalize(numeric_array)\n","                data_arrays.append(numeric_array) \n","            elif feature_type == \"categorical\":\n","                encoder = feature_spec.get(\"encoder\", \"\")\n","                if encoder == \"\":\n","                    data_arrays.append(self.encoders.get(feature + \"_tokenizer\") \\\n","                                                    .encode_category(ddf.loc[:, feature].values))\n","                elif encoder == \"numeric_bucket\":\n","                    raise NotImplementedError\n","                elif encoder == \"hash_bucket\":\n","                    raise NotImplementedError\n","            elif feature_type == \"sequence\":\n","                data_arrays.append(self.encoders.get(feature + \"_tokenizer\") \\\n","                                                .encode_sequence(ddf.loc[:, feature].values))\n","        label_name = self.label_col[\"name\"]\n","        if ddf[label_name].dtype != np.float64:\n","            ddf.loc[:, label_name] = ddf.loc[:, label_name].apply(lambda x: float(x))\n","        data_arrays.append(ddf.loc[:, label_name].values) # add the label column at last\n","        data_arrays = [item.reshape(-1, 1) if item.ndim == 1 else item for item in data_arrays]\n","        data_array = np.hstack(data_arrays)\n","        return data_array\n","\n","    def is_share_embedding_with_sequence(self, feature):\n","        for col in self.feature_cols:\n","            if col.get(\"share_embedding\", None) == feature and col[\"type\"] == \"sequence\":\n","                return True\n","        return False\n","\n","    def load_pickle(self, pickle_file=None):\n","        return pickle.load(open(pickle_file, \"rb\"))\n","\n","    def save_pickle(self, pickle_file):\n","        if not os.path.exists(os.path.dirname(pickle_file)):\n","            os.makedirs(os.path.dirname(pickle_file))\n","        pickle.dump(self.encoders, open(pickle_file, \"wb\"))\n","        \n","    def load_json(self, json_file):\n","        self.feature_map.load(json_file)\n","\n","    def load_data(self, data_path, use_hdf5=True, data_format='csv'):\n","        self.load_json(self.json_file)\n","        self.encoders = self.load_pickle(self.pickle_file)\n","        if data_format == 'h5':\n","            data_array = self.load_hdf5(data_path)\n","            return data_array\n","        elif data_format == 'csv':\n","            hdf5_file = os.path.join(self.processed_dir, \n","                                     os.path.splitext(os.path.basename(data_path))[0] + '.h5')\n","            if use_hdf5 and os.path.exists(hdf5_file):\n","                try:\n","                    data_array = self.load_hdf5(hdf5_file)\n","                    return data_array\n","                except:\n","                    print('Loading h5 file failed, reloading from {}'.format(data_path))\n","            ddf = self.read_csv(data_path)\n","            data_array = self.transform(ddf)\n","            if use_hdf5:\n","                self.save_hdf5(data_array, hdf5_file)\n","        return data_array\n","\n","    def save_hdf5(self, data_array, data_path, key=\"data\"):\n","        if not os.path.exists(os.path.dirname(data_path)):\n","            os.makedirs(os.path.dirname(data_path))\n","        with h5py.File(data_path, 'w') as hf:\n","            hf.create_dataset(key, data=data_array)\n","\n","    def load_hdf5(self, data_path, key=\"data\"):\n","        with h5py.File(data_path, 'r') as hf:\n","            data_array = hf[key][:]\n","        return data_array\n","\n","    def __getitem__(self, index):\n","        X = self.darray[index, 0:-1]\n","        y = self.darray[index, -1]\n","        return X, y\n","    \n","    def __len__(self):\n","        return self.darray.shape[0]"],"metadata":{"id":"Cx0lWbjVhZ65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TaobaoDataset(CTRDataset):\n","\n","    feature_cols = [{'name': [\"userid\",\"adgroup_id\",\"pid\",\"cate_id\",\"campaign_id\",\"customer\",\"brand\",\"cms_segid\",\n","                                \"cms_group_id\",\"final_gender_code\",\"age_level\",\"pvalue_level\",\"shopping_level\",\"occupation\"],\n","                        'active': True, 'dtype': 'str', 'type': 'categorical'}]\n","                        \n","    label_col = {'name': 'clk', 'dtype': float}\n","\n","    train_url = \"https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/train_sample.csv\"\n","    valid_url = \"https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/valid_sample.csv\"\n","    test_url = \"https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/test_sample.csv\"\n","\n","    @property\n","    def raw_file_names(self):\n","        return ['train_sample.csv',\n","                'valid_sample.csv',\n","                'test_sample.csv']\n","\n","    def download(self):\n","        download_url(self.train_url, self.raw_dir)\n","        download_url(self.valid_url, self.raw_dir)\n","        download_url(self.test_url, self.raw_dir)\n","\n","    def convert_hour(self, df, col_name):\n","        return df['time_stamp'].apply(lambda ts: ts[11:13])\n","\n","    def convert_weekday(self, df, col_name):\n","        def _convert_weekday(timestamp):\n","            dt = date(int(timestamp[0:4]), int(timestamp[5:7]), int(timestamp[8:10]))\n","            return dt.strftime('%w')\n","        return df['time_stamp'].apply(_convert_weekday)\n","\n","    def convert_weekend(self, df, col_name):\n","        def _convert_weekend(timestamp):\n","            dt = date(int(timestamp[0:4]), int(timestamp[5:7]), int(timestamp[8:10]))\n","            return '1' if dt.strftime('%w') in ['6', '0'] else '0'\n","        return df['time_stamp'].apply(_convert_weekend)"],"metadata":{"id":"6YK5FLS6h_qs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","class CTRDataModule(LightningDataModule):\n","\n","    dataset_cls: str = \"\"\n","\n","    def __init__(self,\n","                 data_dir: Optional[str] = None,\n","                 num_workers: int = 0,\n","                 normalize: bool = False,\n","                 batch_size: int = 32,\n","                 shuffle: bool = True,\n","                 pin_memory: bool = True,\n","                 drop_last: bool = False,\n","                 *args, \n","                 **kwargs) -> None:\n","        \"\"\"\n","        Args:\n","            data_dir: Where to save/load the data\n","            num_workers: How many workers to use for loading data\n","            normalize: If true applies rating normalize\n","            batch_size: How many samples per batch to load\n","            shuffle: If true shuffles the train data every epoch\n","            pin_memory: If true, the data loader will copy Tensors into CUDA pinned memory before\n","                        returning them\n","            drop_last: If true drops the last incomplete batch\n","        \"\"\"\n","        super().__init__(data_dir)\n","\n","        self.data_dir = data_dir if data_dir is not None else os.getcwd()\n","        self.num_workers = num_workers\n","        self.normalize = normalize\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.pin_memory = pin_memory\n","        self.drop_last = drop_last\n","        self.kwargs = kwargs\n","\n","    def prepare_data(self, *args: Any, **kwargs: Any) -> None:\n","        \"\"\"Saves files to data_dir.\"\"\"\n","        self.dataset = self.dataset_cls(self.data_dir, **self.kwargs)\n","\n","    def setup(self, stage: Optional[str] = None) -> None:\n","        \"\"\"Creates train, val, and test dataset.\"\"\"\n","        if stage == \"fit\" or stage is None:\n","            self.dataset_train = self.dataset_cls(self.data_dir, data_type='train', **self.kwargs)\n","            self.dataset_val = self.dataset_cls(self.data_dir, data_type='valid', **self.kwargs)\n","        if stage == \"test\" or stage is None:\n","            self.dataset_test = self.dataset_cls(self.data_dir, data_type='test', **self.kwargs)\n","\n","    def train_dataloader(self, *args: Any, **kwargs: Any) -> DataLoader:\n","        \"\"\"The train dataloader.\"\"\"\n","        return self._data_loader(self.dataset_train, shuffle=self.shuffle)\n","\n","    def val_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n","        \"\"\"The val dataloader.\"\"\"\n","        return self._data_loader(self.dataset_val)\n","\n","    def test_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n","        \"\"\"The test dataloader.\"\"\"\n","        return self._data_loader(self.dataset_test)\n","\n","    def _data_loader(self, dataset: Dataset, shuffle: bool = False) -> DataLoader:\n","        return DataLoader(\n","            dataset,\n","            batch_size=self.batch_size,\n","            shuffle=shuffle,\n","            num_workers=self.num_workers,\n","            drop_last=self.drop_last,\n","            pin_memory=self.pin_memory,\n","        )"],"metadata":{"id":"FPp-PB-ow_Tv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TaobaoDataModule(CTRDataModule):\n","    dataset_cls = TaobaoDataset"],"metadata":{"id":"o98gB9N_4dL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params = {'model_id': 'DCN_demo',\n","              'data_dir': '/content/data',\n","              'model_root': './checkpoints/',\n","              'dnn_hidden_units': [64, 64],\n","              'dnn_activations': \"relu\",\n","              'crossing_layers': 3,\n","              'learning_rate': 1e-3,\n","              'net_dropout': 0,\n","              'batch_norm': False,\n","              'optimizer': 'adamw',\n","              'task': 'binary_classification',\n","              'loss': 'binary_crossentropy',\n","              'metrics': ['logloss', 'AUC'],\n","              'embedding_dim': 10,\n","              'batch_size': 64,\n","              'epochs': 3,\n","              'shuffle': True,\n","              'seed': 2019,\n","              'use_hdf5': True,\n","              'workers': 1,\n","              'verbose': 0}"],"metadata":{"id":"egWI7tWgVMrX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds = TaobaoDataModule(**params)\n","ds.prepare_data()\n","ds.setup()\n","\n","for batch in ds.train_dataloader():\n","    print(batch)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wm5J-aNOV3uC","executionInfo":{"status":"ok","timestamp":1641901070196,"user_tz":-330,"elapsed":1378,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"08c9a17f-be5e-44e5-b7f0-00f307eba439"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:74: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n","  \"DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n","Downloading https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/train_sample.csv\n","Downloading https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/valid_sample.csv\n","Downloading https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/test_sample.csv\n"]},{"output_type":"stream","name":"stdout","text":["[tensor([[ 9., 30.,  1., 24., 30., 30., 24.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n","        [20., 75.,  1., 42., 73., 72.,  7.,  9.,  9.,  2.,  2.,  2.,  1.,  1.],\n","        [ 5., 53.,  1.,  7.,  2.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n","        [13., 60.,  1.,  2., 58., 58., 45.,  1.,  5.,  2.,  3.,  0.,  3.,  1.],\n","        [ 6., 51.,  1.,  2., 50., 50., 40.,  1.,  6.,  2.,  1.,  0.,  1.,  1.],\n","        [ 5., 52.,  1.,  1., 51., 51.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n","        [ 3., 63.,  1.,  1., 61.,  2.,  3.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n","        [ 5., 37.,  1., 15., 36., 36., 29.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n","        [22., 93.,  2., 16., 91., 90., 61.,  4.,  3.,  1.,  4.,  1.,  1.,  1.],\n","        [ 2.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [ 4., 80.,  2., 44., 78., 77.,  4.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n","        [ 2., 28.,  1.,  6., 28., 28.,  0.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [ 8.,  9.,  2.,  1., 10., 11.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n","        [13., 50.,  1., 36., 49., 49., 39.,  1.,  5.,  2.,  3.,  0.,  3.,  1.],\n","        [14., 41.,  1.,  4., 40., 40., 33.,  1.,  3.,  1.,  4.,  0.,  3.,  1.],\n","        [ 8., 21.,  2.,  1., 21., 21., 18.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n","        [16.,  1.,  1.,  4.,  3.,  4.,  5.,  1.,  5.,  2.,  3.,  0.,  2.,  1.],\n","        [23., 69.,  2.,  2., 67., 66.,  6.,  1.,  6.,  2.,  1.,  1.,  1.,  1.],\n","        [ 2.,  8.,  1., 14.,  9., 10., 14.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [ 2., 25.,  1.,  1., 25., 25., 21.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [ 2., 10.,  1.,  1., 11., 12., 15.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [17.,  1.,  1.,  4.,  3.,  4.,  5.,  7.,  3.,  1.,  4.,  2.,  2.,  1.],\n","        [ 2., 24.,  1.,  3., 24., 24., 20.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [12., 38.,  1., 28., 37., 37., 30.,  5.,  8.,  2.,  5.,  1.,  1.,  1.],\n","        [ 7.,  5.,  2., 12.,  6.,  7., 11.,  4.,  3.,  1.,  4.,  1.,  1.,  1.],\n","        [17., 90.,  1., 19., 88., 87., 60.,  7.,  3.,  1.,  4.,  2.,  2.,  1.],\n","        [ 3., 84.,  1., 45., 82., 81., 57.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n","        [ 3., 82.,  1.,  1., 80., 79.,  0.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n","        [20., 96.,  1., 18., 94., 93., 63.,  9.,  9.,  2.,  2.,  2.,  1.,  1.],\n","        [18., 70.,  1., 38., 68., 67., 51.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [19., 83.,  1., 18., 81., 80., 56.,  8.,  1.,  1.,  1.,  2.,  1.,  2.],\n","        [11., 32.,  2., 25., 32., 32., 26.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n","        [ 7.,  6.,  2., 21.,  7.,  8., 12.,  4.,  3.,  1.,  4.,  1.,  1.,  1.],\n","        [ 1., 16.,  1., 22., 16., 16.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [21., 11.,  2.,  3., 12., 13.,  0.,  1.,  7.,  1.,  5.,  0.,  1.,  1.],\n","        [11., 58.,  2.,  1., 56., 56., 43.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n","        [ 1., 29.,  1.,  9., 29., 29.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [13., 39.,  1., 29., 38., 38., 31.,  1.,  5.,  2.,  3.,  0.,  3.,  1.],\n","        [ 1., 12.,  1.,  8., 13., 14.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 1., 27.,  1.,  9., 27., 27., 23.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 2., 14.,  1.,  5., 15., 15.,  0.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [17., 98.,  1., 19., 96., 95., 65.,  7.,  3.,  1.,  4.,  2.,  2.,  1.],\n","        [ 5., 36.,  1.,  7., 35., 35., 28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n","        [ 2., 18.,  1.,  3., 18., 18.,  0.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [19., 71.,  1., 39., 69., 68., 52.,  8.,  1.,  1.,  1.,  2.,  1.,  2.],\n","        [ 3., 88.,  1.,  6., 86., 85.,  0.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n","        [ 6., 47.,  1., 33., 46., 46., 36.,  1.,  6.,  2.,  1.,  0.,  1.,  1.],\n","        [18., 78.,  1., 43., 76., 75.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 1., 19.,  1.,  8., 19., 19., 17.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 1., 20.,  1.,  9., 20., 20.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 1.,  2.,  1.,  8.,  4.,  5.,  9.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 4., 85.,  2., 46., 83., 82., 58.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n","        [23., 95.,  2.,  2., 93., 92.,  6.,  1.,  6.,  2.,  1.,  1.,  1.,  1.],\n","        [22., 66.,  2., 16., 64., 63., 49.,  4.,  3.,  1.,  4.,  1.,  1.,  1.],\n","        [ 1., 26.,  1., 23., 26., 26., 22.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 2., 17.,  1.,  2., 17., 17., 16.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n","        [ 5., 35.,  1.,  7.,  2.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n","        [ 3., 72.,  1., 40., 70., 69., 53.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n","        [18., 73.,  1., 41., 71., 70.,  7.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 4., 65.,  2., 11., 63., 62., 48.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n","        [10., 34.,  2., 27., 34., 34.,  2.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n","        [ 5., 61.,  1.,  1., 59., 59.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n","        [15., 55.,  2., 10., 53., 53., 41.,  6.,  7.,  1.,  5.,  1.,  1.,  1.],\n","        [ 3., 77.,  1.,  1., 75., 74., 55.,  1.,  4.,  1.,  3.,  0.,  1.,  1.]],\n","       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)]\n"]},{"output_type":"stream","name":"stderr","text":["Processing...\n","Done!\n"]}]},{"cell_type":"code","source":["from torch import nn\n","import torch\n","\n","class MaskedAveragePooling(nn.Module):\n","    def __init__(self):\n","        super(MaskedAveragePooling, self).__init__()\n","\n","    def forward(self, embedding_matrix):\n","        sum_pooling_matrix = torch.sum(embedding_matrix, dim=1)\n","        non_padding_length = (embedding_matrix != 0).sum(dim=1)\n","        embedding_vec = sum_pooling_matrix / (non_padding_length.float() + 1e-16)\n","        return embedding_vec\n","\n","\n","class MaskedSumPooling(nn.Module):\n","    def __init__(self):\n","        super(MaskedSumPooling, self).__init__()\n","\n","    def forward(self, embedding_matrix):\n","        # mask by zeros\n","        return torch.sum(embedding_matrix, dim=1)"],"metadata":{"id":"m6bTZbgzW2PU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import h5py\n","import os\n","import numpy as np\n","from collections import OrderedDict\n","# from . import sequence\n","\n","\n","class EmbeddingLayer_v3(nn.Module):\n","    def __init__(self, \n","                 feature_map, \n","                 embedding_dim,\n","                 embedding_dropout=0,\n","                 required_feature_columns=[],\n","                 not_required_feature_columns=[]):\n","        super(EmbeddingLayer_v3, self).__init__()\n","        self.embedding_layer = EmbeddingDictLayer(feature_map, \n","                                                  embedding_dim,\n","                                                  required_feature_columns,\n","                                                  not_required_feature_columns)\n","        self.dropout = nn.Dropout2d(embedding_dropout) if embedding_dropout > 0 else None\n","\n","    def forward(self, X):\n","        feature_emb_dict = self.embedding_layer(X)\n","        feature_emb = torch.stack(self.embedding_layer.dict2list(feature_emb_dict), dim=1)\n","        if self.dropout is not None:\n","            feature_emb = self.dropout(feature_emb)\n","        return feature_emb\n","\n","\n","class EmbeddingDictLayer(nn.Module):\n","    def __init__(self, \n","                 feature_map, \n","                 embedding_dim, \n","                 required_feature_columns=[],\n","                 not_required_feature_columns=[]):\n","        super(EmbeddingDictLayer, self).__init__()\n","        self._feature_map = feature_map\n","        self.required_feature_columns = required_feature_columns\n","        self.not_required_feature_columns = not_required_feature_columns\n","        self.embedding_layer = nn.ModuleDict()\n","        self.seq_encoder_layer = nn.ModuleDict()\n","        for feature, feature_spec in self._feature_map.feature_specs.items():\n","            if self.is_required(feature):\n","                # Set embedding_layer according to share_embedding\n","                if \"share_embedding\" in feature_spec:\n","                    self.embedding_layer[feature] = self.embedding_layer[feature_spec[\"share_embedding\"]]\n","                feat_emb_dim = feature_spec.get(\"embedding_dim\", embedding_dim)\n","                if feature_spec[\"type\"] == \"numeric\":\n","                    if feature not in self.embedding_layer:\n","                        self.embedding_layer[feature] = nn.Linear(1, feat_emb_dim, bias=False)\n","                elif feature_spec[\"type\"] == \"categorical\":\n","                    if feature not in self.embedding_layer:\n","                        padding_idx = feature_spec.get(\"padding_idx\", None)\n","                        embedding_matrix = nn.Embedding(feature_spec[\"vocab_size\"], \n","                                                        feat_emb_dim, \n","                                                        padding_idx=padding_idx)\n","                        if \"pretrained_emb\" in feature_spec:\n","                            embeddings = self.get_pretrained_embedding(feature_map.data_dir, feature, feature_spec)\n","                            embedding_matrix = self.set_pretrained_embedding(embedding_matrix, embeddings, \n","                                                                             freeze=feature_spec[\"freeze_emb\"],\n","                                                                             padding_idx=padding_idx)\n","                        self.embedding_layer[feature] = embedding_matrix\n","                elif feature_spec[\"type\"] == \"sequence\":\n","                    if feature not in self.embedding_layer:\n","                        padding_idx = feature_spec[\"vocab_size\"] - 1\n","                        embedding_matrix = nn.Embedding(feature_spec[\"vocab_size\"], \n","                                                        feat_emb_dim, \n","                                                        padding_idx=padding_idx)\n","                        if \"pretrained_emb\" in feature_spec:\n","                            embeddings = self.get_pretrained_embedding(feature_map.data_dir, feature, feature_spec)\n","                            embedding_matrix = self.set_pretrained_embedding(embedding_matrix, embeddings, \n","                                                                             freeze=feature_spec[\"freeze_emb\"],\n","                                                                             padding_idx=padding_idx)\n","                        self.embedding_layer[feature] = embedding_matrix\n","                    self.set_sequence_encoder(feature, feature_spec.get(\"encoder\", None))\n","\n","    def is_required(self, feature):\n","        \"\"\" Check whether feature is required for embedding \"\"\"\n","        feature_spec = self._feature_map.feature_specs[feature]\n","        if len(self.required_feature_columns) > 0 and (feature not in self.required_feature_columns):\n","            return False\n","        elif feature in self.not_required_feature_columns:\n","            return False\n","        else:\n","            return True\n","\n","    def set_sequence_encoder(self, feature, encoder):\n","        if encoder is None or encoder in [\"none\", \"null\"]:\n","            self.seq_encoder_layer.update({feature: None})\n","        elif encoder == \"MaskedAveragePooling\":\n","            self.seq_encoder_layer.update({feature: sequence.MaskedAveragePooling()})\n","        elif encoder == \"MaskedSumPooling\":\n","            self.seq_encoder_layer.update({feature: sequence.MaskedSumPooling()})\n","        else:\n","            raise RuntimeError(\"Sequence encoder={} is not supported.\".format(encoder))\n","\n","    def get_pretrained_embedding(self, data_dir, feature_name, feature_spec):\n","        pretrained_path = os.path.join(data_dir, feature_spec[\"pretrained_emb\"])\n","        with h5py.File(pretrained_path, 'r') as hf:\n","            embeddings = hf[feature_name][:]\n","        return embeddings\n","\n","    def set_pretrained_embedding(self, embedding_matrix, embeddings, freeze=False, padding_idx=None):\n","        if padding_idx is not None:\n","            embeddings[padding_idx] = np.zeros(embeddings.shape[-1])\n","        embeddings = torch.from_numpy(embeddings).float()\n","        embedding_matrix.weight = torch.nn.Parameter(embeddings)\n","        if freeze:\n","            embedding_matrix.weight.requires_grad = False\n","        return embedding_matrix\n","\n","    def dict2list(self, embedding_dict):\n","        return list(embedding_dict.values())\n","\n","    def dict2tensor(self, embedding_dict, feature_source=None, feature_type=None):\n","        if feature_source is not None:\n","            if not isinstance(feature_source, list):\n","                feature_source = [feature_source]\n","            feature_emb_list = []\n","            for feature, feature_spec in self._feature_map.feature_specs.items():\n","                if feature_spec[\"source\"] in feature_source:\n","                    feature_emb_list.append(embedding_dict[feature])\n","            return torch.stack(feature_emb_list, dim=1)\n","        elif feature_type is not None:\n","            if not isinstance(feature_type, list):\n","                feature_type = [feature_type]\n","            feature_emb_list = []\n","            for feature, feature_spec in self._feature_map.feature_specs.items():\n","                if feature_spec[\"type\"] in feature_type:\n","                    feature_emb_list.append(embedding_dict[feature])\n","            return torch.stack(feature_emb_list, dim=1)\n","        else:\n","            return torch.stack(list(embedding_dict.values()), dim=1)\n","\n","    def forward(self, X):\n","        feature_emb_dict = OrderedDict()\n","        for feature, feature_spec in self._feature_map.feature_specs.items():\n","            if feature in self.embedding_layer:\n","                if feature_spec[\"type\"] == \"numeric\":\n","                    inp = X[:, feature_spec[\"index\"]].float().view(-1, 1)\n","                    embedding_vec = self.embedding_layer[feature](inp)\n","                elif feature_spec[\"type\"] == \"categorical\":\n","                    inp = X[:, feature_spec[\"index\"]].long()\n","                    embedding_vec = self.embedding_layer[feature](inp)\n","                elif feature_spec[\"type\"] == \"sequence\":\n","                    inp = X[:, feature_spec[\"index\"]].long()\n","                    seq_embed_matrix = self.embedding_layer[feature](inp)\n","                    if self.seq_encoder_layer[feature] is not None:\n","                        embedding_vec = self.seq_encoder_layer[feature](seq_embed_matrix)\n","                    else:\n","                        embedding_vec = seq_embed_matrix\n","                feature_emb_dict[feature] = embedding_vec\n","        return feature_emb_dict"],"metadata":{"id":"-6YE_P-qWlaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from torch import nn\n","import torch\n","\n","\n","def get_activation(activation):\n","    if isinstance(activation, str):\n","        if activation.lower() == \"relu\":\n","            return nn.ReLU()\n","        elif activation.lower() == \"sigmoid\":\n","            return nn.Sigmoid()\n","        elif activation.lower() == \"tanh\":\n","            return nn.Tanh()\n","        else:\n","            return getattr(nn, activation)()\n","    else:\n","        return activation\n","\n","\n","class DNN_Layer(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 output_dim=None, \n","                 hidden_units=[], \n","                 hidden_activations=\"ReLU\",\n","                 final_activation=None, \n","                 dropout_rates=[], \n","                 batch_norm=False, \n","                 use_bias=True):\n","        super(DNN_Layer, self).__init__()\n","        dense_layers = []\n","        if not isinstance(dropout_rates, list):\n","            dropout_rates = [dropout_rates] * len(hidden_units)\n","        if not isinstance(hidden_activations, list):\n","            hidden_activations = [hidden_activations] * len(hidden_units)\n","        hidden_activations = [set_activation(x) for x in hidden_activations]\n","        hidden_units = [input_dim] + hidden_units\n","        for idx in range(len(hidden_units) - 1):\n","            dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))\n","            if batch_norm:\n","                dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))\n","            if hidden_activations[idx]:\n","                dense_layers.append(hidden_activations[idx])\n","            if dropout_rates[idx] > 0:\n","                dense_layers.append(nn.Dropout(p=dropout_rates[idx]))\n","        if output_dim is not None:\n","            dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))\n","        if final_activation is not None:\n","            dense_layers.append(get_activation(final_activation))\n","        self.dnn = nn.Sequential(*dense_layers) # * used to unpack list\n","    \n","    def forward(self, inputs):\n","        return self.dnn(inputs)"],"metadata":{"id":"SfFOjn2XW__7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CrossNet(nn.Module):\n","    def __init__(self, input_dim, num_layers):\n","        super(CrossNet, self).__init__()\n","        self.num_layers = num_layers\n","        self.cross_net = nn.ModuleList(CrossInteractionLayer(input_dim)\n","                                       for _ in range(self.num_layers))\n","\n","    def forward(self, X_0):\n","        X_i = X_0 # b x dim\n","        for i in range(self.num_layers):\n","            X_i = X_i + self.cross_net[i](X_0, X_i)\n","        return X_i\n","\n","\n","class CrossInteractionLayer(nn.Module):\n","    def __init__(self, input_dim):\n","        super(CrossInteractionLayer, self).__init__()\n","        self.weight = nn.Linear(input_dim, 1, bias=False)\n","        self.bias = nn.Parameter(torch.zeros(input_dim))\n","\n","    def forward(self, X_0, X_i):\n","        interaction_out = self.weight(X_i) * X_0 + self.bias\n","        return interaction_out"],"metadata":{"id":"YFVnf3TC-X5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n","import numpy as np\n","import logging\n","\n","\n","def evaluate_metrics(y_true, y_pred, metrics):\n","    result = dict()\n","    for metric in metrics:\n","        if metric in ['logloss', 'binary_crossentropy']:\n","            result[metric] = log_loss(y_true, y_pred, eps=1e-7)\n","        elif metric == 'AUC':\n","            result[metric] = roc_auc_score(y_true, y_pred)\n","        elif metric == \"ACC\":\n","            y_pred = np.argmax(y_pred, axis=1)\n","            result[metric] = accuracy_score(y_true, y_pred)\n","    logging.info('[Metrics] ' + ' - '.join('{}: {:.6f}'.format(k, v) for k, v in result.items()))\n","    return result"],"metadata":{"id":"RwU1b2_kXoNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n","\n","from torch.nn import functional as F\n","\n","from pytorch_lightning import LightningModule\n","\n","\n","class CTRModel(LightningModule):\n","    def __init__(self, \n","                 feature_map, \n","                 model_id=\"BaseModel\",\n","                 optimizer='adamw',\n","                 learning_rate = 0.003,\n","                 **kwargs):\n","        super().__init__()\n","        self._feature_map = feature_map\n","        self.model_id = model_id\n","        self.optimizer = optimizer\n","        self.learning_rate = learning_rate\n","        self.model_dir = os.path.join(kwargs[\"model_root\"], feature_map.dataset_id)\n","        self.checkpoint = os.path.abspath(os.path.join(self.model_dir, self.model_id + \"_model.ckpt\"))\n","        self._validation_metrics = kwargs[\"metrics\"]\n","        self._verbose = kwargs[\"verbose\"]\n","\n","    def forward(self, users, items):\n","        raise NotImplementedError\n","\n","    def training_step(self, batch, batch_idx):\n","        features, y_true = batch\n","        y_pred = self(features)\n","\n","        y_pred = y_pred.view(-1,1).squeeze()\n","        y_true = y_true.float()\n","\n","        loss = self.loss_fn(y_pred, y_true)\n","\n","        return {\n","            \"loss\": loss,\n","            \"y_pred\": y_pred.detach(),\n","        }\n","\n","    def training_epoch_end(self, outputs):\n","        # This function recevies as parameters the output from \"training_step()\"\n","        # Outputs is a list which contains a dictionary like:\n","        # [{'pred':x,'target':x,'loss':x}, {'pred':x,'target':x,'loss':x}, ...]\n","        pass\n","\n","    def validation_step(self, batch, batch_idx):\n","        features, y_true = batch\n","        y_pred = self(features)\n","\n","        y_pred = np.array(y_pred.cpu().numpy().reshape(-1), np.float64)\n","        y_true = np.array(y_true.cpu().numpy().reshape(-1), np.float64)\n","        val_logs = evaluate_metrics(y_true, y_pred, self._validation_metrics)\n","        self.log(\"Val Metrics\", val_logs, prog_bar=True)\n","\n","        return {\n","            \"y_pred\": y_pred,\n","        }\n","\n","    def validation_epoch_end(self, outputs):\n","        pass\n","\n","    def test_step(self, batch, batch_idx):\n","        features, y_true = batch\n","        y_pred = self(features)\n","\n","        y_pred = np.array(y_pred.cpu().numpy().reshape(-1), np.float64)\n","        y_true = np.array(y_true.cpu().numpy().reshape(-1), np.float64)\n","        test_logs = evaluate_metrics(y_true, y_pred, self._validation_metrics)\n","        self.log(\"Test Metrics\", test_logs, prog_bar=True)\n","\n","        return {\n","            \"y_pred\": y_pred,\n","        }\n","\n","    def test_epoch_end(self, outputs):\n","        pass\n","\n","    def configure_optimizers(self):\n","        if self.optimizer == 'adamw':\n","            return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n","        elif self.optimizer == 'adam':\n","            return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        elif self.optimizer == 'sgd':\n","            return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n","        else:\n","            raise ValueError(f'Invalid optimizer type: {self.optimizer}')\n","\n","    def loss_fn(self, y_pred, y_true):\n","        return F.binary_cross_entropy(y_pred, y_true, reduction='mean')        \n","\n","    def init_weights(self, embedding_initializer=None):\n","        def _initialize(m):\n","            if type(m) == nn.ModuleDict:\n","                for k, v in m.items():\n","                    if type(v) == nn.Embedding:\n","                        if \"pretrained_emb\" in self._feature_map.feature_specs[k]: # skip pretrained\n","                            continue\n","                        if embedding_initializer is not None:\n","                            try:\n","                                initializer = embedding_initializer.replace(\"(\", \"(v.weight,\")\n","                                eval(initializer)\n","                            except:\n","                                raise NotImplementedError(\"embedding_initializer={} is not supported.\"\\\n","                                                          .format(embedding_initializer))\n","                        else:\n","                            nn.init.xavier_normal_(v.weight)\n","            if type(m) == nn.Linear:\n","                nn.init.xavier_normal_(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.fill_(0)\n","        self.apply(_initialize)\n","\n","    def get_final_activation(self, task=\"binary_classification\"):\n","        if task == \"binary_classification\":\n","            return nn.Sigmoid()\n","        elif task == \"multi_classification\":\n","            return nn.Softmax(dim=-1)\n","        elif task == \"regression\":\n","            return None\n","        else:\n","            raise NotImplementedError(\"task={} is not supported.\".format(task))"],"metadata":{"id":"TRBB39Hb-Zrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DCN(CTRModel):\n","    def __init__(self, \n","                 feature_map, \n","                 model_id=\"DCN\",\n","                 task=\"binary_classification\",\n","                 learning_rate=1e-3, \n","                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n","                 embedding_dim=10, \n","                 dnn_hidden_units=[], \n","                 dnn_activations=\"ReLU\",\n","                 crossing_layers=3, \n","                 embedding_dropout=0,\n","                 net_dropout=0, \n","                 batch_norm=False,\n","                 **kwargs):\n","        super(DCN, self).__init__(feature_map, \n","                                  model_id=model_id,\n","                                  **kwargs)\n","        self.embedding_layer = EmbeddingLayer_v3(feature_map, embedding_dim)\n","        input_dim = feature_map.num_fields * embedding_dim\n","        self.dnn = DNN_Layer(input_dim=input_dim,\n","                             output_dim=None, # output hidden layer\n","                             hidden_units=dnn_hidden_units,\n","                             hidden_activations=dnn_activations,\n","                             final_activation=None, \n","                             dropout_rates=net_dropout, \n","                             batch_norm=batch_norm, \n","                             use_bias=True) \\\n","                   if dnn_hidden_units else None # in case of only crossing net used\n","        self.crossnet = CrossNet(input_dim, crossing_layers)\n","        final_dim = input_dim\n","        if isinstance(dnn_hidden_units, list) and len(dnn_hidden_units) > 0: # if use dnn\n","            final_dim += dnn_hidden_units[-1]\n","        self.fc = nn.Linear(final_dim, 1) # [cross_part, dnn_part] -> logit\n","        self.final_activation = self.get_final_activation(task)\n","        self.init_weights(embedding_initializer=embedding_initializer)\n","\n","    def forward(self, inputs):\n","        feature_emb = self.embedding_layer(inputs)\n","        flat_feature_emb = feature_emb.flatten(start_dim=1)\n","        cross_out = self.crossnet(flat_feature_emb)\n","        if self.dnn is not None:\n","            dnn_out = self.dnn(flat_feature_emb)\n","            final_out = torch.cat([cross_out, dnn_out], dim=-1)\n","        else:\n","            final_out = cross_out\n","        y_pred = self.fc(final_out)\n","        if self.final_activation is not None:\n","            y_pred = self.final_activation(y_pred)\n","        return y_pred"],"metadata":{"id":"bsVvu-uLV8DQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DCN(ds.dataset.feature_map, **params)"],"metadata":{"id":"OKIJr8n2Xzpk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from recohut.trainers.pl_trainer import pl_trainer\n","\n","pl_trainer(model, ds, max_epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622,"referenced_widgets":["90c8d6ca4eab4a18826d9f17e6c0636a","0ec7f324b42548408e92d5e46941457a","b035f3678cc1405fbd6f7564cff1ed11","7bd2575559c3495eb007f0f0cb977fe0","54e7a8030d7249f5b17f49c5f8a3fbbc","e872ca6772ac425d93cac83b155c5fc7","f8eab32990684d6d958ab7d0d33245be","9053c6b2a56b4cd186e75fc9b3c5062d","4f3374782cd14df7af446143297b001f","d32f1e4393d9438a9945bd212b171116","b1af7ef3e7fa4319bbdf0df5edd916aa","399ba0f7457b40049d8799902fe84a91","bbf347d0912447aa82a179a4afb84fe1","ab51107fe2c2487296db1c604995a759","0066aea305524b8ab1e49e75ea706bc3","da32380ac57e472cb9e23be86d72ef5d","b0b15b2a71dc49d9ad3f33866efb64c9","ef5473426dbb4d508d6c98fef517d5ab","f7d4c60a11f141bd99c2be2477f7545d","b00a382d630a47bd99d43f9aa522061c","21d0bf55dbc6452d8a8941a729f36901","6f91cad0cbd446b98b75d7eab4410549","ae0c3dbb7dca40428364891d2254562c","7a916ae44a904a7898a052075fbcf171","9803d4cbd47141eaa472448302796be4","a120902b27c04d88a3e93e43eb418f92","d8ba096c094142568e5685850227ca01","f4dcc40365c7488e8a0e3e4d3aa06826","5901fa87e7b24fefb813c0b4657a1786","bebf2c2da8cf48eea3a72820045f0937","b0a77af7b06d4842bde904047902b68c","a350823ebf0940c9a217e25ac9e2e96d","14da0a7882d44739a9632db85ed19385"]},"id":"dTacyen5HGrn","executionInfo":{"status":"ok","timestamp":1641901617100,"user_tz":-330,"elapsed":4467,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"14a811e9-9de2-40a2-d7e3-3f3082178de8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n","  f\"DataModule.{name} has already been called, so it will not be called again. \"\n","Missing logger folder: /content/default\n","\n","  | Name             | Type              | Params\n","-------------------------------------------------------\n","0 | embedding_layer  | EmbeddingLayer_v3 | 4.8 K \n","1 | dnn              | DNN_Layer         | 13.2 K\n","2 | crossnet         | CrossNet          | 840   \n","3 | fc               | Linear            | 205   \n","4 | final_activation | Sigmoid           | 0     \n","-------------------------------------------------------\n","19.0 K    Trainable params\n","0         Non-trainable params\n","19.0 K    Total params\n","0.076     Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /content exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90c8d6ca4eab4a18826d9f17e6c0636a","version_minor":0,"version_major":2},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"399ba0f7457b40049d8799902fe84a91","version_minor":0,"version_major":2},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n","  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae0c3dbb7dca40428364891d2254562c","version_minor":0,"version_major":2},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1633)}}\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1633)}}]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":[""],"metadata":{"id":"d7Iqbq8K9lqc"},"execution_count":null,"outputs":[]}]}