# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/models/layers/models.layers.encoding.ipynb (unless otherwise specified).

__all__ = ['Intermediate', 'DistIntermediate', 'Layer', 'DistLayer', 'DistMeanSALayer', 'DistSAEncoder',
           'DistMeanSAEncoder', 'Encoder']

# Cell
import torch
import torch.nn as nn
import torch.nn.functional as F

import copy

from .activation import gelu, swish
from .attention import SelfAttention, DistSelfAttention, DistMeanSelfAttention

# Internal Cell
ACT2FN = {"gelu": gelu, "relu": F.relu, "swish": swish}

# Cell
class Intermediate(nn.Module):
    def __init__(self, hidden_size, hidden_act, hidden_dropout_prob):
        super().__init__()
        self.dense_1 = nn.Linear(hidden_size, hidden_size * 4)
        if isinstance(hidden_act, str):
            self.intermediate_act_fn = ACT2FN[hidden_act]
        else:
            self.intermediate_act_fn = hidden_act

        self.dense_2 = nn.Linear(hidden_size * 4, hidden_size)
        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(hidden_dropout_prob)

    def forward(self, input_tensor):

        hidden_states = self.dense_1(input_tensor)
        hidden_states = self.intermediate_act_fn(hidden_states)

        hidden_states = self.dense_2(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.layernorm(hidden_states + input_tensor)

        return hidden_states

# Cell
class DistIntermediate(nn.Module):
    def __init__(self, hidden_size, hidden_dropout_prob):
        super().__init__()
        self.dense_1 = nn.Linear(hidden_size, hidden_size * 4)
        self.intermediate_act_fn = nn.ELU()

        self.dense_2 = nn.Linear(hidden_size * 4, hidden_size)
        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(hidden_dropout_prob)

    def forward(self, input_tensor):

        hidden_states = self.dense_1(input_tensor)
        hidden_states = self.intermediate_act_fn(hidden_states)

        hidden_states = self.dense_2(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.layernorm(hidden_states + input_tensor)

        return hidden_states

# Cell
class Layer(nn.Module):
    def __init__(self, hidden_size, hidden_act, num_attention_heads,
                 hidden_dropout_prob, attention_probs_dropout_prob):
        super().__init__()
        self.attention = SelfAttention(hidden_size, num_attention_heads,
                                       attention_probs_dropout_prob, hidden_dropout_prob)
        self.intermediate = Intermediate(hidden_size, hidden_act, hidden_dropout_prob)

    def forward(self, hidden_states, attention_mask):
        attention_output, attention_scores = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        return intermediate_output, attention_scores

# Cell
class DistLayer(nn.Module):
    def __init__(self, hidden_size, num_attention_heads, hidden_dropout_prob,
                 attention_probs_dropout_prob, distance_metric='wasserstein'):
        super().__init__()
        self.attention = DistSelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,
                                           attention_probs_dropout_prob, distance_metric)
        self.mean_intermediate = DistIntermediate(hidden_size, hidden_dropout_prob)
        self.cov_intermediate = DistIntermediate(hidden_size, hidden_dropout_prob)
        self.activation_func = nn.ELU()

    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask):
        mean_attention_output, cov_attention_output, attention_scores = self.attention(mean_hidden_states, cov_hidden_states, attention_mask)
        mean_intermediate_output = self.mean_intermediate(mean_attention_output)
        cov_intermediate_output = self.activation_func(self.cov_intermediate(cov_attention_output)) + 1
        return mean_intermediate_output, cov_intermediate_output, attention_scores

# Cell
class DistMeanSALayer(nn.Module):
    def __init__(self, hidden_size, num_attention_heads, hidden_dropout_prob,
                 attention_probs_dropout_prob):
        super().__init__()
        self.attention = DistMeanSelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,
                                               attention_probs_dropout_prob)
        self.mean_intermediate = DistIntermediate(hidden_size, hidden_dropout_prob)
        self.cov_intermediate = DistIntermediate(hidden_size, hidden_dropout_prob)
        self.activation_func = nn.ELU()

    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask):
        mean_attention_output, cov_attention_output, attention_scores = self.attention(mean_hidden_states, cov_hidden_states, attention_mask)
        mean_intermediate_output = self.mean_intermediate(mean_attention_output)
        cov_intermediate_output = self.activation_func(self.cov_intermediate(cov_attention_output)) + 1
        return mean_intermediate_output, cov_intermediate_output, attention_scores

# Cell
class DistSAEncoder(nn.Module):
    def __init__(self, hidden_size, num_attention_heads, hidden_dropout_prob,
                 attention_probs_dropout_prob, num_hidden_layers,
                 distance_metric='wasserstein'):
        super().__init__()
        layer = DistLayer(hidden_size, num_attention_heads, hidden_dropout_prob,
                          attention_probs_dropout_prob, distance_metric)
        self.layer = nn.ModuleList([copy.deepcopy(layer)
                                    for _ in range(num_hidden_layers)])

    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask, output_all_encoded_layers=True):
        all_encoder_layers = []
        for layer_module in self.layer:
            maen_hidden_states, cov_hidden_states, att_scores = layer_module(mean_hidden_states, cov_hidden_states, attention_mask)
            if output_all_encoded_layers:
                all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])
        if not output_all_encoded_layers:
            all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])
        return all_encoder_layers

# Cell
class DistMeanSAEncoder(nn.Module):
    def __init__(self, hidden_size, num_attention_heads, hidden_dropout_prob,
                 attention_probs_dropout_prob, num_hidden_layers):
        super().__init__()
        layer = DistMeanSALayer(hidden_size, num_attention_heads, hidden_dropout_prob,
                 attention_probs_dropout_prob)
        self.layer = nn.ModuleList([copy.deepcopy(layer)
                                    for _ in range(num_hidden_layers)])

    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask, output_all_encoded_layers=True):
        all_encoder_layers = []
        for layer_module in self.layer:
            maen_hidden_states, cov_hidden_states, att_scores = layer_module(mean_hidden_states, cov_hidden_states, attention_mask)
            if output_all_encoded_layers:
                all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])
        if not output_all_encoded_layers:
            all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])
        return all_encoder_layers

# Cell
class Encoder(nn.Module):
    def __init__(self, hidden_size, hidden_act, num_attention_heads,
                 hidden_dropout_prob, attention_probs_dropout_prob,
                 num_hidden_layers):
        super().__init__()
        layer = Layer(hidden_size, hidden_act, num_attention_heads,
                 hidden_dropout_prob, attention_probs_dropout_prob)
        self.layer = nn.ModuleList([copy.deepcopy(layer)
                                    for _ in range(num_hidden_layers)])

    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):
        all_encoder_layers = []
        for layer_module in self.layer:
            hidden_states, attention_scores = layer_module(hidden_states, attention_mask)
            if output_all_encoded_layers:
                all_encoder_layers.append([hidden_states, attention_scores])
        if not output_all_encoded_layers:
            all_encoder_layers.append([hidden_states, attention_scores])
        return all_encoder_layers