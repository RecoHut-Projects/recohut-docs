# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/datasets/datasets.criteo.ipynb (unless otherwise specified).

__all__ = ['CriteoDataset', 'CriteoDataModule', 'CriteoSampleDataset', 'sparseFeature', 'denseFeature',
           'create_criteo_dataset']

# Cell
from .bases.ctr import *
from ..utils.common_utils import download_url

import pandas as pd
import numpy as np
import os
from datetime import datetime, date

# Cell
class CriteoDataset(CTRDataset):

    feature_cols = [{'name': ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13'],
                     'active': True, 'dtype': float, 'type': 'categorical', 'preprocess': 'convert_to_bucket', 'na_value': 0},
                    {'name': ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14','C15','C16','C17',
                              'C18','C19','C20','C21','C22','C23','C24','C25','C26'],
                     'active': True, 'dtype': str, 'type': 'categorical', 'na_value': ""}]

    label_col = {'name': 'Label', 'dtype': float}

    train_url = "https://github.com/RecoHut-Datasets/criteo/raw/v2/train.csv"
    valid_url = "https://github.com/RecoHut-Datasets/criteo/raw/v2/valid.csv"
    test_url = "https://github.com/RecoHut-Datasets/criteo/raw/v2/test.csv"

    @property
    def raw_file_names(self):
        return ['train.csv',
                'valid.csv',
                'test.csv']

    def download(self):
        download_url(self.train_url, self.raw_dir)
        download_url(self.valid_url, self.raw_dir)
        download_url(self.test_url, self.raw_dir)

    def convert_to_bucket(self, df, col_name):
        def _convert_to_bucket(value):
            if value > 2:
                value = int(np.floor(np.log(value) ** 2))
            else:
                value = int(value)
            return value
        return df[col_name].map(_convert_to_bucket).astype(int)

# Cell
class CriteoDataModule(CTRDataModule):
    dataset_cls = CriteoDataset

# Cell
import math
import shutil
import struct
from collections import defaultdict
from functools import lru_cache
from pathlib import Path

import lmdb
import numpy as np
import pandas as pd
import torch.utils.data
from tqdm import tqdm

import torch
from torch.utils.data import Dataset, TensorDataset

from .bases.common import Dataset
from ..utils.common_utils import download_url

# Cell
class CriteoSampleDataset(Dataset):
    """Criteo Sample Dataset

    Reference:
        1. https://github.com/huangjunheng/recommendation_model/blob/master/DCN/dcn.py
    """
    url = 'https://github.com/RecoHut-Datasets/criteo/raw/v1/dac_sample.txt'

    def __init__(self, root, test_size=0.2, random_seed=42):
        super().__init__(root)
        self.test_size = test_size
        self.random_seed = random_seed

        self._process()

    @property
    def raw_file_names(self) -> str:
        return 'dac_sample.txt'

    @property
    def processed_file_names(self) -> str:
        return ['train.pt', 'test.pt']

    def download(self):
        path = download_url(self.url, self.raw_dir)

    def process(self):
        sparse_feature = ['C' + str(i) for i in range(1, 27)]
        dense_feature = ['I' + str(i) for i in range(1, 14)]
        col_names = ['label'] + dense_feature + sparse_feature
        data = pd.read_csv(self.raw_paths[0], names=col_names, sep='\t')

        data[sparse_feature] = data[sparse_feature].fillna('-1', )
        data[dense_feature] = data[dense_feature].fillna('0',)

        feat_sizes = {}
        feat_sizes_dense = {feat:1 for feat in dense_feature}
        feat_sizes_sparse = {feat:len(data[feat].unique()) for feat in sparse_feature}
        feat_sizes.update(feat_sizes_dense)
        feat_sizes.update(feat_sizes_sparse)
        self.feat_sizes = feat_sizes

        from sklearn.preprocessing import LabelEncoder, MinMaxScaler
        for feat in sparse_feature:
            lbe = LabelEncoder()
            data[feat] = lbe.fit_transform(data[feat])
        nms = MinMaxScaler(feature_range=(0, 1))
        data[dense_feature] = nms.fit_transform(data[dense_feature])

        fixlen_feature_columns = [(feat,'sparse') for feat in sparse_feature ]  + [(feat,'dense') for feat in dense_feature]
        self.dnn_feature_columns = fixlen_feature_columns
        self.linear_feature_columns = fixlen_feature_columns

        from sklearn.model_selection import train_test_split
        train, test = train_test_split(data, test_size=self.test_size, random_state=self.random_seed)

        train_label = pd.DataFrame(train['label'])
        train = train.drop(columns=['label'])
        train_tensor_data = TensorDataset(torch.from_numpy(np.array(train)), torch.from_numpy(np.array(train_label)))
        torch.save(train_tensor_data, self.processed_paths[0])

        test_label = pd.DataFrame(test['label'])
        test = test.drop(columns=['label'])
        test_tensor_data = TensorDataset(torch.from_numpy(np.array(test)), torch.from_numpy(np.array(test_label)))
        torch.save(test_tensor_data, self.processed_paths[1])

    def load(self):
        train_tensor_data = torch.load(self.processed_paths[0])
        test_tensor_data = torch.load(self.processed_paths[1])
        return train_tensor_data, test_tensor_data

# Cell
import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer
from sklearn.model_selection import train_test_split

# Cell
def sparseFeature(feat, feat_num, embed_dim=4):
    """
    create dictionary for sparse feature
    :param feat: feature name
    :param feat_num: the total number of sparse features that do not repeat
    :param embed_dim: embedding dimension
    :return:
    """
    return {'feat_name': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}


def denseFeature(feat):
    """
    create dictionary for dense feature
    :param feat: dense feature name
    :return:
    """
    return {'feat_name': feat}

# Cell
def create_criteo_dataset(file, embed_dim=8, read_part=True, sample_num=100000, test_size=0.2):
    """
    a example about creating criteo dataset
    :param file: dataset's path
    :param embed_dim: the embedding dimension of sparse features
    :param read_part: whether to read part of it
    :param sample_num: the number of instances if read_part is True
    :param test_size: ratio of test dataset
    :return: feature columns, train, test
    """
    names = ['label', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11',
             'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',
             'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22',
             'C23', 'C24', 'C25', 'C26']

    if read_part:
        data_df = pd.read_csv(file, sep='\t', iterator=True, header=None,
                          names=names)
        data_df = data_df.get_chunk(sample_num)

    else:
        data_df = pd.read_csv(file, sep='\t', header=None, names=names)

    sparse_features = ['C' + str(i) for i in range(1, 27)]
    dense_features = ['I' + str(i) for i in range(1, 14)]
    features = sparse_features + dense_features

    data_df[sparse_features] = data_df[sparse_features].fillna('-1')
    data_df[dense_features] = data_df[dense_features].fillna(0)

    # Bin continuous data into intervals.
    est = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='uniform')
    data_df[dense_features] = est.fit_transform(data_df[dense_features])

    for feat in sparse_features:
        le = LabelEncoder()
        data_df[feat] = le.fit_transform(data_df[feat])

    # ==============Feature Engineering===================

    # ====================================================
    feature_columns = [sparseFeature(feat, int(data_df[feat].max()) + 1, embed_dim=embed_dim)
                        for feat in features]
    train, test = train_test_split(data_df, test_size=test_size)

    train_X = train[features].values.astype('int32')
    train_y = train['label'].values.astype('int32')
    test_X = test[features].values.astype('int32')
    test_y = test['label'].values.astype('int32')

    return feature_columns, (train_X, train_y), (test_X, test_y)