---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.7
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="9hnUHcuxJC-H" -->
# Data Poisoning Attack using LFM and ItemAE on Synthetic Dataset
<!-- #endregion -->

<!-- #region id="L1pij1c8NyAE" -->
Our goal is to promote an item of our choice. So we will poison a recommender model (victim) by feeding malicious data of fake users. So when the victim model will re-train itself on real+fake data next time, it will recommender our target time often. 

This fake data is carefully crafted by adversarial training.
<!-- #endregion -->

```python id="Fg71y33ez-hN"
!pip install higher
```

```python id="X9hWvjHKzp0b"
import random
from functools import partial
import numpy as np

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim

import higher
```

<!-- #region id="W5cIUqfe6h2A" -->
## Data
<!-- #endregion -->

<!-- #region id="tQ6knbxN6jR6" -->
### Synthetic Toy Dataset
<!-- #endregion -->

<!-- #region id="jqxWcEmp6oF6" -->
We synthesize a toy datset that is more controllable. Specifically, each data point in $X$ is generated by $x=\mu \nu^\top$, where both $\nu \in \mathcal{R}^d$ and $\mu\in \mathcal{R}^d$ are sampled from $\mathcal{N}_d(\mathbf{0}, \mathbf{I})$ with $d<<\min(|U|, |V|)$. By generating data point for $\forall x \in X$, the synthesized dataset is presumably to have low-rank, similar to other real-world recommendation datsets. Lastly, we binarize $X$ to transform it into implicit feedback data by setting a threshold $\epsilon$. By controlling the value of $(|U|,V|,d,\epsilon)$, we are able to have arbitrary-size synthetic datasets with different ranks and sparsity levels.
<!-- #endregion -->

<!-- #region id="7kUU7YUBzif3" -->
Define some hyperparameters for synthetic data generation
<!-- #endregion -->

```python id="mFqwuyK6zif4"
n_users = 1000        # number of total users (normal and fake), |U| + |V|
n_items = 300         # number of items |I|
n_fakes = 100         # number of fake user |V|

data_ranks = 20       # synthetic data rank d
binary_threshold = 5  # threshold epsilon

# Without loss of generality, let's suppose we're targeting the item `i0`.
# i.e., we want to push `i0` to true users' recommendation lists.
target_item = 0
```

<!-- #region id="P-fT129Dzif5" -->
Generate the synthetic data following the above procedure
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="E6plCLzl7PKs" executionInfo={"status": "ok", "timestamp": 1631972559794, "user_tz": -330, "elapsed": 10, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "13037694610922482904"}} outputId="a69a4ebe-fbac-4bdb-c004-99e1dcbd75fb"
dense_data_x = torch.mm(torch.randn(n_users, data_ranks),
                        torch.randn(data_ranks, n_items))
data_x = torch.zeros_like(dense_data_x)
test_items = np.empty([n_users - n_fakes], dtype=np.int64)

for i, x in enumerate(dense_data_x):
    # Apply thresholding to make synthetic data for implicit feedback.
    # it means the current user has interacted (views/bought/consumed etc.) with items
    # whose value is 1 and vica-versa
    x_binary = torch.where(x > binary_threshold,
                           torch.ones_like(x), torch.zeros_like(x))
    
    # For the true users, sample 1 item per user as the test item.
    nnz_inds = torch.nonzero(x_binary).view(-1)
    rand_ind = np.random.randint(0, nnz_inds.shape[0])
    test_item = nnz_inds[rand_ind]
    # Also we have to remove it from training data.
    if i < (n_users - n_fakes):
        x_binary[test_item] = 0.0
        test_items[i] = test_item.item()
    data_x[i] = x_binary

target_x = torch.zeros_like(data_x)
target_x[:, target_item] = 1.0

print(data_x)
print("data sparsity: %.2f" % (1 - data_x.sum().item() / data_x.view(-1).shape[0]))
print(target_x)
```

<!-- #region id="cN3wt2cPz8no" -->
## Models
<!-- #endregion -->

<!-- #region id="sjfhPQtFC8z8" -->
We have to learn 2 functions - A fake data generator and a recommender. We will first generate fake data of dimension a x b where a are no. of fake users and b are no. of items in the catalog.

After generating the fake user's data, we will train the recommender on real+fake data and evaluate its performance. i.e. How good it is in recommending our target item to real users. 

On the basis of this loss, we will update the gradients of both the functions/models (data generator and recommender).
<!-- #endregion -->

<!-- #region id="YJy9fLp3EERG" -->
**Solve the bi-level optimization problem to learn fake data for adversearial goal**

Given a well-trained surrogate model that is under-attack and a set of fake users $V = \{v_1, v_2,..,v_{|V|}\}$, the fake data $\widehat{X} \in \{0,1\}^{|V| \times |I|}$ will be learned to optimize an adversarial objective function ${\mathcal{L}_{\rm{adv}}}$:
$$
\min_{\widehat{X}}{\mathcal{L}_{\rm{adv}}}(R_{\theta^*}),
$$
$$
\textrm{subject to}\quad \theta^* = \text{argmin}_\theta \big ( {\mathcal{L}_{\rm{train}}}(X, R_{\theta}) + {\mathcal{L}_{\rm{train}}}(\widehat{X}, \widehat{R}_{\theta}) \big ),
$$

The inner objective shows that after fake data $\widehat{X}$ are injected, the surrogate model will first consume them (i.e., train from scratch with the poisoned dataset), we then obtain the trained model parameters $\theta^*$.
The outer objective shows that after fake data are consumed, we can achieve the malicious goal defined on normal user's predictions $R_{\theta^*}$.
<!-- #endregion -->

<!-- #region id="73KK14Cm0GU8" -->
### LFM
<!-- #endregion -->

<!-- #region id="1FQxoPRi0HQQ" -->
Standard Latent Factor Model
<!-- #endregion -->

```python id="uNWls4wU0J9C"
class LatentFactorModel(nn.Module):
    def __init__(self, n_users, n_items, hidden_dim):
        super(LatentFactorModel, self).__init__()

        self.n_users = n_users
        self.n_items = n_items
        self.dim = hidden_dim

        # Random-normal initialization of item embedding matrix
        self.V = nn.Parameter(
            torch.randn([self.n_items, self.dim]), requires_grad=True)
        
        # Random-normal initialization of user embedding matrix
        self.U = nn.Parameter(
            torch.randn([self.n_users, self.dim]), requires_grad=True)
        
        self.params = nn.ParameterList([self.V, self.U])

    def forward(self, user_id=None, item_id=None):
        if user_id is None and item_id is None:
            return torch.mm(self.U, self.V.t())
        if user_id is not None:
            return torch.mm(self.U[[user_id]], self.V.t())
        if item_id is not None:
            return torch.mm(self.U, self.V[[item_id]].t())
```

<!-- #region id="JNxK3pji1N-r" -->
### ItemAE
<!-- #endregion -->

<!-- #region id="GIerOJqG4x4I" -->
Approximate the adversarial gradient with parial derivatives.

this method uses itembased autoencoder optimized with Adam as surrogate model. The special design of ItemAE allows us to obtain non-zero partial derivatives. Thus, when accumulating adversarial gradients, we unroll either 0 steps (using only partial derivative) or 10% of total training steps.
<!-- #endregion -->

```python id="rHfEeDGl4vMZ"
class ItemAE(nn.Module):
    def __init__(self, input_dim, hidden_dims):
        super(ItemAE, self).__init__()
        self.q_dims = [input_dim] + hidden_dims
        self.p_dims = self.q_dims[::-1]

        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for
                                       d_in, d_out in
                                       zip(self.q_dims[:-1],
                                           self.q_dims[1:])])
        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for
                                       d_in, d_out in
                                       zip(self.p_dims[:-1],
                                           self.p_dims[1:])])

    def encode(self, input):
        h = input
        for i, layer in enumerate(self.q_layers):
            h = layer(h)
            h = F.selu(h)
        return h

    def decode(self, z):
        h = z
        for i, layer in enumerate(self.p_layers):
            h = layer(h)
            if i != len(self.p_layers) - 1:
                h = F.selu(h)
        return h

    def forward(self, data):
        z = self.encode(data.t())
        return self.decode(z).t()
```

<!-- #region id="sdbKjdj55l6p" -->
### Model factory
<!-- #endregion -->

```python id="lLhXAUUq5pdx"
def get_model(name="LFM"):
    model, inner_opt = None, None
    if name == "LFM":
        hidden_dim = 16
        model = LatentFactorModel(n_users, n_items, hidden_dim)
        inner_opt = optim.Adam(model.parameters(), lr=0.5)
    elif name == "ItemAE":
        hidden_dims = [64, 32]
        model = ItemAE(n_users, hidden_dims)
        inner_opt = optim.Adam(model.parameters(), lr=1e-2)

    return model, inner_opt
```

<!-- #region id="PErWFm2w5sv2" -->
## Utils
<!-- #endregion -->

```python id="5sdThiVV6DGv"
def set_random_seed(seed, use_cuda):
    random.seed(seed)
    np.random.seed(seed)
    if use_cuda:
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
    else:
        torch.manual_seed(seed)
```

```python id="AGn1kZdN6FKL"
def project_data(data, threshold=0.5):
    result = data.clone()
    return torch.where(result > threshold,
                       torch.ones_like(result), torch.zeros_like(result))
```

<!-- #region id="9RWAosZp6HHl" -->
## Trainer
<!-- #endregion -->

<!-- #region id="f-uFXbUB6RSh" -->
### Loss functions
<!-- #endregion -->

<!-- #region id="xDqHEupi6WbI" -->
We use the softmax loss as the adversarial objective $\mathcal{L}_{\rm{adv}}$ and the weighted MSE as the surrogate training objective $\mathcal{L}_{\rm{train}}$, one can also define & explore other objectives.
<!-- #endregion -->

```python id="vlH5gHQ86Tti"
def softmax_cross_entropy_with_logits(logits, targets):
    log_probs = F.log_softmax(logits, dim=1)
    loss = -log_probs * targets
    return loss.sum()

def mse_loss(logits, targets, weight=1):
    weights = torch.ones_like(logits)
    weights[targets > 0] = weight
    loss = weights * (targets - logits)**2
    return loss.sum()
```

<!-- #region id="s5wujoxF6dWt" -->
### Evaluation function
<!-- #endregion -->

```python id="knUeRxKS6NOT"
def evaluate(data_x, preds, test_items, target_item=None, cutoff=10):
    test_rankings = []
    target_rankings = []

    filtered_preds = preds.clone()
    filtered_preds.require_grad = False
    filtered_preds[torch.where(data_x)] = -np.inf
    rankings = filtered_preds.argsort(dim=1, descending=True).tolist()

    for user, test_item in enumerate(test_items):
        rank = rankings[user]
        test_rank = rank.index(test_item)
        test_rankings.append(test_rank)
        if target_item is not None:
            target_rank = rank.index(target_item)
            target_rankings.append(target_rank)
    test_rankings = np.asarray(test_rankings)
    target_rankings = np.asarray(target_rankings)

    mean_hr_test = (test_rankings < cutoff).mean()
    mean_rank_test = test_rankings.mean()

    result = {"hr@%d_test" % cutoff: "%.4f" % mean_hr_test,
              "mean_rank_test": "%.4f" % mean_rank_test}
    if target_item is not None:
        mean_hr_target = (target_rankings < cutoff).mean()
        mean_rank_target = target_rankings.mean()
        result["hr@%d_target" % cutoff] = "%.4f" % mean_hr_target
        result["mean_rank_target"] = "%.4f" % mean_rank_target
    return result
```

<!-- #region id="OBiJxZ_ZF2kv" -->
### Training Hyperparams
<!-- #endregion -->

```python id="bvnog7UUD8Xk"
seed = 1234           # random seed
use_cuda = False      # whether to use GPU
verbose = False       # whether to output full log

n_inner_iter = 100    # epochs for inner optimization
n_outer_iter = 50     # epochs for outer optimization
unroll_epochs = 100   # how many epochs to unroll

# Learning rate and momentum for outer objective optimizer, make sure
# they are carefully tuned for different surrogate and unroll_epochs.
outer_lr = 1.0
outer_momentum = 0.99

surrogate = "LFM"    # which surrogate model to use (LFM or ItemAE)
```

<!-- #region id="SMD6LimTF416" -->
### Training
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="kHAw1MgID-MK" executionInfo={"status": "ok", "timestamp": 1631972907822, "user_tz": -330, "elapsed": 292767, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "13037694610922482904"}} outputId="0ea288e9-c4d2-4b0f-ff55-4b23e4cdc273"
device = "cuda" if use_cuda else "cpu"
set_random_seed(seed, use_cuda)

inner_loss_func = partial(mse_loss, weight=20)
outer_loss_func = softmax_cross_entropy_with_logits

data_x.requires_grad_()
outer_opt = optim.SGD([data_x], lr=outer_lr, momentum=outer_momentum)

data_x = data_x.to(device)
target_x = target_x.to(device)

results = {"adv_loss": [], "hr@10_target": [], "hr@10_test": []}

for j in range(1, n_outer_iter + 1):
    torch.manual_seed(seed)
    model, inner_opt = get_model(surrogate)
    model = model.to(device)
    
    outer_opt.zero_grad()    
    ### Begin: Inner-level optimization
    # Train a few steps without tracking gradients.
    for i in range(1, n_inner_iter - unroll_epochs + 1):
        model.train()
        preds = model() if surrogate == "LFM" else model(data_x)
        inner_loss = inner_loss_func(preds, data_x)
        inner_opt.zero_grad()
        inner_loss.backward()
        inner_opt.step()
        # Evaluate recommendation performance along training steps.
        model.eval()
        result = evaluate(data_x, preds.detach(), test_items)
        if verbose and i % 10 == 0:
            print("inner epoch: %d, loss: %.2f, %s" % (
                i, inner_loss.item(), str(result)))
    # Start using tracking gradients.
    with higher.innerloop_ctx(model, inner_opt) as (fmodel, diffopt):
        if verbose:
            print("Switching to higher mode at epoch %d" % i)
        for i in range(n_inner_iter - unroll_epochs + 1, n_inner_iter + 1):
            fmodel.train()
            preds = fmodel() if surrogate == "LFM" else fmodel(data_x)
            inner_loss = inner_loss_func(preds, data_x)
            diffopt.step(inner_loss)
            # Evaluate recommendation performance along training steps.
            fmodel.eval()
            result = evaluate(data_x, preds.detach(), test_items)
            if verbose and i % 10 == 0:
                print("inner epoch: %d, loss: %.2f, %s" % (
                    i, inner_loss.item(), str(result)))
    ### End: Inner-level optimization
    
    ### Start: outer-level optimization
    preds = fmodel() if surrogate == "LFM" else fmodel(data_x)
    outer_loss = outer_loss_func(preds[:-n_fakes, ],
                                 target_x[:-n_fakes, ])
    results["adv_loss"].append(float("%.4f" % outer_loss.item()))
    grad_x = torch.autograd.grad(outer_loss, data_x)[0]
    # Only change the fake data by setting normal data gradient to 0.
    grad_x[:-n_fakes, :] = 0.0
    if data_x.grad is None:
        data_x.grad = grad_x
    else:
        data_x.grad.data = grad_x
    outer_opt.step()
    # Project fake data onto allowable area.
    data_x.data = project_data(data_x.data, 0.2)
    ### End: outer-level optimization
    
    # Evaluate the recommendation performance and attack performance.
    fmodel.eval()
    preds = fmodel() if surrogate == "LFM" else fmodel(data_x)
    result = evaluate(data_x, preds.detach(), test_items, target_item)
    results["hr@10_target"].append(float(result["hr@10_target"]))
    results["hr@10_test"].append(float(result["hr@10_test"]))
    print("outer epoch: %d, loss: %.2f, %s" % (
        j, outer_loss.item(), str(result)))
    print("-" * 100)
```

<!-- #region id="FGNdvWwtFqBW" -->
## Visualization
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 312} id="NFQ9NdruF-dd" executionInfo={"status": "ok", "timestamp": 1631972907831, "user_tz": -330, "elapsed": 75, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "13037694610922482904"}} outputId="68a631f1-97a9-4529-a1db-0f9341bbd188"
import matplotlib
import matplotlib.pyplot as plt

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,4))

ax1.plot(results['adv_loss'])
ax1.set_xlabel("iteration")
ax1.set_title("adversarial loss")

ax2.plot(results['hr@10_target'])
ax2.set_xlabel("iteration")
ax2.set_title("HR@10 for target item")

ax3.plot(results['hr@10_test'])
ax3.set_xlabel("iteration")
ax3.set_title("HR@10 for test item")
```
