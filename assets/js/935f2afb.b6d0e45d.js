"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:function(e){e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Concept - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Challenges","href":"/docs/concept-basics/challenges","docId":"concept-basics/challenges"},{"type":"link","label":"User Feedback","href":"/docs/concept-basics/implicit-feedback","docId":"concept-basics/implicit-feedback"},{"type":"link","label":"Processes","href":"/docs/concept-basics/processes","docId":"concept-basics/processes"},{"type":"link","label":"Session-based Recommenders","href":"/docs/concept-basics/session-based-recommenders","docId":"concept-basics/session-based-recommenders"},{"type":"link","label":"Tasks","href":"/docs/concept-basics/tasks","docId":"concept-basics/tasks"},{"type":"link","label":"Types of Recommender Systems","href":"/docs/concept-basics/types-of-recommender-systems","docId":"concept-basics/types-of-recommender-systems"}]},{"type":"category","label":"Concept - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Amazon Personalize","href":"/docs/concept-extras/amazon-personalize","docId":"concept-extras/amazon-personalize"},{"type":"link","label":"Bias & Fairness","href":"/docs/concept-extras/bias-&-fairness","docId":"concept-extras/bias-&-fairness"},{"type":"link","label":"Causal Inference","href":"/docs/concept-extras/causal-inference","docId":"concept-extras/causal-inference"},{"type":"link","label":"Cold Start","href":"/docs/concept-extras/cold-start","docId":"concept-extras/cold-start"},{"type":"link","label":"Cross-domain","href":"/docs/concept-extras/cross-domain","docId":"concept-extras/cross-domain"},{"type":"link","label":"Data Science","href":"/docs/concept-extras/data-science","docId":"concept-extras/data-science"},{"type":"link","label":"Diversity","href":"/docs/concept-extras/diversity","docId":"concept-extras/diversity"},{"type":"link","label":"Emerging Concepts in Recommender Systems","href":"/docs/concept-extras/emerging-concepts-in-recommender-systems","docId":"concept-extras/emerging-concepts-in-recommender-systems"},{"type":"link","label":"Graph Embeddings","href":"/docs/concept-extras/graph-embeddings","docId":"concept-extras/graph-embeddings"},{"type":"link","label":"Incremental Learning","href":"/docs/concept-extras/incremental-learning","docId":"concept-extras/incremental-learning"},{"type":"link","label":"Meta Learning","href":"/docs/concept-extras/meta-learning","docId":"concept-extras/meta-learning"},{"type":"link","label":"MLOps","href":"/docs/concept-extras/mlops","docId":"concept-extras/mlops"},{"type":"link","label":"Model Deployment","href":"/docs/concept-extras/model-deployment","docId":"concept-extras/model-deployment"},{"type":"category","label":"NLP","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chatbot","href":"/docs/concept-extras/nlp/chatbot","docId":"concept-extras/nlp/chatbot"},{"type":"link","label":"Language Modeling","href":"/docs/concept-extras/nlp/language-modeling","docId":"concept-extras/nlp/language-modeling"},{"type":"link","label":"Named Entity Recognition","href":"/docs/concept-extras/nlp/named-entity-recognition","docId":"concept-extras/nlp/named-entity-recognition"},{"type":"link","label":"Text Analysis","href":"/docs/concept-extras/nlp/text-analysis","docId":"concept-extras/nlp/text-analysis"},{"type":"link","label":"Text Classification","href":"/docs/concept-extras/nlp/text-classification","docId":"concept-extras/nlp/text-classification"},{"type":"link","label":"Text Generation","href":"/docs/concept-extras/nlp/text-generation","docId":"concept-extras/nlp/text-generation"},{"type":"link","label":"Text Similarity","href":"/docs/concept-extras/nlp/text-similarity","docId":"concept-extras/nlp/text-similarity"},{"type":"link","label":"Text Style Transfer","href":"/docs/concept-extras/nlp/text-style-transfer","docId":"concept-extras/nlp/text-style-transfer"},{"type":"link","label":"Text Summarization","href":"/docs/concept-extras/nlp/text-summarization","docId":"concept-extras/nlp/text-summarization"},{"type":"link","label":"Topic Modeling","href":"/docs/concept-extras/nlp/topic-modeling","docId":"concept-extras/nlp/topic-modeling"},{"type":"link","label":"Transformers","href":"/docs/concept-extras/nlp/transformers","docId":"concept-extras/nlp/transformers"}]},{"type":"category","label":"Success Stories","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"1mg Prod2vec","href":"/docs/concept-extras/success-stories/1mg-prod2vec","docId":"concept-extras/success-stories/1mg-prod2vec"},{"type":"link","label":"Airbnb Experiences","href":"/docs/concept-extras/success-stories/airbnb-experiences","docId":"concept-extras/success-stories/airbnb-experiences"},{"type":"link","label":"Alipay CTR","href":"/docs/concept-extras/success-stories/alipay-ctr","docId":"concept-extras/success-stories/alipay-ctr"},{"type":"link","label":"Doordash Contextual Bandit","href":"/docs/concept-extras/success-stories/doordash-contextual-bandit","docId":"concept-extras/success-stories/doordash-contextual-bandit"},{"type":"link","label":"Etsy Personalization","href":"/docs/concept-extras/success-stories/etsy-personalization","docId":"concept-extras/success-stories/etsy-personalization"},{"type":"link","label":"Huawei AppGallery","href":"/docs/concept-extras/success-stories/huawei-appgallery","docId":"concept-extras/success-stories/huawei-appgallery"},{"type":"link","label":"LinkedIn GLMix","href":"/docs/concept-extras/success-stories/linkedin-glmix","docId":"concept-extras/success-stories/linkedin-glmix"},{"type":"link","label":"MarketCloud Real-time","href":"/docs/concept-extras/success-stories/marketcloud-real-time","docId":"concept-extras/success-stories/marketcloud-real-time"},{"type":"link","label":"Netflix Personalize Images","href":"/docs/concept-extras/success-stories/netflix-personalize-images","docId":"concept-extras/success-stories/netflix-personalize-images"},{"type":"link","label":"Pinterest Multi-task Learning","href":"/docs/concept-extras/success-stories/pinterest-multi-task-learning","docId":"concept-extras/success-stories/pinterest-multi-task-learning"},{"type":"link","label":"Santander Banking Products","href":"/docs/concept-extras/success-stories/santander-banking-products","docId":"concept-extras/success-stories/santander-banking-products"},{"type":"link","label":"Scribd Real-time","href":"/docs/concept-extras/success-stories/scribd-real-time","docId":"concept-extras/success-stories/scribd-real-time"},{"type":"link","label":"Spotify Contextual Bandits","href":"/docs/concept-extras/success-stories/spotify-contextual-bandits","docId":"concept-extras/success-stories/spotify-contextual-bandits"},{"type":"link","label":"Spotify RL","href":"/docs/concept-extras/success-stories/spotify-rl","docId":"concept-extras/success-stories/spotify-rl"},{"type":"link","label":"StitchFix Multi-armed Bandit","href":"/docs/concept-extras/success-stories/stitchfix-multi-armed-bandit","docId":"concept-extras/success-stories/stitchfix-multi-armed-bandit"},{"type":"link","label":"Taobao BST","href":"/docs/concept-extras/success-stories/taobao-bst","docId":"concept-extras/success-stories/taobao-bst"},{"type":"link","label":"The Long Tail","href":"/docs/concept-extras/success-stories/the-long-tail","docId":"concept-extras/success-stories/the-long-tail"},{"type":"link","label":"UberEats Personalization","href":"/docs/concept-extras/success-stories/ubereats-personalization","docId":"concept-extras/success-stories/ubereats-personalization"},{"type":"link","label":"Walmart Model Selection","href":"/docs/concept-extras/success-stories/walmart-model-selection","docId":"concept-extras/success-stories/walmart-model-selection"}]},{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Facial Analytics","href":"/docs/concept-extras/vision/facial-analytics","docId":"concept-extras/vision/facial-analytics"},{"type":"link","label":"Image Segmentation","href":"/docs/concept-extras/vision/image-segmentation","docId":"concept-extras/vision/image-segmentation"},{"type":"link","label":"Image Similarity","href":"/docs/concept-extras/vision/image-similarity","docId":"concept-extras/vision/image-similarity"},{"type":"link","label":"Object Detection","href":"/docs/concept-extras/vision/object-detection","docId":"concept-extras/vision/object-detection"},{"type":"link","label":"Object Tracking","href":"/docs/concept-extras/vision/object-tracking","docId":"concept-extras/vision/object-tracking"},{"type":"link","label":"Pose Estimation","href":"/docs/concept-extras/vision/pose-estimation","docId":"concept-extras/vision/pose-estimation"},{"type":"link","label":"Scene Text Recognition","href":"/docs/concept-extras/vision/scene-text-recognition","docId":"concept-extras/vision/scene-text-recognition"},{"type":"link","label":"Video Action Recognition","href":"/docs/concept-extras/vision/video-action-recognition","docId":"concept-extras/vision/video-action-recognition"}]}]},{"type":"category","label":"Models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"A3C","href":"/docs/models/a3c","docId":"models/a3c"},{"type":"link","label":"AFM","href":"/docs/models/afm","docId":"models/afm"},{"type":"link","label":"AFN","href":"/docs/models/afn","docId":"models/afn"},{"type":"link","label":"AR","href":"/docs/models/ar","docId":"models/ar"},{"type":"link","label":"ASMG","href":"/docs/models/asmg","docId":"models/asmg"},{"type":"link","label":"AttRec","href":"/docs/models/attrec","docId":"models/attrec"},{"type":"link","label":"AutoInt","href":"/docs/models/autoint","docId":"models/autoint"},{"type":"link","label":"BCQ","href":"/docs/models/bcq","docId":"models/bcq"},{"type":"link","label":"Behavior Propensity Modeling","href":"/docs/models/beh-prop","docId":"models/beh-prop"},{"type":"link","label":"BiasOnly","href":"/docs/models/biasonly","docId":"models/biasonly"},{"type":"link","label":"BPR","href":"/docs/models/bpr","docId":"models/bpr"},{"type":"link","label":"CASER","href":"/docs/models/caser","docId":"models/caser"},{"type":"link","label":"DCN","href":"/docs/models/dcn","docId":"models/dcn"},{"type":"link","label":"DDPG","href":"/docs/models/ddpg","docId":"models/ddpg"},{"type":"link","label":"DeepCross","href":"/docs/models/deepcross","docId":"models/deepcross"},{"type":"link","label":"DeepFM","href":"/docs/models/deepfm","docId":"models/deepfm"},{"type":"link","label":"DeepWalk","href":"/docs/models/deepwalk","docId":"models/deepwalk"},{"type":"link","label":"DGTN","href":"/docs/models/dgtn","docId":"models/dgtn"},{"type":"link","label":"DQN","href":"/docs/models/dqn","docId":"models/dqn"},{"type":"link","label":"DRQN","href":"/docs/models/drqn","docId":"models/drqn"},{"type":"link","label":"DRR","href":"/docs/models/drr","docId":"models/drr"},{"type":"link","label":"Dueling DQN","href":"/docs/models/dueling-dqn","docId":"models/dueling-dqn"},{"type":"link","label":"FFM","href":"/docs/models/ffm","docId":"models/ffm"},{"type":"link","label":"FGNN","href":"/docs/models/fgnn","docId":"models/fgnn"},{"type":"link","label":"FM","href":"/docs/models/fm","docId":"models/fm"},{"type":"link","label":"GAT","href":"/docs/models/gat","docId":"models/gat"},{"type":"link","label":"GC-SAN","href":"/docs/models/gc-san","docId":"models/gc-san"},{"type":"link","label":"GCE-GNN","href":"/docs/models/gce-gnn","docId":"models/gce-gnn"},{"type":"link","label":"GRU4Rec","href":"/docs/models/gru4rec","docId":"models/gru4rec"},{"type":"link","label":"HMLET","href":"/docs/models/hmlet","docId":"models/hmlet"},{"type":"link","label":"ItemPop","href":"/docs/models/itempop","docId":"models/itempop"},{"type":"link","label":"LESSR","href":"/docs/models/lessr","docId":"models/lessr"},{"type":"link","label":"LightFM WARP","href":"/docs/models/lightfm-warp","docId":"models/lightfm-warp"},{"type":"link","label":"LightGCN","href":"/docs/models/lightgcn","docId":"models/lightgcn"},{"type":"link","label":"LIRD","href":"/docs/models/lird","docId":"models/lird"},{"type":"link","label":"Markov Chains","href":"/docs/models/markov-chains","docId":"models/markov-chains"},{"type":"link","label":"MB-GMN","href":"/docs/models/mb-gmn","docId":"models/mb-gmn"},{"type":"link","label":"MF","href":"/docs/models/mf","docId":"models/mf"},{"type":"link","label":"MIAN","href":"/docs/models/mian","docId":"models/mian"},{"type":"link","label":"NeuMF","href":"/docs/models/neumf","docId":"models/neumf"},{"type":"link","label":"NFM","href":"/docs/models/nfm","docId":"models/nfm"},{"type":"link","label":"NGCF","href":"/docs/models/ngcf","docId":"models/ngcf"},{"type":"link","label":"PNN","href":"/docs/models/pnn","docId":"models/pnn"},{"type":"link","label":"PPO","href":"/docs/models/ppo","docId":"models/ppo"},{"type":"link","label":"Q-learning","href":"/docs/models/q-learning","docId":"models/q-learning"},{"type":"link","label":"SAC","href":"/docs/models/sac","docId":"models/sac"},{"type":"link","label":"SARSA","href":"/docs/models/sarsa","docId":"models/sarsa"},{"type":"link","label":"SASRec","href":"/docs/models/sasrec","docId":"models/sasrec"},{"type":"link","label":"SGL","href":"/docs/models/sgl","docId":"models/sgl"},{"type":"link","label":"SiReN","href":"/docs/models/siren","docId":"models/siren"},{"type":"link","label":"SLIST","href":"/docs/models/slist","docId":"models/slist"},{"type":"link","label":"SPop","href":"/docs/models/spop","docId":"models/spop"},{"type":"link","label":"SR-GNN","href":"/docs/models/sr-gnn","docId":"models/sr-gnn"},{"type":"link","label":"SR-SAN","href":"/docs/models/sr-san","docId":"models/sr-san"},{"type":"link","label":"SR","href":"/docs/models/sr","docId":"models/sr"},{"type":"link","label":"STAMP","href":"/docs/models/stamp","docId":"models/stamp"},{"type":"link","label":"SVAE","href":"/docs/models/svae","docId":"models/svae"},{"type":"link","label":"TAGNN-PP","href":"/docs/models/tagnn-pp","docId":"models/tagnn-pp"},{"type":"link","label":"TAGNN","href":"/docs/models/tagnn","docId":"models/tagnn"},{"type":"link","label":"VNCF","href":"/docs/models/vncf","docId":"models/vncf"},{"type":"link","label":"VSKNN","href":"/docs/models/vsknn","docId":"models/vsknn"},{"type":"link","label":"Wide and Deep","href":"/docs/models/wide-and-deep","docId":"models/wide-and-deep"},{"type":"link","label":"Word2vec","href":"/docs/models/word2vec","docId":"models/word2vec"},{"type":"link","label":"xDeepFM","href":"/docs/models/xdeepfm","docId":"models/xdeepfm"}],"href":"/docs/models/"},{"type":"category","label":"Tutorials","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Science Bookcamp","href":"/docs/tutorials/data-science-bookcamp","docId":"tutorials/data-science-bookcamp"},{"type":"link","label":"Matching and Ranking models in Tensorflow","href":"/docs/tutorials/matching-and-ranking-models-in-tensorflow","docId":"tutorials/matching-and-ranking-models-in-tensorflow"},{"type":"link","label":"Real-time event capturing with Kafka and MongoDB","href":"/docs/tutorials/real-time-event-capturing-with-kafka-and-mongodb","docId":"tutorials/real-time-event-capturing-with-kafka-and-mongodb"},{"type":"link","label":"Session-based Recommendation with Graph Neural Networks","href":"/docs/tutorials/session-based-recommendation-with-graph-neural-net","docId":"tutorials/session-based-recommendation-with-graph-neural-net"},{"type":"link","label":"TensorFlow 2 Reinforcement Learning Cookbook","href":"/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook","docId":"tutorials/tensorflow-2-reinforcement-learning-cookbook"}],"href":"/docs/tutorials/"},{"type":"link","label":"Datasets","href":"/docs/datasets","docId":"datasets"},{"type":"link","label":"Projects","href":"/docs/projects","docId":"projects"}]},"docs":{"concept-basics/challenges":{"id":"concept-basics/challenges","title":"Challenges","description":"The construction of effective Recommender Systems (RS) is a complex process, mainly due to the nature of RSs which involves large scale software-systems and human interactions. Iterative development processes require deep understanding of a current baseline as well as the ability to estimate the impact of changes in multiple variables of interest. Simulations are well suited to address both challenges and potentially leading to a high velocity construction process, a fundamental requirement in commercial contexts. Recently, there has been significant interest in RS Simulation Platforms, which allow RS developers to easily craft simulated environments where their systems can be analyzed.","sidebar":"tutorialSidebar"},"concept-basics/implicit-feedback":{"id":"concept-basics/implicit-feedback","title":"User Feedback","description":"Explicit vs. implicit users feedback","sidebar":"tutorialSidebar"},"concept-basics/processes":{"id":"concept-basics/processes","title":"Processes","description":"Retrieval and Ranking","sidebar":"tutorialSidebar"},"concept-basics/session-based-recommenders":{"id":"concept-basics/session-based-recommenders","title":"Session-based Recommenders","description":"Recommender systems help users find relevant items of interest, for example on e-commerce or media streaming sites. Most academic research is concerned with approaches that personalize the recommendations according to long-term user profiles. In many real-world applications, however, such long-term profiles often do not exist and recommendations, therefore, have to be made solely based on the observed behavior of a user during an ongoing session.","sidebar":"tutorialSidebar"},"concept-basics/tasks":{"id":"concept-basics/tasks","title":"Tasks","description":"Top-K Recommendation","sidebar":"tutorialSidebar"},"concept-basics/types-of-recommender-systems":{"id":"concept-basics/types-of-recommender-systems","title":"Types of Recommender Systems","description":"Group Recommender System","sidebar":"tutorialSidebar"},"concept-extras/amazon-personalize":{"id":"concept-extras/amazon-personalize","title":"Amazon Personalize","description":"This page is a random dump of my notes on Amazon Personalize. Proceed accordingly.","sidebar":"tutorialSidebar"},"concept-extras/bias-&-fairness":{"id":"concept-extras/bias-&-fairness","title":"Bias & Fairness","description":"It can\u2019t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will \u201clearn\u201d these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and it\u2019s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutions\u2014because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions.","sidebar":"tutorialSidebar"},"concept-extras/causal-inference":{"id":"concept-extras/causal-inference","title":"Causal Inference","description":"Typical recommender systems frame the recommendation task as either a distance learning problem between pairs of products, or between pairs of users and products, or as a next item prediction problem. However, a recommender system should not only attempt to model organic user behavior but influence it. This is where causal techniques help, potentially via simple modifications of standard matrix factorization methods.","sidebar":"tutorialSidebar"},"concept-extras/cold-start":{"id":"concept-extras/cold-start","title":"Cold Start","description":"One long-standing challenge for Collaborative Filtering (CF) based recommendation methods is the cold start problem, i.e., to provide recommendations for new users or items who have no historical interaction record. The cold start problem is common in real world applications. For example, 500 hours of new videos are uploaded to YouTube every minute, 500,000 new users register in Facebook every day, and web/mobile apps face the daily challenge of onboarding new users and subscribers. To provide recommendations for these new users and items, many content-based methods and heuristic methods have been deployed, e.g., recommending popular items or geographically near items. However, recent research efforts that tackle the cold start problem from the perspective of machine learning have made promising strides.","sidebar":"tutorialSidebar"},"concept-extras/cross-domain":{"id":"concept-extras/cross-domain","title":"Cross-domain","description":"A common challenge for most current recommender systems is the cold-start problem. Due to the lack of user-item interactions, the fine-tuned recommender systems are unable to handle situations with new users or new items. Recently, some works introduce the meta-optimization idea into the recommendation scenarios, i.e. predicting the user preference by only a few of past interacted items. The core idea is learning a global sharing initialization parameter for all users and then learning the local parameters for each user separately. However, most meta-learning based recommendation approaches adopt model-agnostic meta-learning for parameter initialization, where the global sharing parameter may lead the model into local optima for some users.","sidebar":"tutorialSidebar"},"concept-extras/data-science":{"id":"concept-extras/data-science","title":"Data Science","description":"Data science is used in a variety of ways. Some data scientists focus on the analytics side of things, pulling out hidden patterns and insights from data, then communicating these results with visualizations and statistics. Others work on creating predictive models in order to predict future events, such as predicting whether someone will put solar panels on their house. Yet others work on models for classification; for example, classifying the make and model of a car in an image. One thing ties all applications of data science together: the data. Anywhere you have enough data, you can use data science to accomplish things that seem like magic to the casual observer.","sidebar":"tutorialSidebar"},"concept-extras/diversity":{"id":"concept-extras/diversity","title":"Diversity","description":"Individual-level diversity and System-level diversity","sidebar":"tutorialSidebar"},"concept-extras/emerging-concepts-in-recommender-systems":{"id":"concept-extras/emerging-concepts-in-recommender-systems","title":"Emerging Concepts in Recommender Systems","description":"Real-time Learning and Inference","sidebar":"tutorialSidebar"},"concept-extras/graph-embeddings":{"id":"concept-extras/graph-embeddings","title":"Graph Embeddings","description":"Due to their nature, graphs can be analyzed at different levels of granularity: at the node, edge, and graph level (the whole graph), as depicted in the following figure. For each of those levels, different problems could be faced and, as a consequence, specific algorithms should be used.","sidebar":"tutorialSidebar"},"concept-extras/incremental-learning":{"id":"concept-extras/incremental-learning","title":"Incremental Learning","description":"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference.","sidebar":"tutorialSidebar"},"concept-extras/meta-learning":{"id":"concept-extras/meta-learning","title":"Meta Learning","description":"Meta learning covers a wide range of topics and has contributed to a booming study trend. Few-shot learning is one of successful branches of meta learning. We retrospect some representative meta-learning models with strong connections to our work.","sidebar":"tutorialSidebar"},"concept-extras/mlops":{"id":"concept-extras/mlops","title":"MLOps","description":"The boom in AI has seen a rising demand for better AI infrastructure \u2014 both in the compute hardware layer and AI framework optimizations that make optimal use of accelerated compute. Unfortunately, organizations often overlook the critical importance of a middle tier: infrastructure software that standardizes the ML life cycle, adding a common platform for teams of data scientists and researchers to standardize their approach and eliminate distracting DevOps work. This process of building the ML life cycle is increasingly known as MLOps, with end-to-end platforms being built to automate and standardize repeatable manual processes. Although dozens of MLOps platforms exist, adopting one can be confusing and cumbersome. What should be considered when employing MLOps? What are the core pillars to MLOps, and which features are most critical?","sidebar":"tutorialSidebar"},"concept-extras/model-deployment":{"id":"concept-extras/model-deployment","title":"Model Deployment","description":"modeldeployment","sidebar":"tutorialSidebar"},"concept-extras/nlp/chatbot":{"id":"concept-extras/nlp/chatbot","title":"Chatbot","description":"/img/content-concepts-raw-nlp-chatbot-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/language-modeling":{"id":"concept-extras/nlp/language-modeling","title":"Language Modeling","description":"Language Models (LMs) estimate the probability of different linguistic units: symbols, tokens, token sequences.","sidebar":"tutorialSidebar"},"concept-extras/nlp/named-entity-recognition":{"id":"concept-extras/nlp/named-entity-recognition","title":"Named Entity Recognition","description":"/img/content-concepts-raw-nlp-named-entity-recognition-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-analysis":{"id":"concept-extras/nlp/text-analysis","title":"Text Analysis","description":"Rapid text analysis can save lives. Let\u2019s consider a real-world incident when US soldiers stormed a terrorist compound. In the compound, they discovered a computer containing terabytes of archived data. The data included documents, text messages, and emails pertaining to terrorist activities. The documents were too numerous to be read by any single human being. Fortunately, the soldiers were equipped with special software that could perform very fast text analysis. The software allowed the soldiers to process all of the text data without even having to leave the compound. The onsite analysis immediately revealed an active terrorist plot in a nearby neighborhood. The soldiers instantly responded to the plot and prevented a terrorist attack.","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-classification":{"id":"concept-extras/nlp/text-classification","title":"Text Classification","description":"/img/content-concepts-raw-text-classification-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-generation":{"id":"concept-extras/nlp/text-generation","title":"Text Generation","description":"Natural language generation (NLG) can actually tell a story \u2013 exactly like that of a human analyst \u2013 by writing the sentences and paragraphs for you. It can also summarize reports.","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-similarity":{"id":"concept-extras/nlp/text-similarity","title":"Text Similarity","description":"/img/content-concepts-raw-nlp-text-similarity-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-style-transfer":{"id":"concept-extras/nlp/text-style-transfer","title":"Text Style Transfer","description":"How to adapt the text to different situations, audiences and purposes by making some changes? The style of the text usually includes many aspects such as morphology, grammar, emotion, complexity, fluency, tense, tone and so on.","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-summarization":{"id":"concept-extras/nlp/text-summarization","title":"Text Summarization","description":"/img/content-concepts-raw-nlp-text-summarization-untitled.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/topic-modeling":{"id":"concept-extras/nlp/topic-modeling","title":"Topic Modeling","description":"/img/content-concepts-raw-nlp-topic-modeling-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/transformers":{"id":"concept-extras/nlp/transformers","title":"Transformers","description":"The old, obsolete, 1980 architecture of Recurrent Neural Networks(RNNs) including the LSTMs were simply not producing good results anymore. In less than two years, transformer models wiped RNNs off the map and even outperformed human baselines for many tasks.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/1mg-prod2vec":{"id":"concept-extras/success-stories/1mg-prod2vec","title":"1mg Prod2vec","description":"Gurgaon (India) based 1mg is an online pharmacy and healthcare platform that offers medicines, lab tests, and doctor consultations. Launched in 2013 as Healthkartplus, 1mg initially focussed on the alternative medicine space with AYUSH products. Over the years, it rebranded itself as 1mg which is an online pharmacy and healthcare platform that offers medicines, lab tests, and doctor consultations. Today, 1mg provides a wide range of healthcare services. 1mg also provides information on medicines. It facilitates lab tests at home. At present, the platform has about 2,000 tests and 120 verified labs listed and users can consult a doctor across 20 specialties. The company earns from its services like diagnostics, sale of medicines, preventive healthcare, and online consultations, as well as through native ads on its platform.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/airbnb-experiences":{"id":"concept-extras/success-stories/airbnb-experiences","title":"Airbnb Experiences","description":"Machine Learning-Powered Search Ranking of Airbnb Experiences","sidebar":"tutorialSidebar"},"concept-extras/success-stories/alipay-ctr":{"id":"concept-extras/success-stories/alipay-ctr","title":"Alipay CTR","description":"An online A/B testing was conducted in the production environment of Alipay for 10 days. The candidate items recommended to users include cash reward, coupons, prizes and member credits. The goal is to increase the CTR of the candidate items while constraining the total cost due to limited budget. The recommendation system serves at the scale of tens of millions of users in real traffic, hence the traffic is very expensive in the business view.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/doordash-contextual-bandit":{"id":"concept-extras/success-stories/doordash-contextual-bandit","title":"Doordash Contextual Bandit","description":"Read here on Doordash\'s blog","sidebar":"tutorialSidebar"},"concept-extras/success-stories/etsy-personalization":{"id":"concept-extras/success-stories/etsy-personalization","title":"Etsy Personalization","description":"Two-sided marketplaces such as eBay, Etsy and Taobao have two distinct groups of customers: buyers who use the platform to seek the most relevant and interesting item to purchase and sellers who view the same platform as a tool to reach out to their audience and grow their business. Additionally, platforms have their own objectives ranging from growing both buyer and seller user bases to revenue maximization. It is not difficult to see that it would be challenging to obtain a globally favorable outcome for all parties. Taking the search experience as an example, any interventions are likely to impact either buyers or sellers unfairly to course correct for a greater perceived need.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/huawei-appgallery":{"id":"concept-extras/success-stories/huawei-appgallery","title":"Huawei AppGallery","description":"Untitled","sidebar":"tutorialSidebar"},"concept-extras/success-stories/linkedin-glmix":{"id":"concept-extras/success-stories/linkedin-glmix","title":"LinkedIn GLMix","description":"A snapshot of the LinkedIn jobs homepage.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/marketcloud-real-time":{"id":"concept-extras/success-stories/marketcloud-real-time","title":"MarketCloud Real-time","description":"Model Training and Deployment","sidebar":"tutorialSidebar"},"concept-extras/success-stories/netflix-personalize-images":{"id":"concept-extras/success-stories/netflix-personalize-images","title":"Netflix Personalize Images","description":"Read here on Netflix\'s blog","sidebar":"tutorialSidebar"},"concept-extras/success-stories/pinterest-multi-task-learning":{"id":"concept-extras/success-stories/pinterest-multi-task-learning","title":"Pinterest Multi-task Learning","description":"Multi-task Learning for Related Products Recommendations at Pinterest","sidebar":"tutorialSidebar"},"concept-extras/success-stories/santander-banking-products":{"id":"concept-extras/success-stories/santander-banking-products","title":"Santander Banking Products","description":"The goal of the bank is to predict which new products customers will purchase. The data starts on 2015\u201301\u201328, and has monthly records of the products each customer has, such as a credit card, savings account, etc. In addition, the dataset also records user personal data such as average income, age, gender, and so on. The monthly statistics are provided until 2015\u201305\u201328. Finally, the model predicts which additional products a customer will start using from the following month, 2016\u201306\u201328. Thus, the dataset spans 17 months from 2015\u201301\u201328 to 2016\u201305\u201328, and the output set contains only the timestamp corresponding to 2016\u201306\u201328. Models are therefore trained on sequences of 16 months to predict products acquired on their respective last month.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/scribd-real-time":{"id":"concept-extras/success-stories/scribd-real-time","title":"Scribd Real-time","description":"Transformer based model architecture can be applied to recommendation applications as well but recommendation problems are a bit more complex than NLP domain so it needs to be adapted according to the business needs. Therefore, instead of predicting next word based on the past sequence of words, at Scribd, we are interested in predicting what user would like to read next based on rich user interaction history with multiple types of documents and multiple types of interactions, where position in sequence & relative time are both important factors.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/spotify-contextual-bandits":{"id":"concept-extras/success-stories/spotify-contextual-bandits","title":"Spotify Contextual Bandits","description":"Read more in this paper","sidebar":"tutorialSidebar"},"concept-extras/success-stories/spotify-rl":{"id":"concept-extras/success-stories/spotify-rl","title":"Spotify RL","description":"https://towardsdatascience.com/recommendation-system-with-reinforcement-learning-3362cb4422c8","sidebar":"tutorialSidebar"},"concept-extras/success-stories/stitchfix-multi-armed-bandit":{"id":"concept-extras/success-stories/stitchfix-multi-armed-bandit","title":"StitchFix Multi-armed Bandit","description":"Read more on official blog","sidebar":"tutorialSidebar"},"concept-extras/success-stories/taobao-bst":{"id":"concept-extras/success-stories/taobao-bst","title":"Taobao BST","description":"The BST has been deployed in rank stage for Taobao recommendation, which provides recommending service for hundreds of millions of consumers everyday. In this paper, Alibaba described its usage and technical details in Taobao platform.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/the-long-tail":{"id":"concept-extras/success-stories/the-long-tail","title":"The Long Tail","description":"Here is an excerpt from the book The Long Tail by Chris Anderson: \\"In 1988, a British mountain climber named Joe Simpson wrote a book called Touching the Void, a harrowing account of near-death in the Peruvian Andes. It got good reviews but, only a modest success, it was soon forgotten. Then, a decade later, a strange thing happened. Jon Krakauer wrote Into Thin Air, another book about a mountain-c\xe4mbing tragedy, which became a publishing sensation. Suddenly Touching the Void started to sell again\\".","sidebar":"tutorialSidebar"},"concept-extras/success-stories/ubereats-personalization":{"id":"concept-extras/success-stories/ubereats-personalization","title":"UberEats Personalization","description":"Food Discovery with Uber Eats: Recommending for the Marketplace","sidebar":"tutorialSidebar"},"concept-extras/success-stories/walmart-model-selection":{"id":"concept-extras/success-stories/walmart-model-selection","title":"Walmart Model Selection","description":"In this paper, Walmart researchers briefly discussed their approach and experiment details.","sidebar":"tutorialSidebar"},"concept-extras/vision/facial-analytics":{"id":"concept-extras/vision/facial-analytics","title":"Facial Analytics","description":"/img/content-concepts-raw-computer-vision-facial-analytics-img.png","sidebar":"tutorialSidebar"},"concept-extras/vision/image-segmentation":{"id":"concept-extras/vision/image-segmentation","title":"Image Segmentation","description":"/img/content-concepts-raw-computer-vision-image-segmentation-slide46.png","sidebar":"tutorialSidebar"},"concept-extras/vision/image-similarity":{"id":"concept-extras/vision/image-similarity","title":"Image Similarity","description":"/img/content-concepts-raw-computer-vision-image-similarity-slide19.png","sidebar":"tutorialSidebar"},"concept-extras/vision/object-detection":{"id":"concept-extras/vision/object-detection","title":"Object Detection","description":"/img/content-concepts-raw-computer-vision-object-detection-slide29.png","sidebar":"tutorialSidebar"},"concept-extras/vision/object-tracking":{"id":"concept-extras/vision/object-tracking","title":"Object Tracking","description":"/img/content-concepts-raw-computer-vision-object-tracking-img.png","sidebar":"tutorialSidebar"},"concept-extras/vision/pose-estimation":{"id":"concept-extras/vision/pose-estimation","title":"Pose Estimation","description":"/img/content-concepts-raw-computer-vision-pose-estimation-slide52.png","sidebar":"tutorialSidebar"},"concept-extras/vision/scene-text-recognition":{"id":"concept-extras/vision/scene-text-recognition","title":"Scene Text Recognition","description":"/img/content-concepts-raw-computer-vision-scene-text-recognition-img.png","sidebar":"tutorialSidebar"},"concept-extras/vision/video-action-recognition":{"id":"concept-extras/vision/video-action-recognition","title":"Video Action Recognition","description":"/img/content-concepts-raw-computer-vision-video-action-recognition-img.png","sidebar":"tutorialSidebar"},"datasets":{"id":"datasets","title":"Datasets","description":"| Title | Link | Description |","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"It is evident that the pace that technology advances have been increased over the last decades. Scientific discoveries and technological growth introduced to people a huge variety of options and possibilities. One of the most important advantages that technology offers is the direct and easy access to information. Nowadays access to vast networks of information is easy and people can be informed about almost anything they desire. Even though ease of access provided people with the ability to acquire the needed information, they are now facing a new obstacle: this of easily finding what they need. On one hand, information abundance covers the majority of needs but on the other hinders accessibility to information truly valuable to the user. The term that describes this phenomenon is \u201cInformation Overload\u201d. Often users are presented with seemingly similar information to their inquiry but irrelevant to their actual needs, rendering this way the discovery of the desired knowledge a difficult task. Continuous expanding of information overload necessitated the development of systems that aim to alleviate such problems. Such systems were introduced in order to filter or retrieve the desired information. Recommendation systems is an example. Recommenders aim to filter out all the unnecessary and irrelevant information and present those that fit the user\u2019s needs. This way the user is relieved of the burden of discovering what he needs making this way information truly accessible.","sidebar":"tutorialSidebar"},"models/a3c":{"id":"models/a3c","title":"A3C","description":"A3C stands for Asynchronous Advantage Actor-Critic. The A3C algorithm builds upon the Actor-Critic class of algorithms by using a neural network to approximate the actor (and critic). The actor learns the policy function using a deep neural network, while the critic estimates the value function. The asynchronous nature of the algorithm allows the agent to learn from different parts of the state space, allowing parallel learning and faster convergence. Unlike DQN agents, which use an experience replay memory, the A3C agent uses multiple workers to gather more samples for learning.","sidebar":"tutorialSidebar"},"models/afm":{"id":"models/afm","title":"AFM","description":"AFM stands for Attentional Factorization Machines. It Improves FM by discriminating the importance of different feature interactions, and learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task that AFM performs betters than FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters.","sidebar":"tutorialSidebar"},"models/afn":{"id":"models/afn","title":"AFN","description":"AFN stands for Adaptive Factorization Network.","sidebar":"tutorialSidebar"},"models/ar":{"id":"models/ar","title":"AR","description":"Simple Association Rules (AR) are a simplified version of the association rule mining technique [Agrawal et al. 1993] with a maximum rule size of two. The method is designed to capture the frequency of two co-occurring events, e.g., \u201cCustomers who bought . . . also bought\u201d.","sidebar":"tutorialSidebar"},"models/asmg":{"id":"models/asmg","title":"ASMG","description":"ASMG stands for Adaptive Sequential Model Generation.","sidebar":"tutorialSidebar"},"models/attrec":{"id":"models/attrec","title":"AttRec","description":"AttRec stands for Self-Attentive Sequential Recommendation.","sidebar":"tutorialSidebar"},"models/autoint":{"id":"models/autoint","title":"AutoInt","description":"Song et. al., \u201cAutomatic Feature Interaction Learning via Self-Attentive Neural Networks\u201d. CIKM, 2018.","sidebar":"tutorialSidebar"},"models/bcq":{"id":"models/bcq","title":"BCQ","description":"Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available.","sidebar":"tutorialSidebar"},"models/beh-prop":{"id":"models/beh-prop","title":"Behavior Propensity Modeling","description":"behavior-propensity-modeling","sidebar":"tutorialSidebar"},"models/biasonly":{"id":"models/biasonly","title":"BiasOnly","description":"BiasOnly is a simple baseline that assumes no interactions between users and items. Formally, it learns: (1) a global bias \ud835\udefc; (2) scalar biases $\\\\betau$ for each user \ud835\udc62 \u2208 U; and (3) scalar biases $\\\\betai$ for each item \ud835\udc56 \u2208 I. Ultimately, the rating/relevance for user \ud835\udc62 and item \ud835\udc56 is modeled as $\\\\hati^u = \\\\alpha + \\\\betau + \\\\beta_i$.","sidebar":"tutorialSidebar"},"models/bpr":{"id":"models/bpr","title":"BPR","description":"BPR stands for Bayesian Personalized Ranking. In matrix factorization (MF), to compute the prediction we have to multiply the user factors to the item factors:","sidebar":"tutorialSidebar"},"models/caser":{"id":"models/caser","title":"CASER","description":"CASER stands for Convolutional Sequence Embedding Recommendation. Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a \'near future\'. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. Convolutional Sequence Embedding Recommendation Model (Caser) address this requirement by embedding a sequence of recent items into an image\' in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. In other words, Caser adopts convolutional neural networks capture the dynamic pattern influences of users\u2019 recent activities.","sidebar":"tutorialSidebar"},"models/dcn":{"id":"models/dcn","title":"DCN","description":"DCN stands for Deep and Cross Network. Manual explicit feature crossing process is very laborious and inefficient. On the other hand, automatic implicit feature crossing methods like MLPs cannot efficiently approximate even 2nd or 3rd-order feature crosses. Deep-cross networks provides a solution to this problem. DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions.","sidebar":"tutorialSidebar"},"models/ddpg":{"id":"models/ddpg","title":"DDPG","description":"Deterministic Policy Gradient (DPG)\xa0is a type\xa0of Actor-Critic RL algorithm that uses two neural networks: one for estimating the action value function, and the other for estimating the optimal target policy. The\xa0Deep Deterministic Policy Gradient\xa0(DDPG) agent\xa0builds upon the idea of DPG and is quite efficient compared to vanilla Actor-Critic agents due\xa0to the use\xa0of deterministic action policies.","sidebar":"tutorialSidebar"},"models/deepcross":{"id":"models/deepcross","title":"DeepCross","description":"Shan, Y., Hoens, T., Jiao, J., Wang, H., Yu, D. and Mao, J., 2016.\xa0Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features. [online] Kdd.org.","sidebar":"tutorialSidebar"},"models/deepfm":{"id":"models/deepfm","title":"DeepFM","description":"DeepFM stands for Deep Factorization Machines. It consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction.","sidebar":"tutorialSidebar"},"models/deepwalk":{"id":"models/deepwalk","title":"DeepWalk","description":"DeepWalk learns representations of online social networks graphs. By performing random walks to generate sequences, the paper demonstrated that it was able to learn vector representations of nodes (e.g., profiles, content) in the graph.","sidebar":"tutorialSidebar"},"models/dgtn":{"id":"models/dgtn","title":"DGTN","description":"DGTN stands for Dual-channel Graph Transition Network.","sidebar":"tutorialSidebar"},"models/dqn":{"id":"models/dqn","title":"DQN","description":"The\xa0Q-learning component of DQN was invented in 1989 by Christopher Watkins in his PhD thesis titled \u201cLearning from Delayed Rewards\u201d. Experience replay quickly followed, invented by Long-Ji Lin in 1992. This played a major role in improving the efficiency of\xa0Q-learning. In the years that followed, however, there were no major success stories involving deep\xa0Q-learning. This is perhaps not surprising given the combination of limited computational power in the 1990s and early 2000s, data-hungry deep learning architectures, and the sparse, noisy, and delayed feedback signals experienced in RL. Progress had to wait for the emergence of general-purpose GPU programming, for example with the launch of CUDA in 2006, and the reignition of interest in deep learning within the machine learning community that began in the mid-2000s and rapidly accelerated after 2012.","sidebar":"tutorialSidebar"},"models/drqn":{"id":"models/drqn","title":"DRQN","description":"DRQN stands for Deep Recurrent Q-Learning. It is a combination of a recurrent neural network (RNN) and a deep Q-network (DQN). The idea being that the RNN will be able to retain information from states further back in time and incorporate that into predicting better Q values and thus performing better on games that require long term planning.","sidebar":"tutorialSidebar"},"models/drr":{"id":"models/drr","title":"DRR","description":"DRR Framework","sidebar":"tutorialSidebar"},"models/dueling-dqn":{"id":"models/dueling-dqn","title":"Dueling DQN","description":"A\xa0Dueling DQN agent explicitly estimates two quantities through a modified network architecture:","sidebar":"tutorialSidebar"},"models/ffm":{"id":"models/ffm","title":"FFM","description":"FFM stands for Field-aware Factorization Machines. In the official FFM paper, it is empirically proven that for large, sparse datasets with many categorical features, FFM performs better. Conversely, for small and dense datasets or numerical datasets, FFM may not be as effective as FM. FFM is also prone to overfitting on the training dataset, hence one should use a standalone validation set and use early stopping when the loss increases.","sidebar":"tutorialSidebar"},"models/fgnn":{"id":"models/fgnn","title":"FGNN","description":"Ruihong Qiu, Jingjing Li, Zi Huang and Hongzhi Yin, \u201cRethinking the Item Order in Session-based Recommendation with Graph Neural Networks\u201d. CIKM, 2019.","sidebar":"tutorialSidebar"},"models/fm":{"id":"models/fm","title":"FM","description":"Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.","sidebar":"tutorialSidebar"},"models/gat":{"id":"models/gat","title":"GAT","description":"GAT stands for Graph Attention Networks. This is a special GNN model that addresses several key challenges of spectral models, such as poor ability of generalization from a specific graph structure to another and sophisticated computation of matrix inverse. GAT utilizes attention mechanisms to aggregate neighborhood features (embeddings) by specifying different weights to different nodes.","sidebar":"tutorialSidebar"},"models/gc-san":{"id":"models/gc-san","title":"GC-SAN","description":"GC-SAN stands for Graph contextualized self-attention.","sidebar":"tutorialSidebar"},"models/gce-gnn":{"id":"models/gce-gnn","title":"GCE-GNN","description":"GCE-GNN stands for Global Context Enhanced Graph Neural Networks. It exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session.","sidebar":"tutorialSidebar"},"models/gru4rec":{"id":"models/gru4rec","title":"GRU4Rec","description":"It uses session-parallel mini-batch approach where we first create an order for the sessions and then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs.","sidebar":"tutorialSidebar"},"models/hmlet":{"id":"models/hmlet","title":"HMLET","description":"HMLET stands for Hybrid Method of Linear and nonlinEar collaborative filTering (HMLET, pronounced as Hamlet). It is a GCN-based CF method.","sidebar":"tutorialSidebar"},"models/itempop":{"id":"models/itempop","title":"ItemPop","description":"Itempop is a na\xefve baseline that simply ranks items according to overall train-set popularity. Note that this method is unaffected by the user for which items are being recommended, and has the same global ranking of all items","sidebar":"tutorialSidebar"},"models/lessr":{"id":"models/lessr","title":"LESSR","description":"Tianwen Chen and Raymond Chi-Wing Wong, \u201cLESSR: Handling Information Loss of Graph Neural Networks for Session-based Recommendation\u201d. KDD, 2020.","sidebar":"tutorialSidebar"},"models/lightfm-warp":{"id":"models/lightfm-warp","title":"LightFM WARP","description":"LightFM is probably the only recommender package implementing the WARP (Weighted Approximate-Rank Pairwise) loss for implicit feedback learning-to-rank. Generally, it performs better than the more popular BPR (Bayesian Personalised Ranking) loss --- often by a large margin.","sidebar":"tutorialSidebar"},"models/lightgcn":{"id":"models/lightgcn","title":"LightGCN","description":"GCN is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is defined as follows:","sidebar":"tutorialSidebar"},"models/lird":{"id":"models/lird","title":"LIRD","description":"Existing reinforcement learning recommender methods also could recommend a list of items. E.g. DQN can calculate Q-values of all recalled items separately, and recommend a list of items with highest Q-values. But these recommendations are similar in Euclidean space and we want to find similarity in associative space. For instance, for a bread \ud83c\udf5e, I want egg \ud83e\udd5a, milk \ud83e\udd5b in my recommendation list instead of white bread \ud83c\udf5e, brown bread \ud83e\udd6a, bun \ud83e\uded3 etc.","sidebar":"tutorialSidebar"},"models/markov-chains":{"id":"models/markov-chains","title":"Markov Chains","description":"Markov chains, named after Andrey Markov, are mathematical systems that hop from one \\"state\\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby\'s behavior, you might include \\"playing,\\" \\"eating\\", \\"sleeping,\\" and \\"crying\\" as states, which together with other behaviors could form a \'state space\': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probability of hopping, or \\"transitioning,\\" from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.","sidebar":"tutorialSidebar"},"models/mb-gmn":{"id":"models/mb-gmn","title":"MB-GMN","description":"MB-GMN stands for Multi-behavior pattern modeling with meta-knowledge learner.","sidebar":"tutorialSidebar"},"models/mf":{"id":"models/mf","title":"MF","description":"Matrix Factorization is an iterative approach of SVD called Regularized SVD. It uses the gradient-descent method to estimate the resulting matrices. The obtained model will not be a true SVD of the rating-matrix, as the component matrices are no longer orthogonal, but tends to be more accurate at predicting unseen preferences than the standard SVD [Ekstrand et al. 2011].","sidebar":"tutorialSidebar"},"models/mian":{"id":"models/mian","title":"MIAN","description":"MIAN stands for Multi-Interactive Attention Network. It aggregate multiple information, and gain latent representations through interactions between candidate items and other fine-grained features.","sidebar":"tutorialSidebar"},"models/models":{"id":"models/models","title":"Models","description":"CTR Prediction Models","sidebar":"tutorialSidebar"},"models/neumf":{"id":"models/neumf","title":"NeuMF","description":"NMF Leverages the representation power of deep neural-networks to capture nonlinear correlations between user and item embeddings. Formally, the rating/relevance for user \ud835\udc62 and item \ud835\udc56 is modeled as $\\\\hati^u = \\\\alpha + \\\\betau + \\\\betai + f(\\\\gammau || \\\\gammai || \\\\gammau \\\\cdot \\\\gammai)$ where $\\\\gammau , \\\\gamma_i \\\\in \\\\mathbb{R}^d$, \u2018||\u2019 represents the concatenation operation, and $f: \\\\mathbb{R}^{3d} \\\\rightarrow \\\\mathbb{R}$ represents an arbitrarily complex neural network.","sidebar":"tutorialSidebar"},"models/nfm":{"id":"models/nfm","title":"NFM","description":"NFM stands for Neural Factorization Machine.","sidebar":"tutorialSidebar"},"models/ngcf":{"id":"models/ngcf","title":"NGCF","description":"NGCF stands for Neural Graph Collaborative Filtering. This GNN-based approach follows basic operations inherited from the standard GCN to explore the high-order connectivity information. More specifically, NGCF stacks embedding layers and concatenates embeddings obtained in all layers to constitute the final embeddings.","sidebar":"tutorialSidebar"},"models/pnn":{"id":"models/pnn","title":"PNN","description":"PNN stands for Product-based Neural Network.","sidebar":"tutorialSidebar"},"models/ppo":{"id":"models/ppo","title":"PPO","description":"The PPO (Proximal Policy Optimization) algorithm was\xa0introduced by the OpenAI team in 2017\xa0and quickly became one of the most popular Reinforcement Learning method that pushed all other RL methods at that moment aside. PPO involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with that batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an \u201con-policy learning\u201d approach where the experience samples collected are only useful for updating the current policy.","sidebar":"tutorialSidebar"},"models/q-learning":{"id":"models/q-learning","title":"Q-learning","description":"Q-learning can be applied to model-free RL problems. It supports off-policy learning and therefore provides a practical solution to problems where available experiences were/are collected using some other policy or by some other agent (even humans).","sidebar":"tutorialSidebar"},"models/sac":{"id":"models/sac","title":"SAC","description":"SAS stands for Soft Actor-Critic. It not only boasts of being more sample efficient than traditional RL algorithms but also promises to be robust to brittleness in convergence.","sidebar":"tutorialSidebar"},"models/sarsa":{"id":"models/sarsa","title":"SARSA","description":"The SARSA algorithm can be applied to model-free control problems and allows us to optimize the value function of an unknown MDP. SARSA is an on-policy\xa0temporal difference learning-based control algorithm. The SARSA algorithm can be summarized as follows:","sidebar":"tutorialSidebar"},"models/sasrec":{"id":"models/sasrec","title":"SASRec","description":"SASRec stands for Self-Attentive Sequential Recommendation. It relies on the sequence modeling capabilities of self-attentive neural networks to predict the occurence of the next item in a user\u2019s consumption sequence. To be precise, given a user \ud835\udc62 and their time-ordered consumption history $S^\ud835\udc62 = (S1^u, S2^u, \\\\dots, S_{|S^u|}^\ud835\udc62),$ SASRec first applies self-attention on $S^\ud835\udc62$ followed by a series of non-linear feed-forward layers to finally obtain the next item likelihood.","sidebar":"tutorialSidebar"},"models/sgl":{"id":"models/sgl","title":"SGL","description":"SGL is the latest baseline for top-k recommendations. It introduces self-supervised learning into the recommendation system based on the contrastive learning framework. It is implemented on LightGCN and uses a multitask approach that unites the contrastive loss and the BPR loss function. SGL mainly benefits from graph contrastive learning to reinforce user and item representations.","sidebar":"tutorialSidebar"},"models/siren":{"id":"models/siren","title":"SiReN","description":"Existing literature often ignores the negative feedback e.g. dislikes on YouTube videos, and only capture the homophily (or assortativity) patterns by positive feedback. This is a missed opportunity situation. Performance of GNN-based Recommender Systems can be improved by including negative feedbacks. Disassortivity patterns can be learned by negative feedback. LightGCN can capture the assortativity patterns. and the MLP network can capture the disassortivity patterns.","sidebar":"tutorialSidebar"},"models/slist":{"id":"models/slist","title":"SLIST","description":"SLIST stands for Session-aware Linear Similarity/Transition. It is built by unifying two linear models - SLIS and SLIT.","sidebar":"tutorialSidebar"},"models/spop":{"id":"models/spop","title":"SPop","description":"SPop stands for Session-based Popularity. It is a session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.","sidebar":"tutorialSidebar"},"models/sr":{"id":"models/sr","title":"SR","description":"SR stands for Sequential Rules. The SR method is a variation of MC and AR. It also takes the order of actions into account, but in a less restrictive manner. In contrast to the MC method, we create a rule when an item q appeared after an item p in a session even when other events happened between p and q. When assigning weights to the rules, we consider the number of elements appearing between p and q in the session.","sidebar":"tutorialSidebar"},"models/sr-gnn":{"id":"models/sr-gnn","title":"SR-GNN","description":"SR-GNN stands for Session-based Recommendation with Graph Neural Networks.","sidebar":"tutorialSidebar"},"models/sr-san":{"id":"models/sr-san","title":"SR-SAN","description":"SR-SAN stands for Session-based Recommendation with Self-Attention Networks.","sidebar":"tutorialSidebar"},"models/stamp":{"id":"models/stamp","title":"STAMP","description":"/img/content-models-raw-mp2-stamp-untitled.png","sidebar":"tutorialSidebar"},"models/svae":{"id":"models/svae","title":"SVAE","description":"SVAE stands for Sequential Variational Autoencoder.","sidebar":"tutorialSidebar"},"models/tagnn":{"id":"models/tagnn","title":"TAGNN","description":"TAGNN stands for Target Attentive Graph Neural Network. Session-based recommendations are challenging due to limited user-item interactions. Typical sequential models are not able to capture complex patterns from all previous interactions. SessionGraph (a graph representation of sessions) can capture the complex patterns from all previous interactions.","sidebar":"tutorialSidebar"},"models/tagnn-pp":{"id":"models/tagnn-pp","title":"TAGNN-PP","description":"TAGNN-PP models item interactions with GNN, and both local and global user interactions with  a Transformer.","sidebar":"tutorialSidebar"},"models/vncf":{"id":"models/vncf","title":"VNCF","description":"VNCF stands for Variational Neural Collaborative Filtering.","sidebar":"tutorialSidebar"},"models/vsknn":{"id":"models/vsknn","title":"VSKNN","description":"VSKNN stands for Vector Multiplication Session-Based kNN. The idea of this variant is to put more emphasis on the more recent events of a session when computing the similarities. Instead of encoding a session as a binary vector, we use real-valued vectors to encode the current session. Only the very last element of the session obtains a value of \u201c1\u201d; the weights of the other elements are determined using a linear decay function that depends on the position of the element within the session, where elements appearing earlier in the session obtain a lower weight. As a result, when using the dot product as a similarity function between the current weight-encoded session and a binary-encoded past session, more emphasis is given to elements that appear later in the sessions.","sidebar":"tutorialSidebar"},"models/wide-and-deep":{"id":"models/wide-and-deep","title":"Wide and Deep","description":"Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It\'s useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.","sidebar":"tutorialSidebar"},"models/word2vec":{"id":"models/word2vec","title":"Word2vec","description":"/img/content-models-raw-mp2-word2vec-untitled.png","sidebar":"tutorialSidebar"},"models/xdeepfm":{"id":"models/xdeepfm","title":"xDeepFM","description":"xDeepFM stands for Extreme Deep Factorization Machines.","sidebar":"tutorialSidebar"},"projects":{"id":"projects","title":"Projects","description":"| Project Id | Title | Links |","sidebar":"tutorialSidebar"},"tutorials/data-science-bookcamp":{"id":"tutorials/data-science-bookcamp","title":"Data Science Bookcamp","description":"Key points","sidebar":"tutorialSidebar"},"tutorials/matching-and-ranking-models-in-tensorflow":{"id":"tutorials/matching-and-ranking-models-in-tensorflow","title":"Matching and Ranking models in Tensorflow","description":"Matching models on ML-1m and ranking models on Criteo (sample) dataset in TF v2.5","sidebar":"tutorialSidebar"},"tutorials/real-time-event-capturing-with-kafka-and-mongodb":{"id":"tutorials/real-time-event-capturing-with-kafka-and-mongodb","title":"Real-time event capturing with Kafka and MongoDB","description":"| | |","sidebar":"tutorialSidebar"},"tutorials/session-based-recommendation-with-graph-neural-net":{"id":"tutorials/session-based-recommendation-with-graph-neural-net","title":"Session-based Recommendation with Graph Neural Networks","description":"Session-based recommendation tasks are performed based on the user\'s anonymous historical behavior sequence and implicit feedback data, such as clicks, browsing, purchasing, etc., rather than rating or comment data. The primary aim is to predict the next behavior based on a sequence of the historical sequence of the session. Session-based recommendation aims to predict which item a user will click next, solely based on the user\u2019s current sequential session data without access to the long-term preference profile.","sidebar":"tutorialSidebar"},"tutorials/tensorflow-2-reinforcement-learning-cookbook":{"id":"tutorials/tensorflow-2-reinforcement-learning-cookbook","title":"TensorFlow 2 Reinforcement Learning Cookbook","description":"Process flow","sidebar":"tutorialSidebar"},"tutorials/tutorials":{"id":"tutorials/tutorials","title":"Tutorials","description":"Data pipeline","sidebar":"tutorialSidebar"}}}')}}]);