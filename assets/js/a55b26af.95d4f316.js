"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2383],{3905:function(e,t,n){n.d(t,{Zo:function(){return m},kt:function(){return p}});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),d=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},m=function(e){var t=d(e.components);return r.createElement(c.Provider,{value:t},e.children)},l={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=d(n),p=a,f=u["".concat(c,".").concat(p)]||u[p]||l[p]||o;return n?r.createElement(f,s(s({ref:t},m),{},{components:n})):r.createElement(f,s({ref:t},m))}));function p(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,s=new Array(o);s[0]=u;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i.mdxType="string"==typeof e?e:a,s[1]=i;for(var d=2;d<o;d++)s[d]=n[d];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},41279:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return c},metadata:function(){return d},toc:function(){return m},default:function(){return u}});var r=n(87462),a=n(63366),o=(n(67294),n(3905)),s=["components"],i={},c="DPADL",d={unversionedId:"models/dpadl",id:"models/dpadl",title:"DPADL",description:"Data Poisoning Attacks to Deep Learning Based Recommender Systems",source:"@site/docs/models/dpadl.mdx",sourceDirName:"models",slug:"/models/dpadl",permalink:"/docs/models/dpadl",editUrl:"https://github.com/sparsh-ai/ml-utils/docs/models/dpadl.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"DMT",permalink:"/docs/models/dmt"},next:{title:"DQN",permalink:"/docs/models/dqn"}},m=[],l={toc:m};function u(e){var t=e.components,n=(0,a.Z)(e,s);return(0,o.kt)("wrapper",(0,r.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"dpadl"},"DPADL"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Data Poisoning Attacks to Deep Learning Based Recommender Systems")),(0,o.kt)("p",null,"Data poisoning attacks inject fake users to a recommender system and thereby modify the recommendation lists. Specifically, to construct a poisoning attack, the attacker first needs to register a number of fake users in a web service associated with the recommender system. Each fake user generates well-crafted rating scores for a chosen subset of items. These fake data will be included in the training dataset of the target recommender system and then poisons the training process. "),(0,o.kt)("p",null,"Particularly, in a data poisoning attack, an attacker injects fake users with carefully crafted ratings to a recommender system such that the recommender system makes recommendations as the attacker desires, e.g., an attacker-chosen target item is recommended to many normal users. Data poisoning attacks pose severe threats to the trustworthiness of recommender systems and could manipulate Internet opinions. For instance, if an attacker manipulates a news recommender system such that a particular type of news are always recommended to users, then the attacker may be able to manipulate the users\u2019 opinions. However, existing data poisoning attacks are either agnostic to recommender system algorithms, or optimized to traditional recommender system algorithms such as association-rule-based, graph-based, and matrix factorization based. Although deep learning based recommender systems gain increasing attention and are deployed in industry, their security against data poisoning attacks is largely unknown."),(0,o.kt)("p",null,"According to whether data poisoning attacks are focused on a specific type of recommender system, we can divide them into two categories: algorithm-agnostic and algorithm-specific. The former (e.g., types of shilling attacks like random attacks and bandwagon attacks) does not consider the algorithm used by the recommender system and therefore often has limited effectiveness. For instance, random attacks just choose rated items at random from the whole item set for fake users, and bandwagon attacks tend to select certain items with high popularity in the dataset for fake users. The algorithm-specific data poisoning attacks are optimized to a specific type of recommender systems and have been developed for graph-based recommender systems, association-rule-based recommender systems, matrix-factorization-based recommender systems, and neighborhood-based recommender systems."),(0,o.kt)("figure",null,(0,o.kt)("p",null,(0,o.kt)("center",null,(0,o.kt)("img",{src:"https://github.com/recohut/recsys-attacks/raw/d7472b7296515249c1bd1bbb8ea0afa9b07f6d9d/docs/_images/C361387_1.png"}),(0,o.kt)("figcaption",null,"An overview of the data poisoning attack system.")))),(0,o.kt)("p",null,"We first use approximation methods to transform the optimization problem into a tractable one and obtain a loss function. Second, according to the obtained loss function, the algorithm used in the target recommender system, and the training dataset, we train a poison model that simulates the compromised target recommender system. Third, we select filler items according to the predicted rankings generated by the poison model and the selection probability. Note that, we will repeat the second and third steps until enough fake users are generated to construct the attack, and the selection probability will be updated in each iteration."),(0,o.kt)("figure",null,(0,o.kt)("p",null,(0,o.kt)("center",null,(0,o.kt)("img",{src:"https://github.com/recohut/recsys-attacks/raw/d7472b7296515249c1bd1bbb8ea0afa9b07f6d9d/docs/_images/C361387_2.png"}),(0,o.kt)("figcaption",null,"Algorithm of the attack method.")))))}u.isMDXComponent=!0}}]);