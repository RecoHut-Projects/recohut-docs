{
  "unversionedId": "tutorials/applying-gradient-descent-on-a-simple-function-from-scratch",
  "id": "tutorials/applying-gradient-descent-on-a-simple-function-from-scratch",
  "title": "applying-gradient-descent-on-a-simple-function-from-scratch",
  "description": "Gradient descent, also known as steepest descent, is an optimization algorithm for finding the local minimum of a function. To find a local minimum, the function \"steps\" in the  direction of the negative of the gradient. Gradient ascent is the same as gradient descent, except that it steps in the direction of the positive of the gradient and therefore finds local maximums instead of minimums. The algorithm of gradient descent can be outlined as follows:",
  "source": "@site/docs/tutorials/applying-gradient-descent-on-a-simple-function-from-scratch.md",
  "sourceDirName": "tutorials",
  "slug": "/tutorials/applying-gradient-descent-on-a-simple-function-from-scratch",
  "permalink": "/docs/tutorials/applying-gradient-descent-on-a-simple-function-from-scratch",
  "editUrl": "https://github.com/recohut/docs/docs/docs/tutorials/applying-gradient-descent-on-a-simple-function-from-scratch.md",
  "tags": [],
  "version": "current",
  "frontMatter": {
    "jupyter": {
      "jupytext": {
        "text_representation": {
          "extension": ".md",
          "format_name": "markdown",
          "format_version": "1.3",
          "jupytext_version": "1.13.7"
        }
      },
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      }
    }
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "apple-podcast-recommender",
    "permalink": "/docs/tutorials/apple-podcast-recommender"
  },
  "next": {
    "title": "apprentice-mountaincar",
    "permalink": "/docs/tutorials/apprentice-mountaincar"
  }
}