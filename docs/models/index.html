<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-react-helmet="true">Models | Recohut</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://docs.recohut.com/docs/models/"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Models | Recohut"><meta data-react-helmet="true" name="description" content="In the following sections, we will more systematically introduce the following models:"><meta data-react-helmet="true" property="og:description" content="In the following sections, we will more systematically introduce the following models:"><link data-react-helmet="true" rel="icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://docs.recohut.com/docs/models/"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/models/" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/models/" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.87c99533.css">
<link rel="preload" href="/assets/js/runtime~main.b3efbc13.js" as="script">
<link rel="preload" href="/assets/js/main.71372f28.js" as="script">
</head>
<body data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Recohut</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/concept-basics/">Concept - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Concept - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/concept-extras/">Concept - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Concept - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active hasHref_TwRn" href="/docs/models/">Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Models&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/a3c">A3C</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/afm">AFM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/afn">AFN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/ar">AR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/asmg">ASMG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/attrec">AttRec</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/aush">AUSH</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/autoint">AutoInt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/basr">BASR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/bcq">BCQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/beh-prop">Behavior Propensity Modeling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/biasonly">BiasOnly</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/biggraph">BigGraph</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/bpr">BPR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/bst">BST</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/caser">CASER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/cigc">CIGC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/coke">CoKE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dcn">DCN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/ddpg">DDPG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/deepcross">DeepCross</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/deepfm">DeepFM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/deepwalk">DeepWalk</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dgtn">DGTN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dm">DM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dmt">DMT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dpadl">DPADL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dqn">DQN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dr">Doubly Robust</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/drqn">DRQN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/drr">DRR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/dueling-dqn">Dueling DQN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/ffm">FFM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/fgnn">FGNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/fm">FM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/gat">GAT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/gc-san">GC-SAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/gce-gnn">GCE-GNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/glmix">GLMix</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/gru4rec">GRU4Rec</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/hmlet">HMLET</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/incctr">IncCTR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/ipw">IPW</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/itempop">ItemPop</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/khgt">KHGT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/lessr">LESSR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/lightfm-warp">LightFM WARP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/lightgcn">LightGCN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/line">LINE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/lird">LIRD</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/markov-chains">Markov Chains</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/matn">MATN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/mb-gmn">MB-GMN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/mdp">MDP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/mf">MF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/mian">MIAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/mmoe">MMoE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/mpnn">MPNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/neumf">NeuMF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/nfm">NFM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/ngcf">NGCF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/nmrn">NMRN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/node2vec">Node2vec</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/pnn">PNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/ppo">PPO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/q-learning">Q-learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/random_walk">Random Walk</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/rkmf">RKMF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sac">SAC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sarsa">SARSA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sasrec">SASRec</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sdne">SDNE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sgl">SGL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/shared-bottom">Shared Bottom</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/siren">SiReN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/slist">SLIST</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sml">SML</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/spop">SPop</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sr-gnn">SR-GNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sr-san">SR-SAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sr">SR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/sse-pt">SSE-PT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/stamp">STAMP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/struc2vec">Struc2Vec</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/svae">SVAE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/taamr">TAaMR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/tagnn-pp">TAGNN-PP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/tagnn">TAGNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/tgin">TGIN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/vncf">VNCF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/vsknn">VSKNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/wide-and-deep">Wide and Deep</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/word2vec">Word2vec</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/models/xdeepfm">xDeepFM</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/tutorials/mongodb-csv-conversion">Tutorials</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/snippets/python-snippets">Snippets</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/tools/">Tools</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tools&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/datasets">Datasets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/projects">Projects</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Models</h1></header><p>In the following sections, we will more systematically introduce the following models:</p><div class="row"><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/a3c"><h2 class="text--truncate cardTitle_t-cI" title="A3C">📄️<!-- --> <!-- -->A3C</h2><div class="text--truncate cardDescription_aEbl" title="A3C stands for Asynchronous Advantage Actor-Critic. The A3C algorithm builds upon the Actor-Critic class of algorithms by using a neural network to approximate the actor (and critic). The actor learns the policy function using a deep neural network, while the critic estimates the value function. The asynchronous nature of the algorithm allows the agent to learn from different parts of the state space, allowing parallel learning and faster convergence. Unlike DQN agents, which use an experience replay memory, the A3C agent uses multiple workers to gather more samples for learning.">A3C stands for Asynchronous Advantage Actor-Critic. The A3C algorithm builds upon the Actor-Critic class of algorithms by using a neural network to approximate the actor (and critic). The actor learns the policy function using a deep neural network, while the critic estimates the value function. The asynchronous nature of the algorithm allows the agent to learn from different parts of the state space, allowing parallel learning and faster convergence. Unlike DQN agents, which use an experience replay memory, the A3C agent uses multiple workers to gather more samples for learning.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/afm"><h2 class="text--truncate cardTitle_t-cI" title="AFM">📄️<!-- --> <!-- -->AFM</h2><div class="text--truncate cardDescription_aEbl" title="AFM stands for Attentional Factorization Machines. It Improves FM by discriminating the importance of different feature interactions, and learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task that AFM performs betters than FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&amp;Deep and DeepCross with a much simpler structure and fewer model parameters.">AFM stands for Attentional Factorization Machines. It Improves FM by discriminating the importance of different feature interactions, and learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task that AFM performs betters than FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&amp;Deep and DeepCross with a much simpler structure and fewer model parameters.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/afn"><h2 class="text--truncate cardTitle_t-cI" title="AFN">📄️<!-- --> <!-- -->AFN</h2><div class="text--truncate cardDescription_aEbl" title="AFN stands for Adaptive Factorization Network.">AFN stands for Adaptive Factorization Network.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/ar"><h2 class="text--truncate cardTitle_t-cI" title="AR">📄️<!-- --> <!-- -->AR</h2><div class="text--truncate cardDescription_aEbl" title="Simple Association Rules (AR) are a simplified version of the association rule mining technique [Agrawal et al. 1993] with a maximum rule size of two. The method is designed to capture the frequency of two co-occurring events, e.g., “Customers who bought . . . also bought”.">Simple Association Rules (AR) are a simplified version of the association rule mining technique [Agrawal et al. 1993] with a maximum rule size of two. The method is designed to capture the frequency of two co-occurring events, e.g., “Customers who bought . . . also bought”.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/asmg"><h2 class="text--truncate cardTitle_t-cI" title="ASMG">📄️<!-- --> <!-- -->ASMG</h2><div class="text--truncate cardDescription_aEbl" title="ASMG stands for Adaptive Sequential Model Generation.">ASMG stands for Adaptive Sequential Model Generation.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/attrec"><h2 class="text--truncate cardTitle_t-cI" title="AttRec">📄️<!-- --> <!-- -->AttRec</h2><div class="text--truncate cardDescription_aEbl" title="AttRec stands for Self-Attentive Sequential Recommendation.">AttRec stands for Self-Attentive Sequential Recommendation.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/aush"><h2 class="text--truncate cardTitle_t-cI" title="AUSH">📄️<!-- --> <!-- -->AUSH</h2><div class="text--truncate cardDescription_aEbl" title="Attacking Recommender Systems with Augmented User Profiles">Attacking Recommender Systems with Augmented User Profiles</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/autoint"><h2 class="text--truncate cardTitle_t-cI" title="AutoInt">📄️<!-- --> <!-- -->AutoInt</h2><div class="text--truncate cardDescription_aEbl" title="Song et. al., “Automatic Feature Interaction Learning via Self-Attentive Neural Networks”. CIKM, 2018.">Song et. al., “Automatic Feature Interaction Learning via Self-Attentive Neural Networks”. CIKM, 2018.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/basr"><h2 class="text--truncate cardTitle_t-cI" title="BASR">📄️<!-- --> <!-- -->BASR</h2><div class="text--truncate cardDescription_aEbl" title="Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction">Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/bcq"><h2 class="text--truncate cardTitle_t-cI" title="BCQ">📄️<!-- --> <!-- -->BCQ</h2><div class="text--truncate cardDescription_aEbl" title="Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available.">Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/beh-prop"><h2 class="text--truncate cardTitle_t-cI" title="Behavior Propensity Modeling">📄️<!-- --> <!-- -->Behavior Propensity Modeling</h2><div class="text--truncate cardDescription_aEbl" title="behavior-propensity-modeling">behavior-propensity-modeling</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/biasonly"><h2 class="text--truncate cardTitle_t-cI" title="BiasOnly">📄️<!-- --> <!-- -->BiasOnly</h2><div class="text--truncate cardDescription_aEbl" title="BiasOnly is a simple baseline that assumes no interactions between users and items. Formally, it learns: (1) a global bias 𝛼; (2) scalar biases $\betau$ for each user 𝑢 ∈ U; and (3) scalar biases $\betai$ for each item 𝑖 ∈ I. Ultimately, the rating/relevance for user 𝑢 and item 𝑖 is modeled as $\hati^u = \alpha + \betau + \beta_i$.">BiasOnly is a simple baseline that assumes no interactions between users and items. Formally, it learns: (1) a global bias 𝛼; (2) scalar biases $\betau$ for each user 𝑢 ∈ U; and (3) scalar biases $\betai$ for each item 𝑖 ∈ I. Ultimately, the rating/relevance for user 𝑢 and item 𝑖 is modeled as $\hati^u = \alpha + \betau + \beta_i$.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/biggraph"><h2 class="text--truncate cardTitle_t-cI" title="BigGraph">📄️<!-- --> <!-- -->BigGraph</h2><div class="text--truncate cardDescription_aEbl" title="PyTorch-BigGraph: A Large-scale Graph Embedding System">PyTorch-BigGraph: A Large-scale Graph Embedding System</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/bpr"><h2 class="text--truncate cardTitle_t-cI" title="BPR">📄️<!-- --> <!-- -->BPR</h2><div class="text--truncate cardDescription_aEbl" title="BPR stands for Bayesian Personalized Ranking. In matrix factorization (MF), to compute the prediction we have to multiply the user factors to the item factors:">BPR stands for Bayesian Personalized Ranking. In matrix factorization (MF), to compute the prediction we have to multiply the user factors to the item factors:</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/bst"><h2 class="text--truncate cardTitle_t-cI" title="BST">📄️<!-- --> <!-- -->BST</h2><div class="text--truncate cardDescription_aEbl" title="It stands for Behavior Sequence Transformer.">It stands for Behavior Sequence Transformer.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/caser"><h2 class="text--truncate cardTitle_t-cI" title="CASER">📄️<!-- --> <!-- -->CASER</h2><div class="text--truncate cardDescription_aEbl" title="CASER stands for Convolutional Sequence Embedding Recommendation. Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a &#x27;near future&#x27;. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. Convolutional Sequence Embedding Recommendation Model (Caser) address this requirement by embedding a sequence of recent items into an image&#x27; in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. In other words, Caser adopts convolutional neural networks capture the dynamic pattern influences of users’ recent activities.">CASER stands for Convolutional Sequence Embedding Recommendation. Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a &#x27;near future&#x27;. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. Convolutional Sequence Embedding Recommendation Model (Caser) address this requirement by embedding a sequence of recent items into an image&#x27; in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. In other words, Caser adopts convolutional neural networks capture the dynamic pattern influences of users’ recent activities.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/cigc"><h2 class="text--truncate cardTitle_t-cI" title="CIGC">📄️<!-- --> <!-- -->CIGC</h2><div class="text--truncate cardDescription_aEbl" title="To pursue high efficiency, we set the target as using only new data for model updating, meanwhile not sacrificing the recommendation accuracy compared with full model retraining. This is non-trivial to achieve, since the interaction data participates in both the graph structure for model construction and the loss function for model learning, whereas the old graph structure is not allowed to use in model updating. Causal Incremental Graph Convolution (CIGC) estimates the output of full graph convolution. Incremental Graph Convolution (IGC) ingeniously combine the old representations and the incremental graph and effectively fuse the long-term and short-term preference signals. Colliding Effect Distillation (CED) aims to avoid the out-of-date issue of inactive nodes that are not in the incremental graph, which connects the new data with inactive nodes through causal inference. In particular, CED estimates the causal effect of new data on the representation of inactive nodes through the control of their collider.">To pursue high efficiency, we set the target as using only new data for model updating, meanwhile not sacrificing the recommendation accuracy compared with full model retraining. This is non-trivial to achieve, since the interaction data participates in both the graph structure for model construction and the loss function for model learning, whereas the old graph structure is not allowed to use in model updating. Causal Incremental Graph Convolution (CIGC) estimates the output of full graph convolution. Incremental Graph Convolution (IGC) ingeniously combine the old representations and the incremental graph and effectively fuse the long-term and short-term preference signals. Colliding Effect Distillation (CED) aims to avoid the out-of-date issue of inactive nodes that are not in the incremental graph, which connects the new data with inactive nodes through causal inference. In particular, CED estimates the causal effect of new data on the representation of inactive nodes through the control of their collider.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/coke"><h2 class="text--truncate cardTitle_t-cI" title="CoKE">📄️<!-- --> <!-- -->CoKE</h2><div class="text--truncate cardDescription_aEbl" title="Contextualized Knowledge Graph Embedding">Contextualized Knowledge Graph Embedding</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dcn"><h2 class="text--truncate cardTitle_t-cI" title="DCN">📄️<!-- --> <!-- -->DCN</h2><div class="text--truncate cardDescription_aEbl" title="DCN stands for Deep and Cross Network. Manual explicit feature crossing process is very laborious and inefficient. On the other hand, automatic implicit feature crossing methods like MLPs cannot efficiently approximate even 2nd or 3rd-order feature crosses. Deep-cross networks provides a solution to this problem. DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions.">DCN stands for Deep and Cross Network. Manual explicit feature crossing process is very laborious and inefficient. On the other hand, automatic implicit feature crossing methods like MLPs cannot efficiently approximate even 2nd or 3rd-order feature crosses. Deep-cross networks provides a solution to this problem. DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/ddpg"><h2 class="text--truncate cardTitle_t-cI" title="DDPG">📄️<!-- --> <!-- -->DDPG</h2><div class="text--truncate cardDescription_aEbl" title="Deterministic Policy Gradient (DPG) is a type of Actor-Critic RL algorithm that uses two neural networks: one for estimating the action value function, and the other for estimating the optimal target policy. The Deep Deterministic Policy Gradient (DDPG) agent builds upon the idea of DPG and is quite efficient compared to vanilla Actor-Critic agents due to the use of deterministic action policies.">Deterministic Policy Gradient (DPG) is a type of Actor-Critic RL algorithm that uses two neural networks: one for estimating the action value function, and the other for estimating the optimal target policy. The Deep Deterministic Policy Gradient (DDPG) agent builds upon the idea of DPG and is quite efficient compared to vanilla Actor-Critic agents due to the use of deterministic action policies.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/deepcross"><h2 class="text--truncate cardTitle_t-cI" title="DeepCross">📄️<!-- --> <!-- -->DeepCross</h2><div class="text--truncate cardDescription_aEbl" title="Shan, Y., Hoens, T., Jiao, J., Wang, H., Yu, D. and Mao, J., 2016. Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features. [online] Kdd.org.">Shan, Y., Hoens, T., Jiao, J., Wang, H., Yu, D. and Mao, J., 2016. Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features. [online] Kdd.org.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/deepfm"><h2 class="text--truncate cardTitle_t-cI" title="DeepFM">📄️<!-- --> <!-- -->DeepFM</h2><div class="text--truncate cardDescription_aEbl" title="DeepFM stands for Deep Factorization Machines. It consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction.">DeepFM stands for Deep Factorization Machines. It consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/deepwalk"><h2 class="text--truncate cardTitle_t-cI" title="DeepWalk">📄️<!-- --> <!-- -->DeepWalk</h2><div class="text--truncate cardDescription_aEbl" title="DeepWalk learns representations of online social networks graphs. By performing random walks to generate sequences, the paper demonstrated that it was able to learn vector representations of nodes (e.g., profiles, content) in the graph.">DeepWalk learns representations of online social networks graphs. By performing random walks to generate sequences, the paper demonstrated that it was able to learn vector representations of nodes (e.g., profiles, content) in the graph.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dgtn"><h2 class="text--truncate cardTitle_t-cI" title="DGTN">📄️<!-- --> <!-- -->DGTN</h2><div class="text--truncate cardDescription_aEbl" title="DGTN stands for Dual-channel Graph Transition Network.">DGTN stands for Dual-channel Graph Transition Network.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dm"><h2 class="text--truncate cardTitle_t-cI" title="DM">📄️<!-- --> <!-- -->DM</h2><div class="text--truncate cardDescription_aEbl" title="The direct method (DM) involves training a model on the historical data to predict the reward for each context-action instance. In this model, the reward is the target variable, while the context and action are the input features. Using this model, we can predict the rewards for each context-action instance associated with the target policy. This is illustrated below.">The direct method (DM) involves training a model on the historical data to predict the reward for each context-action instance. In this model, the reward is the target variable, while the context and action are the input features. Using this model, we can predict the rewards for each context-action instance associated with the target policy. This is illustrated below.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dmt"><h2 class="text--truncate cardTitle_t-cI" title="DMT">📄️<!-- --> <!-- -->DMT</h2><div class="text--truncate cardDescription_aEbl" title="Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems.">Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dpadl"><h2 class="text--truncate cardTitle_t-cI" title="DPADL">📄️<!-- --> <!-- -->DPADL</h2><div class="text--truncate cardDescription_aEbl" title="Data Poisoning Attacks to Deep Learning Based Recommender Systems">Data Poisoning Attacks to Deep Learning Based Recommender Systems</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dqn"><h2 class="text--truncate cardTitle_t-cI" title="DQN">📄️<!-- --> <!-- -->DQN</h2><div class="text--truncate cardDescription_aEbl" title="The Q-learning component of DQN was invented in 1989 by Christopher Watkins in his PhD thesis titled “Learning from Delayed Rewards”. Experience replay quickly followed, invented by Long-Ji Lin in 1992. This played a major role in improving the efficiency of Q-learning. In the years that followed, however, there were no major success stories involving deep Q-learning. This is perhaps not surprising given the combination of limited computational power in the 1990s and early 2000s, data-hungry deep learning architectures, and the sparse, noisy, and delayed feedback signals experienced in RL. Progress had to wait for the emergence of general-purpose GPU programming, for example with the launch of CUDA in 2006, and the reignition of interest in deep learning within the machine learning community that began in the mid-2000s and rapidly accelerated after 2012.">The Q-learning component of DQN was invented in 1989 by Christopher Watkins in his PhD thesis titled “Learning from Delayed Rewards”. Experience replay quickly followed, invented by Long-Ji Lin in 1992. This played a major role in improving the efficiency of Q-learning. In the years that followed, however, there were no major success stories involving deep Q-learning. This is perhaps not surprising given the combination of limited computational power in the 1990s and early 2000s, data-hungry deep learning architectures, and the sparse, noisy, and delayed feedback signals experienced in RL. Progress had to wait for the emergence of general-purpose GPU programming, for example with the launch of CUDA in 2006, and the reignition of interest in deep learning within the machine learning community that began in the mid-2000s and rapidly accelerated after 2012.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dr"><h2 class="text--truncate cardTitle_t-cI" title="Doubly Robust">📄️<!-- --> <!-- -->Doubly Robust</h2><div class="text--truncate cardDescription_aEbl" title="The doubly robust technique combines the direct method with inverse propensity scores. An intuitive way to think about it is to consider that it uses the predicted rewards from the direct method, and if there is additional information (i.e. exploration and target policy actions match) then the inverse propensity score correction is applied. This is illustrated below.">The doubly robust technique combines the direct method with inverse propensity scores. An intuitive way to think about it is to consider that it uses the predicted rewards from the direct method, and if there is additional information (i.e. exploration and target policy actions match) then the inverse propensity score correction is applied. This is illustrated below.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/drqn"><h2 class="text--truncate cardTitle_t-cI" title="DRQN">📄️<!-- --> <!-- -->DRQN</h2><div class="text--truncate cardDescription_aEbl" title="DRQN stands for Deep Recurrent Q-Learning. It is a combination of a recurrent neural network (RNN) and a deep Q-network (DQN). The idea being that the RNN will be able to retain information from states further back in time and incorporate that into predicting better Q values and thus performing better on games that require long term planning.">DRQN stands for Deep Recurrent Q-Learning. It is a combination of a recurrent neural network (RNN) and a deep Q-network (DQN). The idea being that the RNN will be able to retain information from states further back in time and incorporate that into predicting better Q values and thus performing better on games that require long term planning.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/drr"><h2 class="text--truncate cardTitle_t-cI" title="DRR">📄️<!-- --> <!-- -->DRR</h2><div class="text--truncate cardDescription_aEbl" title="DRR Framework">DRR Framework</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/dueling-dqn"><h2 class="text--truncate cardTitle_t-cI" title="Dueling DQN">📄️<!-- --> <!-- -->Dueling DQN</h2><div class="text--truncate cardDescription_aEbl" title="A Dueling DQN agent explicitly estimates two quantities through a modified network architecture:">A Dueling DQN agent explicitly estimates two quantities through a modified network architecture:</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/ffm"><h2 class="text--truncate cardTitle_t-cI" title="FFM">📄️<!-- --> <!-- -->FFM</h2><div class="text--truncate cardDescription_aEbl" title="FFM stands for Field-aware Factorization Machines. In the official FFM paper, it is empirically proven that for large, sparse datasets with many categorical features, FFM performs better. Conversely, for small and dense datasets or numerical datasets, FFM may not be as effective as FM. FFM is also prone to overfitting on the training dataset, hence one should use a standalone validation set and use early stopping when the loss increases.">FFM stands for Field-aware Factorization Machines. In the official FFM paper, it is empirically proven that for large, sparse datasets with many categorical features, FFM performs better. Conversely, for small and dense datasets or numerical datasets, FFM may not be as effective as FM. FFM is also prone to overfitting on the training dataset, hence one should use a standalone validation set and use early stopping when the loss increases.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/fgnn"><h2 class="text--truncate cardTitle_t-cI" title="FGNN">📄️<!-- --> <!-- -->FGNN</h2><div class="text--truncate cardDescription_aEbl" title="Ruihong Qiu, Jingjing Li, Zi Huang and Hongzhi Yin, “Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks”. CIKM, 2019.">Ruihong Qiu, Jingjing Li, Zi Huang and Hongzhi Yin, “Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks”. CIKM, 2019.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/fm"><h2 class="text--truncate cardTitle_t-cI" title="FM">📄️<!-- --> <!-- -->FM</h2><div class="text--truncate cardDescription_aEbl" title="Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.">Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/gat"><h2 class="text--truncate cardTitle_t-cI" title="GAT">📄️<!-- --> <!-- -->GAT</h2><div class="text--truncate cardDescription_aEbl" title="GAT stands for Graph Attention Networks. This is a special GNN model that addresses several key challenges of spectral models, such as poor ability of generalization from a specific graph structure to another and sophisticated computation of matrix inverse. GAT utilizes attention mechanisms to aggregate neighborhood features (embeddings) by specifying different weights to different nodes.">GAT stands for Graph Attention Networks. This is a special GNN model that addresses several key challenges of spectral models, such as poor ability of generalization from a specific graph structure to another and sophisticated computation of matrix inverse. GAT utilizes attention mechanisms to aggregate neighborhood features (embeddings) by specifying different weights to different nodes.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/gc-san"><h2 class="text--truncate cardTitle_t-cI" title="GC-SAN">📄️<!-- --> <!-- -->GC-SAN</h2><div class="text--truncate cardDescription_aEbl" title="GC-SAN stands for Graph contextualized self-attention.">GC-SAN stands for Graph contextualized self-attention.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/gce-gnn"><h2 class="text--truncate cardTitle_t-cI" title="GCE-GNN">📄️<!-- --> <!-- -->GCE-GNN</h2><div class="text--truncate cardDescription_aEbl" title="GCE-GNN stands for Global Context Enhanced Graph Neural Networks. It exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session.">GCE-GNN stands for Global Context Enhanced Graph Neural Networks. It exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/glmix"><h2 class="text--truncate cardTitle_t-cI" title="GLMix">📄️<!-- --> <!-- -->GLMix</h2><div class="text--truncate cardDescription_aEbl" title="Generalized Linear Mixed Effects model (GLMix) decomposes a personalized recommender system into 2 submodels we first train the fixed effects model, and then train random effects models on the residuals after scoring the fixed effects model, and go back to fixed effects model training again until convergence (Zhang et al., 2016).">Generalized Linear Mixed Effects model (GLMix) decomposes a personalized recommender system into 2 submodels we first train the fixed effects model, and then train random effects models on the residuals after scoring the fixed effects model, and go back to fixed effects model training again until convergence (Zhang et al., 2016).</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/gru4rec"><h2 class="text--truncate cardTitle_t-cI" title="GRU4Rec">📄️<!-- --> <!-- -->GRU4Rec</h2><div class="text--truncate cardDescription_aEbl" title="It uses session-parallel mini-batch approach where we first create an order for the sessions and then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs.">It uses session-parallel mini-batch approach where we first create an order for the sessions and then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/hmlet"><h2 class="text--truncate cardTitle_t-cI" title="HMLET">📄️<!-- --> <!-- -->HMLET</h2><div class="text--truncate cardDescription_aEbl" title="HMLET stands for Hybrid Method of Linear and nonlinEar collaborative filTering (HMLET, pronounced as Hamlet). It is a GCN-based CF method.">HMLET stands for Hybrid Method of Linear and nonlinEar collaborative filTering (HMLET, pronounced as Hamlet). It is a GCN-based CF method.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/incctr"><h2 class="text--truncate cardTitle_t-cI" title="IncCTR">📄️<!-- --> <!-- -->IncCTR</h2><div class="text--truncate cardDescription_aEbl" title="Recently, various deep CTR models are proposed, such as DeepFM, Wide &amp; Deep, PIN, DIN, and DIEN. Generally, deep CTR models include three parts: embedding layer, interaction layer, and prediction layer.">Recently, various deep CTR models are proposed, such as DeepFM, Wide &amp; Deep, PIN, DIN, and DIEN. Generally, deep CTR models include three parts: embedding layer, interaction layer, and prediction layer.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/ipw"><h2 class="text--truncate cardTitle_t-cI" title="IPW">📄️<!-- --> <!-- -->IPW</h2><div class="text--truncate cardDescription_aEbl" title="Inverse Propensity Weighting">Inverse Propensity Weighting</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/itempop"><h2 class="text--truncate cardTitle_t-cI" title="ItemPop">📄️<!-- --> <!-- -->ItemPop</h2><div class="text--truncate cardDescription_aEbl" title="Itempop is a naïve baseline that simply ranks items according to overall train-set popularity. Note that this method is unaffected by the user for which items are being recommended, and has the same global ranking of all items">Itempop is a naïve baseline that simply ranks items according to overall train-set popularity. Note that this method is unaffected by the user for which items are being recommended, and has the same global ranking of all items</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/khgt"><h2 class="text--truncate cardTitle_t-cI" title="KHGT">📄️<!-- --> <!-- -->KHGT</h2><div class="text--truncate cardDescription_aEbl" title="Knowledge-Enhanced Hierarchical Graph Transformer Network for Multi-Behavior Recommendation">Knowledge-Enhanced Hierarchical Graph Transformer Network for Multi-Behavior Recommendation</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/lessr"><h2 class="text--truncate cardTitle_t-cI" title="LESSR">📄️<!-- --> <!-- -->LESSR</h2><div class="text--truncate cardDescription_aEbl" title="Tianwen Chen and Raymond Chi-Wing Wong, “LESSR: Handling Information Loss of Graph Neural Networks for Session-based Recommendation”. KDD, 2020.">Tianwen Chen and Raymond Chi-Wing Wong, “LESSR: Handling Information Loss of Graph Neural Networks for Session-based Recommendation”. KDD, 2020.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/lightfm-warp"><h2 class="text--truncate cardTitle_t-cI" title="LightFM WARP">📄️<!-- --> <!-- -->LightFM WARP</h2><div class="text--truncate cardDescription_aEbl" title="LightFM is probably the only recommender package implementing the WARP (Weighted Approximate-Rank Pairwise) loss for implicit feedback learning-to-rank. Generally, it performs better than the more popular BPR (Bayesian Personalised Ranking) loss --- often by a large margin.">LightFM is probably the only recommender package implementing the WARP (Weighted Approximate-Rank Pairwise) loss for implicit feedback learning-to-rank. Generally, it performs better than the more popular BPR (Bayesian Personalised Ranking) loss --- often by a large margin.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/lightgcn"><h2 class="text--truncate cardTitle_t-cI" title="LightGCN">📄️<!-- --> <!-- -->LightGCN</h2><div class="text--truncate cardDescription_aEbl" title="GCN is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is defined as follows:">GCN is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is defined as follows:</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/line"><h2 class="text--truncate cardTitle_t-cI" title="LINE">📄️<!-- --> <!-- -->LINE</h2><div class="text--truncate cardDescription_aEbl" title="Large-scale Information Network Embedding">Large-scale Information Network Embedding</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/lird"><h2 class="text--truncate cardTitle_t-cI" title="LIRD">📄️<!-- --> <!-- -->LIRD</h2><div class="text--truncate cardDescription_aEbl" title="Existing reinforcement learning recommender methods also could recommend a list of items. E.g. DQN can calculate Q-values of all recalled items separately, and recommend a list of items with highest Q-values. But these recommendations are similar in Euclidean space and we want to find similarity in associative space. For instance, for a bread 🍞, I want egg 🥚, milk 🥛 in my recommendation list instead of white bread 🍞, brown bread 🥪, bun 🫓 etc.">Existing reinforcement learning recommender methods also could recommend a list of items. E.g. DQN can calculate Q-values of all recalled items separately, and recommend a list of items with highest Q-values. But these recommendations are similar in Euclidean space and we want to find similarity in associative space. For instance, for a bread 🍞, I want egg 🥚, milk 🥛 in my recommendation list instead of white bread 🍞, brown bread 🥪, bun 🫓 etc.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/markov-chains"><h2 class="text--truncate cardTitle_t-cI" title="Markov Chains">📄️<!-- --> <!-- -->Markov Chains</h2><div class="text--truncate cardDescription_aEbl" title="Markov chains, named after Andrey Markov, are mathematical systems that hop from one &quot;state&quot; (a situation or set of values) to another. For example, if you made a Markov chain model of a baby&#x27;s behavior, you might include &quot;playing,&quot; &quot;eating&quot;, &quot;sleeping,&quot; and &quot;crying&quot; as states, which together with other behaviors could form a &#x27;state space&#x27;: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probability of hopping, or &quot;transitioning,&quot; from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.">Markov chains, named after Andrey Markov, are mathematical systems that hop from one &quot;state&quot; (a situation or set of values) to another. For example, if you made a Markov chain model of a baby&#x27;s behavior, you might include &quot;playing,&quot; &quot;eating&quot;, &quot;sleeping,&quot; and &quot;crying&quot; as states, which together with other behaviors could form a &#x27;state space&#x27;: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probability of hopping, or &quot;transitioning,&quot; from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/matn"><h2 class="text--truncate cardTitle_t-cI" title="MATN">📄️<!-- --> <!-- -->MATN</h2><div class="text--truncate cardDescription_aEbl" title="Multiplex Behavioral Relation Learning for Recommendation via Memory Augmented Transformer Network.">Multiplex Behavioral Relation Learning for Recommendation via Memory Augmented Transformer Network.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/mb-gmn"><h2 class="text--truncate cardTitle_t-cI" title="MB-GMN">📄️<!-- --> <!-- -->MB-GMN</h2><div class="text--truncate cardDescription_aEbl" title="MB-GMN stands for Multi-behavior pattern modeling with meta-knowledge learner.">MB-GMN stands for Multi-behavior pattern modeling with meta-knowledge learner.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/mdp"><h2 class="text--truncate cardTitle_t-cI" title="MDP">📄️<!-- --> <!-- -->MDP</h2><div class="text--truncate cardDescription_aEbl" title="The Markov decision process (MDP), a reinforcement learning (RL) algorithm, perfectly illustrates how machines have become intelligent in their own unique way. Humans build their decision process on experience. MDPs are memoryless. Humans use logic and reasoning to think problems through. MDPs apply random decisions 100% of the time. Humans think in words, labeling everything they perceive. MDPs have an unsupervised approach that uses no labels or training data. MDPs boost the machine thought process of self-driving cars (SDCs), translation tools, scheduling software, and more. This memoryless, random, and unlabeled machine thought process marks a historical change in the way a former human problem was solved.">The Markov decision process (MDP), a reinforcement learning (RL) algorithm, perfectly illustrates how machines have become intelligent in their own unique way. Humans build their decision process on experience. MDPs are memoryless. Humans use logic and reasoning to think problems through. MDPs apply random decisions 100% of the time. Humans think in words, labeling everything they perceive. MDPs have an unsupervised approach that uses no labels or training data. MDPs boost the machine thought process of self-driving cars (SDCs), translation tools, scheduling software, and more. This memoryless, random, and unlabeled machine thought process marks a historical change in the way a former human problem was solved.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/mf"><h2 class="text--truncate cardTitle_t-cI" title="MF">📄️<!-- --> <!-- -->MF</h2><div class="text--truncate cardDescription_aEbl" title="Matrix Factorization is an iterative approach of SVD called Regularized SVD. It uses the gradient-descent method to estimate the resulting matrices. The obtained model will not be a true SVD of the rating-matrix, as the component matrices are no longer orthogonal, but tends to be more accurate at predicting unseen preferences than the standard SVD [Ekstrand et al. 2011].">Matrix Factorization is an iterative approach of SVD called Regularized SVD. It uses the gradient-descent method to estimate the resulting matrices. The obtained model will not be a true SVD of the rating-matrix, as the component matrices are no longer orthogonal, but tends to be more accurate at predicting unseen preferences than the standard SVD [Ekstrand et al. 2011].</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/mian"><h2 class="text--truncate cardTitle_t-cI" title="MIAN">📄️<!-- --> <!-- -->MIAN</h2><div class="text--truncate cardDescription_aEbl" title="MIAN stands for Multi-Interactive Attention Network. It aggregate multiple information, and gain latent representations through interactions between candidate items and other fine-grained features.">MIAN stands for Multi-Interactive Attention Network. It aggregate multiple information, and gain latent representations through interactions between candidate items and other fine-grained features.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/mmoe"><h2 class="text--truncate cardTitle_t-cI" title="MMoE">📄️<!-- --> <!-- -->MMoE</h2><div class="text--truncate cardDescription_aEbl" title="MMoE stands for Multi-gate Mixture-of-Experts.">MMoE stands for Multi-gate Mixture-of-Experts.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/mpnn"><h2 class="text--truncate cardTitle_t-cI" title="MPNN">📄️<!-- --> <!-- -->MPNN</h2><div class="text--truncate cardDescription_aEbl" title="Message Passing Neural Networks">Message Passing Neural Networks</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/neumf"><h2 class="text--truncate cardTitle_t-cI" title="NeuMF">📄️<!-- --> <!-- -->NeuMF</h2><div class="text--truncate cardDescription_aEbl" title="NMF Leverages the representation power of deep neural-networks to capture nonlinear correlations between user and item embeddings. Formally, the rating/relevance for user 𝑢 and item 𝑖 is modeled as $\hati^u = \alpha + \betau + \betai + f(\gammau || \gammai || \gammau \cdot \gammai)$ where $\gammau , \gamma_i \in \mathbb{R}^d$, ‘||’ represents the concatenation operation, and $f: \mathbb{R}^{3d} \rightarrow \mathbb{R}$ represents an arbitrarily complex neural network.">NMF Leverages the representation power of deep neural-networks to capture nonlinear correlations between user and item embeddings. Formally, the rating/relevance for user 𝑢 and item 𝑖 is modeled as $\hati^u = \alpha + \betau + \betai + f(\gammau || \gammai || \gammau \cdot \gammai)$ where $\gammau , \gamma_i \in \mathbb{R}^d$, ‘||’ represents the concatenation operation, and $f: \mathbb{R}^{3d} \rightarrow \mathbb{R}$ represents an arbitrarily complex neural network.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/nfm"><h2 class="text--truncate cardTitle_t-cI" title="NFM">📄️<!-- --> <!-- -->NFM</h2><div class="text--truncate cardDescription_aEbl" title="NFM stands for Neural Factorization Machine.">NFM stands for Neural Factorization Machine.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/ngcf"><h2 class="text--truncate cardTitle_t-cI" title="NGCF">📄️<!-- --> <!-- -->NGCF</h2><div class="text--truncate cardDescription_aEbl" title="NGCF stands for Neural Graph Collaborative Filtering. This GNN-based approach follows basic operations inherited from the standard GCN to explore the high-order connectivity information. More specifically, NGCF stacks embedding layers and concatenates embeddings obtained in all layers to constitute the final embeddings.">NGCF stands for Neural Graph Collaborative Filtering. This GNN-based approach follows basic operations inherited from the standard GCN to explore the high-order connectivity information. More specifically, NGCF stacks embedding layers and concatenates embeddings obtained in all layers to constitute the final embeddings.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/nmrn"><h2 class="text--truncate cardTitle_t-cI" title="NMRN">📄️<!-- --> <!-- -->NMRN</h2><div class="text--truncate cardDescription_aEbl" title="NMRN is a streaming recommender model based on neural memory networks with external memories to capture and store both long-term stable interests and short-term dynamic interests in a unified way. An adaptive negative sampling framework based on Generative Adversarial Nets (GAN) is developed to optimize our proposed streaming recommender model, which effectively overcomes the limitations of classical negative sampling approaches and improves the effectiveness of the model parameter inference.">NMRN is a streaming recommender model based on neural memory networks with external memories to capture and store both long-term stable interests and short-term dynamic interests in a unified way. An adaptive negative sampling framework based on Generative Adversarial Nets (GAN) is developed to optimize our proposed streaming recommender model, which effectively overcomes the limitations of classical negative sampling approaches and improves the effectiveness of the model parameter inference.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/node2vec"><h2 class="text--truncate cardTitle_t-cI" title="Node2vec">📄️<!-- --> <!-- -->Node2vec</h2><div class="text--truncate cardDescription_aEbl" title="Nodes in networks could be organized based on communities they belong to (i.e., homophily); in other cases, the organization could be based on the structural roles of nodes in the network (i.e., structural equivalence). For instance, in the below figure, we observe nodes $u$ and $s1$ belonging to the same tightly knit community of nodes, while the nodes $u$ and $s6$ in the two distinct communities share the same structural role of a hub node. Real-world networks commonly exhibit a mixture of such equivalences.">Nodes in networks could be organized based on communities they belong to (i.e., homophily); in other cases, the organization could be based on the structural roles of nodes in the network (i.e., structural equivalence). For instance, in the below figure, we observe nodes $u$ and $s1$ belonging to the same tightly knit community of nodes, while the nodes $u$ and $s6$ in the two distinct communities share the same structural role of a hub node. Real-world networks commonly exhibit a mixture of such equivalences.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/pnn"><h2 class="text--truncate cardTitle_t-cI" title="PNN">📄️<!-- --> <!-- -->PNN</h2><div class="text--truncate cardDescription_aEbl" title="PNN stands for Product-based Neural Network.">PNN stands for Product-based Neural Network.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/ppo"><h2 class="text--truncate cardTitle_t-cI" title="PPO">📄️<!-- --> <!-- -->PPO</h2><div class="text--truncate cardDescription_aEbl" title="The PPO (Proximal Policy Optimization) algorithm was introduced by the OpenAI team in 2017 and quickly became one of the most popular Reinforcement Learning method that pushed all other RL methods at that moment aside. PPO involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with that batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an “on-policy learning” approach where the experience samples collected are only useful for updating the current policy.">The PPO (Proximal Policy Optimization) algorithm was introduced by the OpenAI team in 2017 and quickly became one of the most popular Reinforcement Learning method that pushed all other RL methods at that moment aside. PPO involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with that batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an “on-policy learning” approach where the experience samples collected are only useful for updating the current policy.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/q-learning"><h2 class="text--truncate cardTitle_t-cI" title="Q-learning">📄️<!-- --> <!-- -->Q-learning</h2><div class="text--truncate cardDescription_aEbl" title="Q-learning can be applied to model-free RL problems. It supports off-policy learning and therefore provides a practical solution to problems where available experiences were/are collected using some other policy or by some other agent (even humans).">Q-learning can be applied to model-free RL problems. It supports off-policy learning and therefore provides a practical solution to problems where available experiences were/are collected using some other policy or by some other agent (even humans).</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/random_walk"><h2 class="text--truncate cardTitle_t-cI" title="Random Walk">📄️<!-- --> <!-- -->Random Walk</h2><div class="text--truncate cardDescription_aEbl" title="The term &quot;random walk&quot; was first mentioned by Karl Pearson in 1905 in a letter to Nature magazine titled The Problem of the Random Walk. Study of random walks date back even further to the Gambler’s ruin problem, where it could be used to show that a gambler would eventually go bankrupt against an opponent with infinite wealth. It’s only in the last couple of decades, however, that researchers have studied them with respect to networks.">The term &quot;random walk&quot; was first mentioned by Karl Pearson in 1905 in a letter to Nature magazine titled The Problem of the Random Walk. Study of random walks date back even further to the Gambler’s ruin problem, where it could be used to show that a gambler would eventually go bankrupt against an opponent with infinite wealth. It’s only in the last couple of decades, however, that researchers have studied them with respect to networks.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/rkmf"><h2 class="text--truncate cardTitle_t-cI" title="RKMF">📄️<!-- --> <!-- -->RKMF</h2><div class="text--truncate cardDescription_aEbl" title="A kernel function allows to transform the product of the factor matrices. Kernels like the s-shaped logistic function allow to impose bounds on the prediction (e.g. one to five stars) while still being differentiable.">A kernel function allows to transform the product of the factor matrices. Kernels like the s-shaped logistic function allow to impose bounds on the prediction (e.g. one to five stars) while still being differentiable.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sac"><h2 class="text--truncate cardTitle_t-cI" title="SAC">📄️<!-- --> <!-- -->SAC</h2><div class="text--truncate cardDescription_aEbl" title="SAS stands for Soft Actor-Critic. It not only boasts of being more sample efficient than traditional RL algorithms but also promises to be robust to brittleness in convergence.">SAS stands for Soft Actor-Critic. It not only boasts of being more sample efficient than traditional RL algorithms but also promises to be robust to brittleness in convergence.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sarsa"><h2 class="text--truncate cardTitle_t-cI" title="SARSA">📄️<!-- --> <!-- -->SARSA</h2><div class="text--truncate cardDescription_aEbl" title="The SARSA algorithm can be applied to model-free control problems and allows us to optimize the value function of an unknown MDP. SARSA is an on-policy temporal difference learning-based control algorithm. The SARSA algorithm can be summarized as follows:">The SARSA algorithm can be applied to model-free control problems and allows us to optimize the value function of an unknown MDP. SARSA is an on-policy temporal difference learning-based control algorithm. The SARSA algorithm can be summarized as follows:</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sasrec"><h2 class="text--truncate cardTitle_t-cI" title="SASRec">📄️<!-- --> <!-- -->SASRec</h2><div class="text--truncate cardDescription_aEbl" title="SASRec stands for Self-Attentive Sequential Recommendation. It relies on the sequence modeling capabilities of self-attentive neural networks to predict the occurence of the next item in a user’s consumption sequence. To be precise, given a user 𝑢 and their time-ordered consumption history $S^𝑢 = (S1^u, S2^u, \dots, S_{|S^u|}^𝑢),$ SASRec first applies self-attention on $S^𝑢$ followed by a series of non-linear feed-forward layers to finally obtain the next item likelihood.">SASRec stands for Self-Attentive Sequential Recommendation. It relies on the sequence modeling capabilities of self-attentive neural networks to predict the occurence of the next item in a user’s consumption sequence. To be precise, given a user 𝑢 and their time-ordered consumption history $S^𝑢 = (S1^u, S2^u, \dots, S_{|S^u|}^𝑢),$ SASRec first applies self-attention on $S^𝑢$ followed by a series of non-linear feed-forward layers to finally obtain the next item likelihood.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sdne"><h2 class="text--truncate cardTitle_t-cI" title="SDNE">📄️<!-- --> <!-- -->SDNE</h2><div class="text--truncate cardDescription_aEbl" title="Structural Deep Network Embedding">Structural Deep Network Embedding</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sgl"><h2 class="text--truncate cardTitle_t-cI" title="SGL">📄️<!-- --> <!-- -->SGL</h2><div class="text--truncate cardDescription_aEbl" title="SGL is the latest baseline for top-k recommendations. It introduces self-supervised learning into the recommendation system based on the contrastive learning framework. It is implemented on LightGCN and uses a multitask approach that unites the contrastive loss and the BPR loss function. SGL mainly benefits from graph contrastive learning to reinforce user and item representations.">SGL is the latest baseline for top-k recommendations. It introduces self-supervised learning into the recommendation system based on the contrastive learning framework. It is implemented on LightGCN and uses a multitask approach that unites the contrastive loss and the BPR loss function. SGL mainly benefits from graph contrastive learning to reinforce user and item representations.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/shared-bottom"><h2 class="text--truncate cardTitle_t-cI" title="Shared Bottom">📄️<!-- --> <!-- -->Shared Bottom</h2><div class="text--truncate cardDescription_aEbl" title="The shared-bottom model is the simplest and most common multi-task learning architecture. The model has a single base (the shared bottom) from which all of the task-specific subnetworks begin from. This means that this single representation is used for all tasks, and there is no way for individual tasks to adjust what information they get out of the shared bottom compared to other tasks.">The shared-bottom model is the simplest and most common multi-task learning architecture. The model has a single base (the shared bottom) from which all of the task-specific subnetworks begin from. This means that this single representation is used for all tasks, and there is no way for individual tasks to adjust what information they get out of the shared bottom compared to other tasks.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/siren"><h2 class="text--truncate cardTitle_t-cI" title="SiReN">📄️<!-- --> <!-- -->SiReN</h2><div class="text--truncate cardDescription_aEbl" title="Existing literature often ignores the negative feedback e.g. dislikes on YouTube videos, and only capture the homophily (or assortativity) patterns by positive feedback. This is a missed opportunity situation. Performance of GNN-based Recommender Systems can be improved by including negative feedbacks. Disassortivity patterns can be learned by negative feedback. LightGCN can capture the assortativity patterns. and the MLP network can capture the disassortivity patterns.">Existing literature often ignores the negative feedback e.g. dislikes on YouTube videos, and only capture the homophily (or assortativity) patterns by positive feedback. This is a missed opportunity situation. Performance of GNN-based Recommender Systems can be improved by including negative feedbacks. Disassortivity patterns can be learned by negative feedback. LightGCN can capture the assortativity patterns. and the MLP network can capture the disassortivity patterns.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/slist"><h2 class="text--truncate cardTitle_t-cI" title="SLIST">📄️<!-- --> <!-- -->SLIST</h2><div class="text--truncate cardDescription_aEbl" title="SLIST stands for Session-aware Linear Similarity/Transition. It is built by unifying two linear models - SLIS and SLIT.">SLIST stands for Session-aware Linear Similarity/Transition. It is built by unifying two linear models - SLIS and SLIT.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sml"><h2 class="text--truncate cardTitle_t-cI" title="SML">📄️<!-- --> <!-- -->SML</h2><div class="text--truncate cardDescription_aEbl" title="Problem formulation">Problem formulation</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/spop"><h2 class="text--truncate cardTitle_t-cI" title="SPop">📄️<!-- --> <!-- -->SPop</h2><div class="text--truncate cardDescription_aEbl" title="SPop stands for Session-based Popularity. It is a session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.">SPop stands for Session-based Popularity. It is a session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sr-gnn"><h2 class="text--truncate cardTitle_t-cI" title="SR-GNN">📄️<!-- --> <!-- -->SR-GNN</h2><div class="text--truncate cardDescription_aEbl" title="SR-GNN stands for Session-based Recommendation with Graph Neural Networks.">SR-GNN stands for Session-based Recommendation with Graph Neural Networks.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sr-san"><h2 class="text--truncate cardTitle_t-cI" title="SR-SAN">📄️<!-- --> <!-- -->SR-SAN</h2><div class="text--truncate cardDescription_aEbl" title="SR-SAN stands for Session-based Recommendation with Self-Attention Networks.">SR-SAN stands for Session-based Recommendation with Self-Attention Networks.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sr"><h2 class="text--truncate cardTitle_t-cI" title="SR">📄️<!-- --> <!-- -->SR</h2><div class="text--truncate cardDescription_aEbl" title="SR stands for Sequential Rules. The SR method is a variation of MC and AR. It also takes the order of actions into account, but in a less restrictive manner. In contrast to the MC method, we create a rule when an item q appeared after an item p in a session even when other events happened between p and q. When assigning weights to the rules, we consider the number of elements appearing between p and q in the session.">SR stands for Sequential Rules. The SR method is a variation of MC and AR. It also takes the order of actions into account, but in a less restrictive manner. In contrast to the MC method, we create a rule when an item q appeared after an item p in a session even when other events happened between p and q. When assigning weights to the rules, we consider the number of elements appearing between p and q in the session.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/sse-pt"><h2 class="text--truncate cardTitle_t-cI" title="SSE-PT">📄️<!-- --> <!-- -->SSE-PT</h2><div class="text--truncate cardDescription_aEbl" title="Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. SSE-PT overcomes this limitation by employing a Personalized Transformer.">Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. SSE-PT overcomes this limitation by employing a Personalized Transformer.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/stamp"><h2 class="text--truncate cardTitle_t-cI" title="STAMP">📄️<!-- --> <!-- -->STAMP</h2><div class="text--truncate cardDescription_aEbl" title="/img/content-models-raw-mp2-stamp-untitled.png">/img/content-models-raw-mp2-stamp-untitled.png</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/struc2vec"><h2 class="text--truncate cardTitle_t-cI" title="Struc2Vec">📄️<!-- --> <!-- -->Struc2Vec</h2><div class="text--truncate cardDescription_aEbl" title="Learning Node Representations from Structural Identity">Learning Node Representations from Structural Identity</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/svae"><h2 class="text--truncate cardTitle_t-cI" title="SVAE">📄️<!-- --> <!-- -->SVAE</h2><div class="text--truncate cardDescription_aEbl" title="SVAE stands for Sequential Variational Autoencoder.">SVAE stands for Sequential Variational Autoencoder.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/taamr"><h2 class="text--truncate cardTitle_t-cI" title="TAaMR">📄️<!-- --> <!-- -->TAaMR</h2><div class="text--truncate cardDescription_aEbl" title="Targeted Adversarial Attack against Multimedia Recommender Systems">Targeted Adversarial Attack against Multimedia Recommender Systems</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/tagnn-pp"><h2 class="text--truncate cardTitle_t-cI" title="TAGNN-PP">📄️<!-- --> <!-- -->TAGNN-PP</h2><div class="text--truncate cardDescription_aEbl" title="TAGNN-PP models item interactions with GNN, and both local and global user interactions with  a Transformer.">TAGNN-PP models item interactions with GNN, and both local and global user interactions with  a Transformer.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/tagnn"><h2 class="text--truncate cardTitle_t-cI" title="TAGNN">📄️<!-- --> <!-- -->TAGNN</h2><div class="text--truncate cardDescription_aEbl" title="TAGNN stands for Target Attentive Graph Neural Network. Session-based recommendations are challenging due to limited user-item interactions. Typical sequential models are not able to capture complex patterns from all previous interactions. SessionGraph (a graph representation of sessions) can capture the complex patterns from all previous interactions.">TAGNN stands for Target Attentive Graph Neural Network. Session-based recommendations are challenging due to limited user-item interactions. Typical sequential models are not able to capture complex patterns from all previous interactions. SessionGraph (a graph representation of sessions) can capture the complex patterns from all previous interactions.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/tgin"><h2 class="text--truncate cardTitle_t-cI" title="TGIN">📄️<!-- --> <!-- -->TGIN</h2><div class="text--truncate cardDescription_aEbl" title="Jiang et. al., “Triangle Graph Interest Network for Click-through Rate Prediction”. WSDM, 2022.">Jiang et. al., “Triangle Graph Interest Network for Click-through Rate Prediction”. WSDM, 2022.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/vncf"><h2 class="text--truncate cardTitle_t-cI" title="VNCF">📄️<!-- --> <!-- -->VNCF</h2><div class="text--truncate cardDescription_aEbl" title="VNCF stands for Variational Neural Collaborative Filtering.">VNCF stands for Variational Neural Collaborative Filtering.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/vsknn"><h2 class="text--truncate cardTitle_t-cI" title="VSKNN">📄️<!-- --> <!-- -->VSKNN</h2><div class="text--truncate cardDescription_aEbl" title="VSKNN stands for Vector Multiplication Session-Based kNN. The idea of this variant is to put more emphasis on the more recent events of a session when computing the similarities. Instead of encoding a session as a binary vector, we use real-valued vectors to encode the current session. Only the very last element of the session obtains a value of “1”; the weights of the other elements are determined using a linear decay function that depends on the position of the element within the session, where elements appearing earlier in the session obtain a lower weight. As a result, when using the dot product as a similarity function between the current weight-encoded session and a binary-encoded past session, more emphasis is given to elements that appear later in the sessions.">VSKNN stands for Vector Multiplication Session-Based kNN. The idea of this variant is to put more emphasis on the more recent events of a session when computing the similarities. Instead of encoding a session as a binary vector, we use real-valued vectors to encode the current session. Only the very last element of the session obtains a value of “1”; the weights of the other elements are determined using a linear decay function that depends on the position of the element within the session, where elements appearing earlier in the session obtain a lower weight. As a result, when using the dot product as a similarity function between the current weight-encoded session and a binary-encoded past session, more emphasis is given to elements that appear later in the sessions.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/wide-and-deep"><h2 class="text--truncate cardTitle_t-cI" title="Wide and Deep">📄️<!-- --> <!-- -->Wide and Deep</h2><div class="text--truncate cardDescription_aEbl" title="Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It&#x27;s useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.">Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It&#x27;s useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/word2vec"><h2 class="text--truncate cardTitle_t-cI" title="Word2vec">📄️<!-- --> <!-- -->Word2vec</h2><div class="text--truncate cardDescription_aEbl" title="/img/content-models-raw-mp2-word2vec-untitled.png">/img/content-models-raw-mp2-word2vec-untitled.png</div></a></article><article class="col col--6"><a class="card margin-bottom--lg padding--lg cardContainer_w8bb cardContainerLink_AhGd" href="/docs/models/xdeepfm"><h2 class="text--truncate cardTitle_t-cI" title="xDeepFM">📄️<!-- --> <!-- -->xDeepFM</h2><div class="text--truncate cardDescription_aEbl" title="xDeepFM stands for Extreme Deep Factorization Machines.">xDeepFM stands for Extreme Deep Factorization Machines.</div></a></article></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ctr-prediction-models">CTR Prediction Models<a class="hash-link" href="#ctr-prediction-models" title="Direct link to heading">​</a></h2><p>In the early stage of recommendation systems, people spent much time on tedious and onerous feature engineering. At that time, the dimensions of the raw features are relatively small, which makes it possible to implement different combinations of raw features. The newly created features are then fed into a shallow model, such as Logistic Regression (LR) and Gradient Boosting Decision Trees (GBDT) are widely used in CTR prediction. Then, Factorization Machine (FM) transforms the learning of users and items features into a small, shared vector shape. Based on the above method, Field-aware and Field-weighted FM (FFM) further consider the different impact of the fields that a feature belongs to in order to improve the performance of CTR prediction. Along this line, Attentional Factorization Machines (AFM) are proposed to automatically learn weights of deep cross-features and Neural Factorization Machines (NFM) enhances FMs by modelling higher-order and non-linear feature interactions.</p><p>Recently, the success of deep neural networks (DNN) in natural language processing and computer vision brings a new direction to the recommendation system. Among them, the Wide &amp; Deep learning introduces deep neural networks to the CTR prediction. It jointly trains a deep neural network along with the traditional wide linear model. The deep neural networks liberated people from feature engineering while generalizing better combinations of the features. Lots of variants of the Wide &amp; Deep learning have been proposed since it revolutionizes the development of the CTR prediction. Deep &amp; Cross network (DCN) replaces the wide linear part with cross-network, which generates explicit feature crossing among low and high level layers. DeepFM combines the power of DNN and factorization machines for feature representation in the recommendation system. Furthermore, xDeepFM extends the work of DNN by proposing a Compressed Interaction Network to enumerate and compress all feature interactions. Overall, the deep models mentioned above all construct a similar model structure by combing the low-order and high-order features, which greatly reduce the effort of feature engineering and improve the performance of CTR prediction.</p><p>However, these aforementioned shallow or deep models take statistical and categorical features as input, while discarding the sequential behavior information of users. For example, users may search items at an e-commerce system, then view some items of interest, and these items are likely to be clicked or purchased next time. Since the historical behaviors explicitly indicate the preference of the users, it has gained much more attention in the recommendation systems. Among them, DIN proposes a local activation unit that learns the dynamic user interests from the sequential behavior features. DIEN designs an interest evolving layer an attentional update gate to model the dependency between sequential behaviors. The researches above realized the importance of user’s historical behaviors. Unfortunately, they just project other information (i.e., user-specific and context) into one vector and did not pay equal attention to the interactions between the candidate item and fine-grained information, while modeling this interaction has shown extensively progress in many tasks, such as search recommendations and knowledge distillation.</p><p>Different from all previous methods, MIAN can explore the sequential behavior and other fine-grained information simultaneously. Specifically, compared to shallow and deep models, MIAN has a remarkable ability to encode user’s preference through sequential behavior. Compared to sequential models, MIAN can model fine-grained feature interactions better when historical behavior is not enough or representative.</p><table><thead><tr><th>Model</th><th>Paper</th><th>Publication</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>LR</td><td>Predicting Clicks: Estimating the Click-Through Rate for New Ads</td><td>WWW&#x27;07</td><td>Shallow</td><td>Logistic regression (LR) is a simple baseline model for CTR prediction. With the online learning algorithm, FTRL, proposed by Google, LR has been widely adopted in industry. It’s a widely used baseline and applies linear transformation to model the relationship of all the features.</td></tr><tr><td>FM</td><td>Factorization Machines</td><td>ICDM&#x27;10</td><td>Shallow</td><td>While LR fails to capture non-linear feature interactions, Rendle et al. propose factorization machine (FM) that embeds features into dense vectors and models pairwise feature interactions as inner products of the corresponding embedding vectors. Notably, FM also has a linear time complexity in terms of the number of features.</td></tr><tr><td>CCPM</td><td>A Convolutional Click Prediction Model</td><td>CIKM&#x27;15</td><td>Deep</td><td>CCPM reports the first attempt to use convolution for CTR prediction, where feature embeddings are aggregated hierarchically through convolution networks.</td></tr><tr><td>FFM</td><td>Field-aware Factorization Machines for CTR Prediction</td><td>RecSys&#x27;16</td><td>Shallow</td><td>Field-aware factorization machine (FFM) is an extension of FM that considers field information for feature interactions. It was a winner model in several Kaggle contests on CTR prediction.</td></tr><tr><td>YoutubeDNN</td><td>Deep Neural Networks for YouTube Recommendations</td><td>RecSys&#x27;16</td><td>Deep</td><td>DNN is a straightforward deep model, which applies a fully-connected network (termed DNN) after the concatenation of feature embeddings for CTR prediction.</td></tr><tr><td>Wide&amp;Deep</td><td>Wide &amp; Deep Learning for Recommender Systems</td><td>DLRS&#x27;16</td><td>Deep</td><td>Wide&amp;Deep is a general learning framework proposed by Google that combines a wide (or shallow) network and deep network to achieve the advantages of both. It jointly trains a linear model and a deep MLP model to the CTR prediction.</td></tr><tr><td>IPNN</td><td>Product-based Neural Networks for User Response Prediction</td><td>ICDM&#x27;16</td><td>Deep</td><td>PNN is a product-based network that feeds the inner (or outer) products of features embeddings as the input of DNN. Due to the huge memory requirement of pairwise outer products, we use the inner product version, IPNN.</td></tr><tr><td>DeepCross</td><td>Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features</td><td>KDD&#x27;16</td><td>Deep</td><td>Inspired by residual networks, deep crossing to add residual connections between layers of DNNs. DCN is proposed to handle a set of sparse and dense features, and learn cross high-order features jointly with traditional deep MLP.</td></tr><tr><td>HOFM</td><td>Higher-Order Factorization Machines</td><td>NIPS&#x27;16</td><td>Shallow</td><td>Since FM only captures second-order feature interactions, HOFM aims to extend FM to higher-order factorization machines. However, it results in exponential feature combinations that consume huge memory and take a long running time.</td></tr><tr><td>DeepFM</td><td>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</td><td>IJCAI&#x27;17</td><td>Deep</td><td>DeepFM is an extension of Wide&amp;Deep that substitutes LR with FM to explicitly model second-order feature interactions. It combines the explicit high-order interaction module with deep MLP module and traditional FM module, and requires no manual feature engineering.</td></tr><tr><td>NFM</td><td>Neural Factorization Machines for Sparse Predictive Analytics</td><td>SIGIR&#x27;17</td><td>Deep</td><td>Similar to PNN, NFM proposes a Bi-interaction layer that pools the pairwise feature interactions to a vector and then feed it to a DNN for CTR prediction.</td></tr><tr><td>AFM</td><td>Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks</td><td>IJCAI&#x27;17</td><td>Deep</td><td>Instead of treating all feature interactions equally as in FM, AFM learns the weights of feature interactions via attentional networks. Different from FwFM, AFM adjusts the weights dynamically according to the input data sample.</td></tr><tr><td>DCN</td><td>Deep &amp; Cross Network for Ad Click Predictions</td><td>ADKDD&#x27;17</td><td>Deep</td><td>In DCN, a cross network is proposed to perform high-order feature interactions in an explicit way. In addition, it also integrates a DNN network following the Wide&amp;Deep framework.</td></tr><tr><td>FwFM</td><td>Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising</td><td>WWW&#x27;18</td><td>Shallow</td><td>It considers field-wise weights of features interactions. Compared with FFM, it reports comparable performance but uses much fewer model parameters.</td></tr><tr><td>xDeepFM</td><td>xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</td><td>KDD&#x27;18</td><td>xDeepFM</td><td>While high-order feature interactions modeled by DCN are bit-wise, xDeepFM proposes to capture high-order feature interactions in a vector-wise way via a compressed interaction network (CIN). It uses Compressed Interaction Network to enumerate and compress all feature interactions, for modeling an explicit order of interactions.</td></tr><tr><td>DIN</td><td>Deep Interest Network for Click-Through Rate Prediction</td><td>KDD&#x27;18</td><td>Deep</td><td>It’s an early work exploits users’ historical behaviors and uses the attention mechanism to activate user behaviors in which the user be interested in different items.</td></tr><tr><td>FiGNN</td><td>FiGNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction</td><td>CIKM&#x27;19</td><td>Deep</td><td>FiGNN leverages the message passing mechanism of graph neural networks to learn high-order features interactions.</td></tr><tr><td>AutoInt/AutoInt+</td><td>AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks</td><td>CIKM&#x27;19</td><td>Deep</td><td>AutoInt leverages self-attention networks to learn high-order features interactions. AutoInt+ integrates AutoInt with a DNN network.</td></tr><tr><td>FiBiNET</td><td>FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction</td><td>RecSys&#x27;19</td><td>Deep</td><td>FiBiNET leverages squeeze-excitation network to capture important features, and proposes bilinear interactions to enhance feature interactions.</td></tr><tr><td>FGCNN</td><td>Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction</td><td>WWW&#x27;19</td><td>Deep</td><td>FGCNN applies convolution networks and recombination layers to generate additional combinatorial features to enrich existing feature representations.</td></tr><tr><td>HFM/HFM+</td><td>Holographic Factorization Machines for Recommendation</td><td>AAAI&#x27;19</td><td>Deep</td><td>HFM proposes holographic representation and computes compressed outer products via circular convolution to model pairwise feature interactions. HFM+ further integrates a DNN network with HFM.</td></tr><tr><td>ONN</td><td>Operation-aware Neural Networks for User Response Prediction</td><td>Neural Networks&#x27;20</td><td>Deep</td><td>ONN (a.k.a., NFFM) is a model built on FFM. It feeds the interaction outputs from FFM to a DNN network for CTR prediction.</td></tr><tr><td>AFN/AFN+</td><td>Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions</td><td>AAAI&#x27;20</td><td>Deep</td><td>AFN applies logarithmic transformation layers to learn adaptive-order feature interactions. AFN+ further integrates AFN with a DNN network.</td></tr><tr><td>LorentzFM</td><td>Learning Feature Interactions with Lorentzian Factorization</td><td>AAAI&#x27;20</td><td>Shallow</td><td>LorentzFM embed features into a hyperbolic space and model feature interactions via triangle inequality of Lorentz distance.</td></tr><tr><td>InterHAt</td><td>Interpretable Click-through Rate Prediction through Hierarchical Attention</td><td>WSDM&#x27;20</td><td>Deep</td><td>InterHAt employs hierarchical attention networks to model high-order feature interactions in an efficient manner.</td></tr><tr><td>FLEN</td><td>FLEN: Leveraging Field for Scalable CTR Prediction</td><td>DLP-KDD&#x27;20</td><td>Deep</td><td>FLEN: Leveraging Field for Scalable CTR Prediction</td></tr><tr><td>FmFM</td><td>FM^2: Field-matrixed Factorization Machines for Recommender Systems</td><td>WWW&#x27;21</td><td>Deep</td><td>FM^2: Field-matrixed Factorization Machines for Recommender Systems</td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/recohut/docs/docs/docs/models/models.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/concept-extras/vision/video-action-recognition"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Video Action Recognition</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/models/a3c"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A3C</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#ctr-prediction-models" class="table-of-contents__link toc-highlight">CTR Prediction Models</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/concept-basics">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/tutorials">Tutorials</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/projects">Projects</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://nb.recohut.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Jupyter Notebooks<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://step.recohut.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Interactive Stories<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Recohut Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.b3efbc13.js"></script>
<script src="/assets/js/main.71372f28.js"></script>
</body>
</html>