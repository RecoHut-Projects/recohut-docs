<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-react-helmet="true">TensorFlow 2 Reinforcement Learning Cookbook | Recohut</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://docs.recohut.com/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="TensorFlow 2 Reinforcement Learning Cookbook | Recohut"><meta data-react-helmet="true" name="description" content="Process flow"><meta data-react-helmet="true" property="og:description" content="Process flow"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://docs.recohut.com/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.8d002642.css">
<link rel="preload" href="/assets/js/runtime~main.19f23f4b.js" as="script">
<link rel="preload" href="/assets/js/main.f49af512.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Recohut</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/concept-basics/challenges">Concept - Basics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/concept-extras/bias-&amp;-fairness">Concept - Extras</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/models/">Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Models&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_TwRn" href="/docs/tutorials/data-science-bookcamp">Tutorials</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/data-science-bookcamp">Data Science Bookcamp</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/matching-and-ranking-models-in-tensorflow">Matching and Ranking models in Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/real-time-event-capturing-with-kafka-and-mongodb">Real-time event capturing with Kafka and MongoDB</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/session-based-recommendation-with-graph-neural-net">Session-based Recommendation with Graph Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook">TensorFlow 2 Reinforcement Learning Cookbook</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/datasets">Datasets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/projects">Projects</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>TensorFlow 2 Reinforcement Learning Cookbook</h1></header><h2 class="anchor anchorWithStickyNavbar_y2LR" id="process-flow">Process flow<a class="hash-link" href="#process-flow" title="Direct link to heading">â€‹</a></h2><p><img src="https://github.com/RecoHut-Projects/drl-recsys/raw/main/images/S990517_process_flow.svg" alt="https://github.com/RecoHut-Projects/drl-recsys/raw/main/images/S990517_process_flow.svg"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="environments">Environments<a class="hash-link" href="#environments" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="simple-gridworld">Simple Gridworld<a class="hash-link" href="#simple-gridworld" title="Direct link to heading">â€‹</a></h3><p>This is a simple environment where the world is represented as a grid. Each location on the grid can be referred to as a cell. The goal of an agent in this environment is to find its way to the goal state in a grid like the one shown here:</p><p><img alt="Untitled" src="data:image/png;base64,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"></p><p>The agent&#x27;s location is represented by the blue cell in the grid, while the goal and a mine/bomb/obstacle&#x27;s location is represented in the grid using green and red cells, respectively. The agent (blue cell) needs to find its way through the grid to reach the goal (green cell) without running over the mine/bomb (red cell).</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="simple-gridworld-v2">Simple Gridworld v2<a class="hash-link" href="#simple-gridworld-v2" title="Direct link to heading">â€‹</a></h3><p>This is a simplified version of the grid environment. This simplification would help in understanding and visualizing the learning processes of various algorithms. As shown in the figure below, it is a 3x4 gridworld, where the goal is at <!-- -->[0,3]<!-- --> and the bomb is at <!-- -->[1,3]<!-- -->. (so near to each other, that our RL agent need to learn moving with care ðŸ˜‡).</p><p><img alt="Untitled" src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-1-a5e67644f7e4f33e4819100c24f3572a.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stochastic-maze">Stochastic Maze<a class="hash-link" href="#stochastic-maze" title="Direct link to heading">â€‹</a></h3><p>To train RL agents for the real world, we need learning environments that are stochastic, since real-world problems are stochastic in nature. This recipe will walk you through the steps for building a Maze learning environment to train RL agents. The Maze is a simple, stochastic environment where the world is represented as a grid. Each location on the grid can be referred to as a cell. The goal of an agent in this environment is to find its way to the goal state. Consider the maze shown in the following diagram, where the black cells represent walls:</p><p><img alt="Untitled" src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-2-d681a5426a4066838414c6253ec9a598.png"></p><p>The agent&#x27;s location is initialized to be at the top-left cell in the Maze. The agent needs to find its way around the grid to reach the goal located at the top-right cell in the Maze, collecting a maximum number of coins along the way while avoiding walls. The location of the goal, coins, walls, and the agent&#x27;s starting location can be modified in the environment&#x27;s code.</p><p>The reward is based on the number of coins that are collected by the agent before they reach the goal state. Because the environment is stochastic, the action that&#x27;s taken by the environment has a slight (0.1) probability of &quot;slipping&quot; wherein the actual action that&#x27;s executed will be altered stochastically.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cartpole">CartPole<a class="hash-link" href="#cartpole" title="Direct link to heading">â€‹</a></h3><p>In this environment, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.</p><p><img src="https://miro.medium.com/max/600/1*LnQ5sRu-tJmlvRWmDsdSvw.gif" alt="CartPole before training. [source](https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288)."></p><p>CartPole before training. <a href="https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288" target="_blank" rel="noopener noreferrer">source</a>.</p><p><img src="https://miro.medium.com/max/600/1*jLj9SYWI7e6RElIsI3DFjg.gif" alt="CartPole after training. [source](https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288)."></p><p>CartPole after training. <a href="https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288" target="_blank" rel="noopener noreferrer">source</a>.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="mountaincar">MountainCar<a class="hash-link" href="#mountaincar" title="Direct link to heading">â€‹</a></h3><p>In this environment, a car is on a one-dimensional track, positioned between two &quot;mountains&quot;. The goal is to drive up the mountain on the right; however, the car&#x27;s engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pendulum">Pendulum<a class="hash-link" href="#pendulum" title="Direct link to heading">â€‹</a></h3><p>In this environment, the pendulum starts in a random position, and the goal is to swing it up so it stays upright.</p><iframe width="727" height="409" src="https://www.youtube.com/embed/XbXZ9dmKG_s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cryptocurrency-trading">Cryptocurrency Trading<a class="hash-link" href="#cryptocurrency-trading" title="Direct link to heading">â€‹</a></h3><p>This environment simulates a Bitcoin trading exchange based on real-world data from the Gemini cryptocurrency exchange. In this environment, your RL agent can place buy/sell/hold trades and get rewards based on the profit/loss it makes, starting with an initial cash balance in the agent&#x27;s trading account.</p><p>In continuous action space, instead ofÂ allowing the AgentÂ to only take discrete actions, such as buying/selling/holding a pre-set amount of Bitcoin or Ethereum tokens, we allow the Agent to decide how many crypto coins/tokens it would like to buy or sell.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stock-trading">Stock Trading<a class="hash-link" href="#stock-trading" title="Direct link to heading">â€‹</a></h3><p>TheÂ stockÂ market provides anyone with a highly lucrative opportunity to participate and make profits. While it is easily accessible, not all humans can make consistently profitable trades due to the dynamic nature of the market and the emotional aspects that can impair people&#x27;s actions. RL agents take emotion out of the equation and can be trained to make profits consistently.</p><p>This stock market trading environment enable RL agents to trade stocks using real stock market data. When you have trained them enough, you can deploy them so that they automatically make trades (and profits) for you!</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="webgym-world-of-bits-wob">WebGym World-of-Bits (WoB)<a class="hash-link" href="#webgym-world-of-bits-wob" title="Direct link to heading">â€‹</a></h3><p>WebGym is aÂ <strong>World of Bits</strong>Â (<strong>WoB</strong>)-based OpenAI GymÂ compatible learning platform for training RL Agents forÂ world wide web-based real-world tasks. It provides learning environments for agents to perceive the world-wide-web like how we (humans) perceive â€“ using the pixels rendered on to the display screen. The agent interacts with the environment using keyboard and mouse events as actions. This allows the agent to experience the world-wide-web like how we do thereby require no new additional modifications for the agents to train. This allows us to train RL agents that can directly work with the web-based pages and applications to complete real-world tasks. For more informationÂ about WoB, check outÂ <a href="http://proceedings.mlr.press/v70/shi17a/shi17a.pdf" target="_blank" rel="noopener noreferrer">this</a> link.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-a-simple-gridworld-environment">Building a simple Gridworld Environment<a class="hash-link" href="#building-a-simple-gridworld-environment" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T533231_Building_a_simple_Gridworld_Environment.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are building the gridworld environment as a python class object <code>GridworldEnv</code> by subclassing <code>gym.Env</code> module.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-a-simple-gridworld-v2-environment">Building a simple Gridworld v2 Environment<a class="hash-link" href="#building-a-simple-gridworld-v2-environment" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T195475_Building_a_simple_Gridworld_v2_Environment.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are building a simple 3x4 gridworld environment as a python class object <code>GridworldV2Env</code> by subclassing <code>gym.Env</code> module.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-a-stochastic-maze-gridworld-environment">Building a Stochastic Maze Gridworld Environment<a class="hash-link" href="#building-a-stochastic-maze-gridworld-environment" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T495794_Building_a_Stochastic_Maze_Gridworld_Environment.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are building a stochastic maze environment as a python class object <code>MazeEnv</code> by subclassing <code>gym.Env</code> module. The slip probability is set to 10%.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-gridworld-with-mlp-model">Training RL Agent in Gridworld with MLP Model<a class="hash-link" href="#training-rl-agent-in-gridworld-with-mlp-model" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T490651_Training_RL_Agent_in_Gridworld_Environment_with_MLP_Model.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the simple gridworld environment. Then we are building the agent model <code>Brain</code> by subclassing <code>keras.Model</code>, and a wrapper class <code>Agent</code>. Then we train the agent in the given gridworld environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-maze-gridworld-with-value-iteration">Training RL Agent in Maze Gridworld with Value-iteration<a class="hash-link" href="#training-rl-agent-in-maze-gridworld-with-value-iteration" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T920001_Training_RL_Agent_in_Maze_Gridworld_with_Value_iteration_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the stochastic maze environment. Then we apply value-iteration to learn the optimal actions in each and every state.</p><p><img alt="Agent started with a random policy." src="data:image/png;base64,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"></p><p>Agent started with a random policy.</p><p><img alt="And after few iterations, learned the optimal policy." src="data:image/png;base64,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"></p><p>And after few iterations, learned the optimal policy.</p><div class="codeBlockContainer_J+bg theme-code-block"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token plain">Action mapping:[0 - UP; 1 - DOWN; 2 - LEFT; 3 - RIGHT</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Optimal actions:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[1. 1. 1. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 3. 1. 3. 1. 3. 3.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 3. 3. 0. 0. 2. 2. 0. 2. 0. 2. 0. 0.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 0. 1. 0. 0. 1. 1. 0. 1. 0. 3. 0. 0. 3. 3. 0. 3. 0. 3. 0. 0. 3. 0. 0. 0.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 1. 1. 2. 2. 3. 3. 3. 3. 1. 1. 1. 2. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-gridworld-with-temporal-difference">Training RL Agent in Gridworld with Temporal Difference<a class="hash-link" href="#training-rl-agent-in-gridworld-with-temporal-difference" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T122762_Training_RL_Agent_in_Gridworld_with_Temporal_Difference_learning_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the gridworld v2 environment. Then we learn the optimal policy by applying temporal-difference learning method.</p><p><img alt="Ground-truth state." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-1-a5e67644f7e4f33e4819100c24f3572a.png"></p><p>Ground-truth state.</p><p><img alt="optimal state values learned by the TD algorithm." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-5-c034a42812ef622f2f0308af8ebe2770.png"></p><p>optimal state values learned by the TD algorithm.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-gridworld-with-monte-carlo">Training RL Agent in Gridworld with Monte-Carlo<a class="hash-link" href="#training-rl-agent-in-gridworld-with-monte-carlo" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T303629_Training_RL_Agent_in_Gridworld_with_Monte_Carlo_Prediction_and_Control_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the gridworld v2 environment. Then we applied the monte-carlo prediction method to learn the optimal state values and later on, we also applied monte-carlo control method to learn the optimal action values.</p><p><img alt="Monte Carlo Prediction." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-6-17137f2bd3ddb45b58a2a11c5714b89a.png"></p><p>Monte Carlo Prediction.</p><p><img alt="Monte Carlo Control." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-7-f5f6bd2f3ae5c6f87141dc88d400a766.png"></p><p>Monte Carlo Control.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-gridworld-with-sarsa">Training RL Agent in Gridworld with SARSA<a class="hash-link" href="#training-rl-agent-in-gridworld-with-sarsa" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T515396_Training_RL_Agent_in_Gridworld_with_SARSA_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the gridworld v2 environment. Then we applied the SARSA algorithm to learn the optimal action-values.</p><p><img alt="Action values learned by SARSA." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-8-5b1854df05ea25a9e5921d7ffee5f764.png"></p><p>Action values learned by SARSA.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-gridworld-with-q-learning">Training RL Agent in Gridworld with Q-learning<a class="hash-link" href="#training-rl-agent-in-gridworld-with-q-learning" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T453493_Training_RL_Agent_in_Gridworld_with_Q_learning_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the gridworld v2 environment. Then we applied the Q-learning algorithm to learn the optimal action-values.</p><p><img alt="Action values learned by Q-learning." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-9-a5b6f324a04801c92fc390a47dc57d1d.png"></p><p>Action values learned by Q-learning.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-cartpole-env-with-actor-critic">Training RL Agent in CartPole Env. with Actor-Critic<a class="hash-link" href="#training-rl-agent-in-cartpole-env-with-actor-critic" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T043789_Training_RL_Agent_in_CartPole_Environment_with_Actor_Critic_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the CartPole environment using <code>gym.make</code> api call. Then we built the agent model <code>ActorCritic</code> by subclassing <code>keras.Model</code>, and a wrapper class <code>Agent</code>. Then we train the agent in this CartPole environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-cartpole-env-with-dqn">Training RL Agent in CartPole Env. with DQN<a class="hash-link" href="#training-rl-agent-in-cartpole-env-with-dqn" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T473399_Training_RL_Agent_in_CartPole_Environment_with_DQN_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the CartPole environment using <code>gym.make</code> api call. Then we built the <code>ReplayBuffer</code> and <code>DQN</code> class objects, and a wrapper class <code>Agent</code>. Then we train the agent in this CartPole environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-cartpole-env-with-dueling-dqn">Training RL Agent in CartPole Env. with Dueling DQN<a class="hash-link" href="#training-rl-agent-in-cartpole-env-with-dueling-dqn" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T432381_Training_RL_Agent_in_CartPole_Environment_with_Dueling_DQN_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the CartPole environment using <code>gym.make</code> api call. Then we built the <code>ReplayBuffer</code> and <code>DuelingDQN</code> class objects, and a wrapper class <code>Agent</code>. Then we train the agent in this CartPole environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-cartpole-env-with-drqn">Training RL Agent in CartPole Env. with DRQN<a class="hash-link" href="#training-rl-agent-in-cartpole-env-with-drqn" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T244614_Training_RL_Agent_in_CartPole_Environment_with_DRQN_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the CartPole environment using <code>gym.make</code> api call. Then we built the <code>ReplayBuffer</code> and <code>DRQN</code> class objects, and a wrapper class <code>Agent</code>. Then we train the agent in this CartPole environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-mountaincar-env-with-policy-gradient">Training RL Agent in MountainCar Env. with Policy Gradient<a class="hash-link" href="#training-rl-agent-in-mountaincar-env-with-policy-gradient" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T611861_Training_RL_Agent_in_Mountain_Car_Environment_with_Policy_gradient_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the MountainCar environment using <code>gym.make</code> api call. Then we built the agent model <code>PolicyNet</code> by subclassing <code>keras.Model</code>, and a wrapper class <code>Agent</code>. Then we train the agent in this MountainCar environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-mountaincar-env-with-a3c-continuous">Training RL Agent in MountainCar Env. with A3C Continuous<a class="hash-link" href="#training-rl-agent-in-mountaincar-env-with-a3c-continuous" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T307891_Training_RL_Agent_in_Mountain_Car_Environment_with_A3C_Continuous_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the MountainCar-continuous environment using <code>gym.make</code> api call. Then we built the <code>Actor</code> and <code>Critic</code> class objects, and a wrapper class <code>Agent</code>. For A3C algorithm, we also built <code>A3CWorker</code> object by subclassing <code>thread.Thread</code> module. Then we train the agent in this MountainCar-continuous environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-pendulum-env-with-ppo-continuous">Training RL Agent in Pendulum Env. with PPO Continuous<a class="hash-link" href="#training-rl-agent-in-pendulum-env-with-ppo-continuous" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T626473_Training_RL_Agent_in_Pendulum_Environment_with_PPO_Continuous_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the Pendulum environment using <code>gym.make</code> api call. Then we built the <code>Actor</code> and <code>Critic</code> class objects, and a wrapper class <code>Agent</code>. Then we train the agent in this Pendulum environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-in-pendulum-env-with-ddpg">Training RL Agent in Pendulum Env. with DDPG<a class="hash-link" href="#training-rl-agent-in-pendulum-env-with-ddpg" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T559464_Training_RL_Agent_in_Pendulum_Environment_with_DDPG_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first create the Pendulum environment using <code>gym.make</code> api call. Then we built the <code>ReplayBuffer</code>, the <code>Actor</code> and <code>Critic</code> class objects, and a wrapper class <code>Agent</code>. Then we train the agent in this Pendulum environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-bitcoin-and-ethereum-cryptocurrency-trading-env">Building Bitcoin and Ethereum Cryptocurrency Trading Env.<a class="hash-link" href="#building-bitcoin-and-ethereum-cryptocurrency-trading-env" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T350011_Building_Bitcoin_and_Ethereum_Cryptocurrency_based_Trading_RL_Environment.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we implemented custom OpenAI Gym-compatible learning environments for cryptocurrency trading with both discrete and continuous-value action spaces.</p><ul><li>Building a Bitcoin trading RL platform using real market data (<code>CryptoTradingEnv</code> class object)</li><li>Building an Ethereum trading RL platform using price charts (<code>CryptoTradingVisualEnv</code> class object)</li><li>Building an advanced cryptocurrency trading platform for RL agents (<code>CryptoTradingContinuousEnv</code>, and <code>CryptoTradingVisualContinuousEnv</code> class objects)</li></ul><p>We used the Bitcoin (<code>Gemini_BTCUSD_d.csv</code>) and Ethereum (<code>Gemini_ETHUSD_d.csv</code>) data from Gemini in building the environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-for-trading-cryptocurrencies-with-sac">Training RL Agent for Trading Cryptocurrencies with SAC<a class="hash-link" href="#training-rl-agent-for-trading-cryptocurrencies-with-sac" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T778350_Training_an_RL_Agent_for_Trading_Cryptocurrencies_using_SAC_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the crypto trading environment <code>CryptoTradingContinuousEnv</code>, and then using the SAC method to train the RL agent to trade the Bitcoins in the trading RL environment.</p><p><img alt="Untitled" src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-10-9f3bf53b2f6af85b372c008d0c17f71b.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-stock-trading-rl-environment">Building Stock Trading RL Environment<a class="hash-link" href="#building-stock-trading-rl-environment" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T344654_Building_Stock_Trading_RL_Environment.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are:</p><ul><li>Building a stock market trading RL platform using real stock exchange data</li><li>Building a stock market trading RL platform using price charts</li><li>Building an advanced stock trading RL platform to train agents to mimic professional traders</li></ul><p>We used the Tesla (<code>TSLA.csv</code>) and Microsoft (<code>MSFT.csv</code>) stocks data from Gemini in building the environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-rl-agent-for-trading-stocks-with-sac">Training RL Agent for Trading Stocks with SAC<a class="hash-link" href="#training-rl-agent-for-trading-stocks-with-sac" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T836251_Training_an_RL_Agent_for_Trading_Stocks_using_SAC_method.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the stocks trading environment <code>StockTradingContinuousEnv</code>, and then using the SAC method to train the RL agent to trade the Tesla stocks in the trading RL environment.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-rl-agent-to-complete-tasks-on-the-web--call-to-action">Building RL Agent to complete tasks on the web â€“ Call to Action<a class="hash-link" href="#building-rl-agent-to-complete-tasks-on-the-web--call-to-action" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T702798_Building_an_RL_Agent_to_complete_tasks_on_the_web_%E2%80%93_Call_to_Action.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the <code>MiniWoBClickButtonVisualEnv</code> environment, and then training a PPO agent to handleÂ <strong>Call-To-Action</strong>Â (<strong>CTA</strong>) type tasksÂ for you. CTA buttons are the actionableÂ buttons that you typically find on web pages that you need to click in order to proceed to the next step. While there are several CTA button examples available, some common examples include theÂ <strong>OK</strong>/<strong>Cancel</strong>Â dialog boxes, where you need you to click to acknowledge/dismiss the pop-up notification, and theÂ <strong>Click to learn more</strong>Â button. </p><p>The following image illustrates a set of observations from a randomized CTA environment (with different seeds) so that you understand the task that the Agent will be solving:</p><p><img alt="Screenshot of the Agent&amp;#39;s observations from a randomized CTA environment." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-11-1b8bf9a516a4fdce4b81aac067837496.png"></p><p>Screenshot of the Agent&#x27;s observations from a randomized CTA environment.</p><p>Note that for simplicity, we used one instance of the environment, though the code can scale for a greater number of environment instances to speed up training.</p><p>To understand how the Agent training progresses, consider the following sequence of images. During the initial stages of training, when the Agent is trying to understand the task and the objective of the task, the Agent may just be executing random actions (exploration) or even clicking outside the screen, as shown in the following screenshot:</p><p><img alt="Agent clicking outside the screen (no visible blue dot) during initial exploration." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-12-8202f925e3cf376683abbed5cb3a648f.png"></p><p>Agent clicking outside the screen (no visible blue dot) during initial exploration.</p><p>As the AgentÂ learns by stumbling upon theÂ correct button to click, it starts to make progress. The following screenshot shows the Agent making some progress:</p><p><img alt="Deep PPO Agent making progress in the CTA task." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-13-1561d63678469dafbf5d4c6eff27e3cb.png"></p><p>Deep PPO Agent making progress in the CTA task.</p><p>Finally, when the episode is complete or ends (due to a time limit), the Agent receives an observationÂ similar to the one shown in the following screenshot (left):</p><p><img alt="End of episode observation (left) and summary of performance (right)." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-14-07eec35f1dd6e5e0b9e57e5f86764920.png"></p><p>End of episode observation (left) and summary of performance (right).</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-rl-agent-to-auto-login-on-the-web">Building RL Agent to auto-login on the web<a class="hash-link" href="#building-rl-agent-to-auto-login-on-the-web" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T769395_Building_an_RL_Agent_to_auto_login_on_the_web.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>Imagine that you have an Agent or a bot that watches what you are doing and automatically logs you into websites whenever you click on a login screen. While browser plugins exist that can automatically log you in, they do so using hardcoded scripts that only work on the pre-programmed website&#x27;s login URLs. But what if you had an Agent that only relied on the rendered web page â€“ just like you do to perform a task â€“ and worked even when the URL changes and when you are on a new website with no prior saved data? How cool would that be?!</p><p>In this, we are first building the <code>MiniWoBLoginUserVisualEnv</code> environment, and then training a PPO agent to log in on a web page! You will learn how to randomize, customize, and increase the generality of the Agent to get it to work on any login screen. An example of randomizing and customizing the usernames and passwords for a task can be seen in the following image:</p><p><img alt="Sample observations from a randomized user login task." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-15-ed8d08699d04e05f0fbc53166a576a65.png"></p><p>Sample observations from a randomized user login task.</p><p>The login taskÂ involves clicking on the correct form field and typing in the correct username and/or password. For an Agent to be able to do this, it needs to master how to use a mouse and keyboard, in addition to processing the visual web page to understand the task and the web login form. With enough samples, the deep RL Agent will learn a policy to complete this task. Let&#x27;s take a look at the state of the Agent&#x27;s progress, snapshotted at different stages.</p><p>The followingÂ image shows the Agent successfully entering the username and correctly clicking on the password field to enter the password, but not being able to complete the task yet:</p><p><img alt="Screenshot of a trained Agent successfully entering the username but not a password." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-16-d6f7ab9ca990bee87a19e0e355b83cd0.png"></p><p>Screenshot of a trained Agent successfully entering the username but not a password.</p><p>In the following image, you can see that the Agent has learned to enter both the username and password, but they are not quite right for the task to be classed as complete:</p><p><img alt="Agent entering both the username and password but incorrectly." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-17-beb4991719399810ee0e1de119014b00.png"></p><p>Agent entering both the username and password but incorrectly.</p><p>The same AgentÂ with a different checkpoint, after several thousand more episodes of learning, is close to completing the task, as shown in the following image</p><p><img alt="A well-trained Agent model about to complete the login task successfully." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-18-bc790477213cdc9d5ff995b85b5f7aff.png"></p><p>A well-trained Agent model about to complete the login task successfully.</p><p>Now that you understand how the Agent works and behaves, you can customize it to your liking andÂ use use cases to train the Agent to automatically log into any custom website you want!</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-rl-agent-to-book-flights-on-the-web">Building RL Agent to book flights on the web<a class="hash-link" href="#building-rl-agent-to-book-flights-on-the-web" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T462163_Building_an_RL_Agent_to_book_flights_on_the_web.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building the <code>MiniWoBBookFlightVisualEnv</code> environment, and then training a PPO agent to visually operate flight booking websites using a keyboard and mouse to book flights! This task is quite useful but complicated due to the varying amount of task parameters we need to implement, such as source city, destination, date, and more. The following image shows a sample of the start states from a randomizedÂ <strong>MiniWoBBookFlightVisualEnv</strong>Â flight booking environment:</p><p><img alt="Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv environment." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-19-8b36be80b44716b243d539e5bb735d8a.png"></p><p>Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv environment.</p><p>The flight booking environment is quite complex as it requires the Agent to master both the keyboard and the mouse, in addition to understanding the task by looking at visual images of the task description (visual text parsing), inferring the intended task objective, and executing the actions in the correct sequence. The following screenshot shows the performance of the Agent upon completing a sufficiently large number of episodes of training:</p><p><img alt="A screenshot of the Agent performing the flight booking task at different stages of learning." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-20-e4a48e32a12714cd46034cb6af0ea61a.png"></p><p>A screenshot of the Agent performing the flight booking task at different stages of learning.</p><p>The following screenshot shows the Agent&#x27;s screen after the Agent progressed to the final stageÂ of the task (although it&#x27;s not close to completing the task):</p><p><img alt="Screenshot of the Agent progressing all the way to the final stage of the flight booking task." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-21-4e144e5cacfde1eaf5c3939cb2ddabc4.png"></p><p>Screenshot of the Agent progressing all the way to the final stage of the flight booking task.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-rl-agent-to-manage-emails-on-the-web">Building RL Agent to manage emails on the web<a class="hash-link" href="#building-rl-agent-to-manage-emails-on-the-web" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T515244_Building_an_RL_Agent_to_manage_emails_on_the_web.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>Email has become an integral part of many people&#x27;s lives. The number of emails that an average workingÂ professional goes through in a workdayÂ is growing daily. While a lot of email filters exist for spam control, how nice would it be to have an intelligent Agent that can perform a series of email management tasks that just provide a task description (through text or speech via speech-to-text) and are not limited by any APIs that have rate limits?</p><p>In this, we are first building the <code>MiniWoBEmailInboxImportantVisualEnv</code> environment, and then training a PPO agent to manage emails on the web.</p><p>A set of sample tasks can be seen in the following image:</p><p><img alt="A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv environment." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-22-fab8fd6e485b4f6566c64cb8313a4883.png"></p><p>A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv environment.</p><p>The email management environment poses as a nice sequential decision-making problem for the deep RL Agent. First, the Agent hasÂ to choose the correct email from aÂ series of emails in an inbox and then perform the desired action (starring the email and so on). The Agent only has access to the visual rendering of the inbox, so it needs to extract the task specification details, interpret the task specification, and then plan and execute the actions!</p><p>The following is a screenshot of the Agent&#x27;s performance at different stages of learning (loaded from different checkpoints):</p><p><img alt="A series of screenshots showing the Agent&amp;#39;s learning progress." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-23-acd2e947a1734b5d8db396c928b03060.png"></p><p>A series of screenshots showing the Agent&#x27;s learning progress.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="building-rl-agent-to-manage-social-media-accounts-on-the-web">Building RL Agent to manage social media accounts on the web<a class="hash-link" href="#building-rl-agent-to-manage-social-media-accounts-on-the-web" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T098537_Building_an_RL_Agent_to_manage_social_media_accounts_on_the_web.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we are first building 3 environments:</p><ul><li><code>MiniWoBSocialMediaReplyVisualEnv</code> to train a social media like &amp; reply agent with PPO.</li><li><code>MiniWoBSocialMediaMuteUserVisualEnv</code> to train a social media user mute agent with PPO.</li><li><code>MiniWoBSocialMediaMuteUserVisualEnv</code> to train a social media user mute agent with DDPG.</li></ul><p>Then we are building an RL Agent thatÂ is trained to perform management tasks on the social media account! The following image shows a series of (randomized) tasks from the environment that we will be training the Agent in:</p><p><img alt="A sample set of social media account management tasks that the Agent has been asked to solve." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-24-393ac32fb2245f828c7f158d3608d661.png"></p><p>A sample set of social media account management tasks that the Agent has been asked to solve.</p><p>Note that there is a scroll bar in this task that the Agent needs to learn how to use! The tweet that&#x27;s relevant to this task may be hidden from the visible part of the screen, so the Agent will have to actively explore (by sliding the scroll bar up/down) in order to progress!</p><p>Let&#x27;s visually explore how a well-trained Agent progresses through social media management tasks. The following screenshot shows the Agent learning to use the scroll bar to &quot;navigate&quot; in this environment:</p><p><img alt="The Agent learning to navigate using the scroll bar." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-25-c61c7b018abf457f37708c98aa2fa10d.png"></p><p>The Agent learning to navigate using the scroll bar.</p><p>Note thatÂ the task specification does not imply anything related to the scroll bar or the navigation, and that the Agent was able to explore and figure out that it needs to navigate in order to progress with the task! The following screenshot shows the Agent progressing much further by choosing the correct tweet but clicking on the wrong action; that is,Â <strong>Embed Tweet</strong>Â instead of theÂ <strong>Mute</strong>Â button:</p><p><img alt="The Agent clicking on Embed Tweet when the goal was to click on Mute." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-26-f9893df49a6ca1d745b2b2aa82990b8e.png"></p><p>The Agent clicking on Embed Tweet when the goal was to click on Mute.</p><p>After 96 million episodes of training, the Agent was sufficiently able to solve the task. TheÂ following screenshot shows the Agent&#x27;s performance on an evaluation episode (the Agent was loaded from a checkpoint)</p><p><img alt="The Agent loaded from trained parameters about to complete the task successfully." src="/assets/images/content-tutorials-raw-tensorflow-2-reinforcement-learning-cookbook-untitled-27-c269386731ce7c9b3f1082c26c82c0c1.png"></p><p>The Agent loaded from trained parameters about to complete the task successfully.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="training-stock-trading-rl-agent-and-deploying-as-a-service">Training Stock Trading RL Agent and Deploying as a Service<a class="hash-link" href="#training-stock-trading-rl-agent-and-deploying-as-a-service" title="Direct link to heading">â€‹</a></h3><p><a href="https://github.com/RecoHut-Projects/drl-recsys/blob/main/tutorials/T219631_Training_Stock_Trading_RL_Agent_using_SAC_and_Deploying_as_a_Service.ipynb" target="_blank" rel="noopener noreferrer">Link to notebook â†’</a></p><p>In this, we first implemented the essential components for SAC method. Then we built an RL environment simulator as a service and deployed using flask (at <code>0.0.0.0:6666</code>). It contain two core modules â€“ the tradegym server and the tradegym client, which are built based on the OpenAI Gym HTTP API. We first defined a minimum set of custom environments exposed as part of the tradegym library and then built the server and client modules.</p><p>Then we trained a deep RL agent using remote simulator. And then evaluated the trained RL agent. After evaluation, we packaged this trained RL agent to be deployed as a service and deployed using flask (at <code>0.0.0.0:5555</code>). And after deployment, performed a simple testing to make sure that RL agent as a service is successfully deployed.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">â€‹</a></h2><ol><li><a href="https://learning.oreilly.com/library/view/tensorflow-2-reinforcement/9781838982546" target="_blank" rel="noopener noreferrer">&quot;TensorFlow 2 Reinforcement Learning Cookbook&quot; by Praveen Palanisamy (Packt, 2021)</a></li><li><a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook" target="_blank" rel="noopener noreferrer">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a></li><li><a href="https://github.com/RecoHut-Projects/drl-recsys" target="_blank" rel="noopener noreferrer">https://github.com/RecoHut-Projects/drl-recsys</a></li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/recohut/docs/docs/docs/tutorials/tensorflow-2-reinforcement-learning-cookbook.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/tutorials/session-based-recommendation-with-graph-neural-net"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Session-based Recommendation with Graph Neural Networks</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/datasets"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Datasets</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#process-flow" class="table-of-contents__link toc-highlight">Process flow</a></li><li><a href="#environments" class="table-of-contents__link toc-highlight">Environments</a><ul><li><a href="#simple-gridworld" class="table-of-contents__link toc-highlight">Simple Gridworld</a></li><li><a href="#simple-gridworld-v2" class="table-of-contents__link toc-highlight">Simple Gridworld v2</a></li><li><a href="#stochastic-maze" class="table-of-contents__link toc-highlight">Stochastic Maze</a></li><li><a href="#cartpole" class="table-of-contents__link toc-highlight">CartPole</a></li><li><a href="#mountaincar" class="table-of-contents__link toc-highlight">MountainCar</a></li><li><a href="#pendulum" class="table-of-contents__link toc-highlight">Pendulum</a></li><li><a href="#cryptocurrency-trading" class="table-of-contents__link toc-highlight">Cryptocurrency Trading</a></li><li><a href="#stock-trading" class="table-of-contents__link toc-highlight">Stock Trading</a></li><li><a href="#webgym-world-of-bits-wob" class="table-of-contents__link toc-highlight">WebGym World-of-Bits (WoB)</a></li><li><a href="#building-a-simple-gridworld-environment" class="table-of-contents__link toc-highlight">Building a simple Gridworld Environment</a></li><li><a href="#building-a-simple-gridworld-v2-environment" class="table-of-contents__link toc-highlight">Building a simple Gridworld v2 Environment</a></li><li><a href="#building-a-stochastic-maze-gridworld-environment" class="table-of-contents__link toc-highlight">Building a Stochastic Maze Gridworld Environment</a></li><li><a href="#training-rl-agent-in-gridworld-with-mlp-model" class="table-of-contents__link toc-highlight">Training RL Agent in Gridworld with MLP Model</a></li><li><a href="#training-rl-agent-in-maze-gridworld-with-value-iteration" class="table-of-contents__link toc-highlight">Training RL Agent in Maze Gridworld with Value-iteration</a></li><li><a href="#training-rl-agent-in-gridworld-with-temporal-difference" class="table-of-contents__link toc-highlight">Training RL Agent in Gridworld with Temporal Difference</a></li><li><a href="#training-rl-agent-in-gridworld-with-monte-carlo" class="table-of-contents__link toc-highlight">Training RL Agent in Gridworld with Monte-Carlo</a></li><li><a href="#training-rl-agent-in-gridworld-with-sarsa" class="table-of-contents__link toc-highlight">Training RL Agent in Gridworld with SARSA</a></li><li><a href="#training-rl-agent-in-gridworld-with-q-learning" class="table-of-contents__link toc-highlight">Training RL Agent in Gridworld with Q-learning</a></li><li><a href="#training-rl-agent-in-cartpole-env-with-actor-critic" class="table-of-contents__link toc-highlight">Training RL Agent in CartPole Env. with Actor-Critic</a></li><li><a href="#training-rl-agent-in-cartpole-env-with-dqn" class="table-of-contents__link toc-highlight">Training RL Agent in CartPole Env. with DQN</a></li><li><a href="#training-rl-agent-in-cartpole-env-with-dueling-dqn" class="table-of-contents__link toc-highlight">Training RL Agent in CartPole Env. with Dueling DQN</a></li><li><a href="#training-rl-agent-in-cartpole-env-with-drqn" class="table-of-contents__link toc-highlight">Training RL Agent in CartPole Env. with DRQN</a></li><li><a href="#training-rl-agent-in-mountaincar-env-with-policy-gradient" class="table-of-contents__link toc-highlight">Training RL Agent in MountainCar Env. with Policy Gradient</a></li><li><a href="#training-rl-agent-in-mountaincar-env-with-a3c-continuous" class="table-of-contents__link toc-highlight">Training RL Agent in MountainCar Env. with A3C Continuous</a></li><li><a href="#training-rl-agent-in-pendulum-env-with-ppo-continuous" class="table-of-contents__link toc-highlight">Training RL Agent in Pendulum Env. with PPO Continuous</a></li><li><a href="#training-rl-agent-in-pendulum-env-with-ddpg" class="table-of-contents__link toc-highlight">Training RL Agent in Pendulum Env. with DDPG</a></li><li><a href="#building-bitcoin-and-ethereum-cryptocurrency-trading-env" class="table-of-contents__link toc-highlight">Building Bitcoin and Ethereum Cryptocurrency Trading Env.</a></li><li><a href="#training-rl-agent-for-trading-cryptocurrencies-with-sac" class="table-of-contents__link toc-highlight">Training RL Agent for Trading Cryptocurrencies with SAC</a></li><li><a href="#building-stock-trading-rl-environment" class="table-of-contents__link toc-highlight">Building Stock Trading RL Environment</a></li><li><a href="#training-rl-agent-for-trading-stocks-with-sac" class="table-of-contents__link toc-highlight">Training RL Agent for Trading Stocks with SAC</a></li><li><a href="#building-rl-agent-to-complete-tasks-on-the-web--call-to-action" class="table-of-contents__link toc-highlight">Building RL Agent to complete tasks on the web â€“ Call to Action</a></li><li><a href="#building-rl-agent-to-auto-login-on-the-web" class="table-of-contents__link toc-highlight">Building RL Agent to auto-login on the web</a></li><li><a href="#building-rl-agent-to-book-flights-on-the-web" class="table-of-contents__link toc-highlight">Building RL Agent to book flights on the web</a></li><li><a href="#building-rl-agent-to-manage-emails-on-the-web" class="table-of-contents__link toc-highlight">Building RL Agent to manage emails on the web</a></li><li><a href="#building-rl-agent-to-manage-social-media-accounts-on-the-web" class="table-of-contents__link toc-highlight">Building RL Agent to manage social media accounts on the web</a></li><li><a href="#training-stock-trading-rl-agent-and-deploying-as-a-service" class="table-of-contents__link toc-highlight">Training Stock Trading RL Agent and Deploying as a Service</a></li></ul></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Recohut Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.19f23f4b.js"></script>
<script src="/assets/js/main.f49af512.js"></script>
</body>
</html>