---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.7
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="YpmJoZkdZLce" -->
# Shallow embedding methods
<!-- #endregion -->

<!-- #region id="6CT04PsMT72h" -->
## Setup
<!-- #endregion -->

```python id="kkSK4SzkQ6fo"
!pip install git+https://github.com/palash1992/GEM.git
!pip install karateclub
!pip install node2vec
!pip install python-Levenshtein
!pip install gensim==3.8.0
!pip install -U Ipython
```

```python id="cGtvqiNAQpoO"
import matplotlib.pyplot as plt
import networkx as nx
from pathlib import Path
from gem.embedding.gf import GraphFactorization
from karateclub.node_embedding.neighbourhood.grarep import GraRep
from gem.embedding.hope import HOPE
from karateclub.node_embedding.neighbourhood.deepwalk import DeepWalk
from node2vec import Node2Vec
import random
from IPython.display import Code
import inspect
from karateclub import Graph2Vec
from node2vec.edges import HadamardEmbedder

%matplotlib inline

import warnings
warnings.filterwarnings('ignore')
```

```python id="OpBmZR8_QlxT"
def draw_graph(G, node_names={}, node_size=500):
    pos_nodes = nx.spring_layout(G)
    nx.draw(G, pos_nodes, with_labels=True, node_size=node_size, edge_color='gray', arrowsize=30)
    
    pos_attrs = {}
    for node, coords in pos_nodes.items():
        pos_attrs[node] = (coords[0], coords[1] + 0.08)
        
    nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)
    
    plt.axis('off')
    axis = plt.gca()
    axis.set_xlim([1.2*x for x in axis.get_xlim()])
    axis.set_ylim([1.2*y for y in axis.get_ylim()])
    plt.show()
```

<!-- #region id="gUatPmm-UlNJ" -->
## Theory
<!-- #endregion -->

<!-- #region id="U8YM7nq8UmaP" -->
<!-- #endregion -->

<!-- #region id="ZKXy4PGOUt7D" -->
**Shallow embedding methods**

These methods are able to learn and return only the embedding values for the learned input data. Generally speaking, all the unsupervised embedding algorithms based on matrix factorization use the same principle. They all factorize an input graph expressed as a matrix in different components (commonly knows as matrix factorization). The main difference between each method lies in the loss function used during the optimization process. Indeed, different loss functions allow creating an embedding space that emphasizes specific properties of the input graph.
<!-- #endregion -->

<!-- #region id="TG7TgIW4SMP4" -->
## Graph Factorization
<!-- #endregion -->

<!-- #region id="gKYsEA7_U3J4" -->
The GF algorithm was one of the first models to reach good computational performance in order to perform the node embedding of a given graph. The loss function used in this method was mainly designed to improve GF performances and scalability. Indeed, the solution generated by this method could be noisy. Moreover, it should be noted, by looking at its matrix factorization formulation, that GF performs a strong symmetric factorization. This property is particularly suitable for undirected graphs, where the adjacency matrix is symmetric, but could be a potential limitation for undirected graphs.
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 1000} id="7cYqOeK3bS_g" executionInfo={"status": "ok", "timestamp": 1627934167090, "user_tz": -330, "elapsed": 1538, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="afcdfe0e-c7c0-411d-e476-44a98837dd02"
Code(inspect.getsource(GraphFactorization), language='python')
```

```python id="86H0RRJrQlxg"
Path("gem/intermediate").mkdir(parents=True, exist_ok=True)
```

```python id="0TPyCBh-Qlxe"
G = nx.barbell_graph(m1=10, m2=4)
```

```python id="J-Ktrld-Qlxh" colab={"base_uri": "https://localhost:8080/"} executionInfo={"status": "ok", "timestamp": 1627934990483, "user_tz": -330, "elapsed": 13085, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="25f847f2-5b26-43a1-f76a-beb615908b8d"
gf = GraphFactorization(d=2, data_set=None, max_iter=10000, eta=1*10**-4, regu=1.0)
gf.learn_embedding(G)
```

```python colab={"base_uri": "https://localhost:8080/", "height": 319} id="H4Q_1mbqfX5u" executionInfo={"status": "ok", "timestamp": 1627935051273, "user_tz": -330, "elapsed": 1399, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="89754612-727d-4dbb-dc33-ab75585d73c5"
draw_graph(G)
```

```python id="a3u7f_GJQlxj" colab={"base_uri": "https://localhost:8080/", "height": 592} executionInfo={"status": "ok", "timestamp": 1627935038372, "user_tz": -330, "elapsed": 1661, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="2a6344b7-97dd-418d-edcf-1b359131474c"
fig, ax = plt.subplots(figsize=(10,10))

for x in G.nodes():
    v = gf.get_embedding()[x]
    ax.scatter(v[0],v[1], s=1000)
    ax.annotate(str(x), (v[0],v[1]), fontsize=12)
```

<!-- #region id="RhRSTHDSQlxk" -->
## GraphRep
<!-- #endregion -->

<!-- #region id="X1USckrPft01" -->
Graph representation with global structure information (GraphRep), such as HOPE, allows us to preserve higher-order proximity without forcing its embeddings to have symmetric properties.
<!-- #endregion -->

```python id="f0606l_vQlxn" colab={"base_uri": "https://localhost:8080/", "height": 319} executionInfo={"status": "ok", "timestamp": 1627976582146, "user_tz": -330, "elapsed": 609, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="95547bdb-2ced-4c03-8c68-4727445ff12c"
G = nx.barbell_graph(m1=10, m2=4)
draw_graph(G)
```

<!-- #region id="rBzlRkiM-I51" -->
We initialize the GraRep class from the karateclub library. In this implementation, the dimension parameter represents the dimension of the embedding space, while the order parameter defines the maximum number of orders of proximity between nodes. The number of columns of the final embedding matrix (stored, in the example, in the embeddings variable) is `dimension x order`, since, as we said, for each proximity order an embedding is computed and concatenated in the final embedding matrix.
<!-- #endregion -->

```python id="iMEoVgxQSiEa"
gr = GraRep(dimensions=2,order=3)
gr.fit(G)
```

```python colab={"base_uri": "https://localhost:8080/"} id="2sfjgc3e-eqw" executionInfo={"status": "ok", "timestamp": 1627976781352, "user_tz": -330, "elapsed": 8, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="3c0da69c-a39f-492e-a919-75d61d431bb3"
embeddings = gr.get_embedding()
embeddings.shape
```

<!-- #region id="OsiK0yTX-2Sz" -->
To specify, since two dimensions are computed in the example, embeddings[:,:2] represents the embedding obtained for k=1, embeddings[:,2:4] for k=2, and embeddings[:,4:] for k=3. The results of the code are shown in the following graph:
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 336} id="m019HZNz_AKJ" executionInfo={"status": "ok", "timestamp": 1627977163242, "user_tz": -330, "elapsed": 1738, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="95a026d4-7102-47cc-f3ef-3239e1f33d7e"
fig, ax = plt.subplots(1, 3, figsize=(16,5))

for x in G.nodes():
    v = gr.get_embedding()[x]
    ax[0].scatter(v[0],v[1], s=1000)
    ax[0].annotate(str(x), (v[0],v[1]), fontsize=12)
    ax[0].set_title('k=1')
    ax[1].scatter(v[2],v[3], s=1000)
    ax[1].annotate(str(x), (v[2],v[3]), fontsize=12)
    ax[1].set_title('k=2')
    ax[2].scatter(v[4],v[5], s=1000)
    ax[2].annotate(str(x), (v[4],v[5]), fontsize=12)
    ax[2].set_title('k=3')

plt.show()
```

<!-- #region id="gJoPA7k1Qlxs" -->
## HOPE
<!-- #endregion -->

```python id="mZ74JG4rQlxt" colab={"base_uri": "https://localhost:8080/", "height": 319} executionInfo={"status": "ok", "timestamp": 1627931732010, "user_tz": -330, "elapsed": 2507, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="ca0e41c7-428f-448c-fa15-3ff0495e59d9"
G = nx.barbell_graph(m1=10, m2=4)
draw_graph(G)
```

```python colab={"base_uri": "https://localhost:8080/"} id="B6tUyH98St9-" executionInfo={"status": "ok", "timestamp": 1627931732650, "user_tz": -330, "elapsed": 11, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="9679dc1b-35b0-483e-89b0-ed9fc88dd582"
hp = HOPE(d=4, beta=0.01)
hp.learn_embedding(G)
```

```python id="XiibNAuvQlxu" colab={"base_uri": "https://localhost:8080/", "height": 592} executionInfo={"status": "ok", "timestamp": 1627931736338, "user_tz": -330, "elapsed": 1588, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="51689d79-342f-491c-eb7c-ba7703057f74"
fig, ax = plt.subplots(figsize=(10,10))

for x in G.nodes():
    
    v = hp.get_embedding()[x,2:]
    ax.scatter(v[0],v[1], s=1000)
    ax.annotate(str(x), (v[0],v[1]), fontsize=20)
```

<!-- #region id="PEPqIc22Qlxv" -->
## DeepWalk
<!-- #endregion -->

<!-- #region id="Xo1ul4QZJWaz" -->
*fig: All the steps used by the DeepWalk algorithm to generate the node embedding of a given graph*
<!-- #endregion -->

<!-- #region id="PgrJSuhtJUAH" -->
<!-- #endregion -->

```python id="BJnxmxdbQlxw" colab={"base_uri": "https://localhost:8080/", "height": 319} executionInfo={"status": "ok", "timestamp": 1627931777541, "user_tz": -330, "elapsed": 3085, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="657d3357-cd5b-4ac2-e142-027d08387181"
G = nx.barbell_graph(m1=10, m2=4)
draw_graph(G)
```

```python colab={"base_uri": "https://localhost:8080/", "height": 1000} id="Gc6lYTJwJmyc" executionInfo={"status": "ok", "timestamp": 1627979682396, "user_tz": -330, "elapsed": 476, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="e8fd4535-3731-4a97-bd4a-2f1db84cb767"
Code(inspect.getsource(DeepWalk), language='python')
```

```python id="_cjPNrhNS5BS"
dw = DeepWalk(dimensions=2)
dw.fit(G)
```

```python id="C0m9RcpVQlxy" colab={"base_uri": "https://localhost:8080/", "height": 592} executionInfo={"status": "ok", "timestamp": 1627931780210, "user_tz": -330, "elapsed": 1225, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="de20f9cc-71ec-4e69-c43a-16ac3121fab9"
fig, ax = plt.subplots(figsize=(10,10))

for x in G.nodes():
    
    v = dw.get_embedding()[x]
    ax.scatter(v[0],v[1], s=1000)
    ax.annotate(str(x), (v[0],v[1]), fontsize=12)
```

<!-- #region id="DR8kdVm7Qlxy" -->
## Node2Vec
<!-- #endregion -->

<!-- #region id="T_uT5zJXKBd9" -->
The Node2Vec algorithm can be seen as an extension of DeepWalk. Indeed, as with DeepWalk, Node2Vec also generates a set of random walks used as input to a skip-gram model. Once trained, the hidden layers of the skip-gram model are used to generate the embedding of the node in the graph. The main difference between the two algorithms lies in the way the random walks are generated.

Indeed, if DeepWalk generates random walks without using any bias, in Node2Vec a new technique to generate biased random walks on the graph is introduced. The algorithm to generate the random walks combines graph exploration by merging Breadth-First Search (BFS) and Depth-First Search (DFS). The way those two algorithms are combined in the random walk's generation is regularized by two parameters p, and q. p defines the probability of a random walk getting back to the previous node, while q defines the probability that a random walk can pass through a previously unseen part of the graph.

Due to this combination, Node2Vec can preserve high-order proximities by preserving local structures in the graph as well as global community structures. This new method of random walk generation allows solving the limitation of DeepWalk preserving the local neighborhood properties of the node.
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 1000} id="J6CitHqNJvLd" executionInfo={"status": "ok", "timestamp": 1627979714828, "user_tz": -330, "elapsed": 801, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="7db5eaff-394e-4748-a5ff-dda7cd53c1b7"
Code(inspect.getsource(Node2Vec), language='python')
```

```python id="ax0ifBaoQlxz" colab={"base_uri": "https://localhost:8080/", "height": 319} executionInfo={"status": "ok", "timestamp": 1627931949785, "user_tz": -330, "elapsed": 1362, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="87244310-a86e-4c1b-dd6b-a279515424e6"
G = nx.barbell_graph(m1=10, m2=4)
draw_graph(G)
```

```python colab={"base_uri": "https://localhost:8080/", "height": 83, "referenced_widgets": ["2a11bdd49a0b490db73725d5a135797d", "7d2450c6de4543aab74badadf0b92a1a", "fa8be0234feb48b38156e99a50693e5a", "97dbbd8e38164a3fadcd6c517518c973", "cfc96c2d008d421b9d4200854181b07e", "e32699a491f6420aa17dac75ae9fc474", "3d01384b00014acd9e0f073de6cfe9d6", "16a1ba6c00024c5ebe27d1a401fc5834"]} id="ZNmB6MsxTGpx" executionInfo={"status": "ok", "timestamp": 1627931958117, "user_tz": -330, "elapsed": 1775, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="2528e672-61ef-4a08-edeb-f98881358d0e"
node2vec = Node2Vec(G, dimensions=2)
model = node2vec.fit(window=10)
embeddings = model.wv
```

```python id="YX1VbkRfQlx0" colab={"base_uri": "https://localhost:8080/", "height": 592} executionInfo={"status": "ok", "timestamp": 1627931990901, "user_tz": -330, "elapsed": 1805, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="8dcfefb1-f9e9-4a7f-8364-3172fae5d594"
fig, ax = plt.subplots(figsize=(10,10))

for x in G.nodes():
    
    v = model.wv[str(x)]
    ax.scatter(v[0],v[1], s=1000)
    ax.annotate(str(x), (v[0],v[1]), fontsize=16)

plt.show()
```

<!-- #region id="6t-IgDwFQlx0" -->
## Edge2Vec
<!-- #endregion -->

<!-- #region id="2t_klo3ZKdCf" -->
Contrary to the other embedding function, the Edge to Vector (Edge2Vec) algorithm generates the embedding space on edges, instead of nodes. This algorithm is a simple side effect of the embedding generated by using Node2Vec. The main idea is to use the node embedding of two adjacent nodes to perform some basic mathematical operations in order to extract the embedding of the edge connecting them.

The operators described in the below table can be used in order to compute the embedding of their edge:
<!-- #endregion -->

<!-- #region id="Wg6FhDbaKnxB" -->
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 136} id="CvMZJ9L4LOfI" executionInfo={"status": "ok", "timestamp": 1627980104159, "user_tz": -330, "elapsed": 616, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="f79f856a-f1c0-499f-8770-f659a38b5c9f"
Code(inspect.getsource(HadamardEmbedder), language='python')
```

```python colab={"base_uri": "https://localhost:8080/", "height": 1000} id="8VnpqibeLTUx" executionInfo={"status": "ok", "timestamp": 1627980166869, "user_tz": -330, "elapsed": 19, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="918f8a54-2380-43e5-ee97-10afe7440e53"
from node2vec.edges import EdgeEmbedder
Code(inspect.getsource(EdgeEmbedder), language='python')
```

```python id="uVly06JaQlx1"
edges_embs = HadamardEmbedder(keyed_vectors=model.wv)
```

```python id="IqXTXnepQlx1" colab={"base_uri": "https://localhost:8080/", "height": 592} executionInfo={"status": "ok", "timestamp": 1627931999485, "user_tz": -330, "elapsed": 2146, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="bfb6bd61-0f26-4b81-9238-1954eb753c13"
fig, ax = plt.subplots(figsize=(10,10))

for x in G.edges():
    
    v = edges_embs[(str(x[0]), str(x[1]))]
    ax.scatter(v[0],v[1], s=1000)
    ax.annotate(str(x), (v[0],v[1]), fontsize=16)

plt.show()
```

<!-- #region id="7NW8nxuwQlx2" -->
## Graph2Vec
<!-- #endregion -->

<!-- #region id="nkB0BRuNK9rd" -->
To specify, given a set of graphs, the Graph2Vec algorithms generate an embedding space where each point represents a graph. This algorithm generates its embedding using an evolution of the Word2Vec skip-gram model known as Document to Vector (Doc2Vec). We can graphically see a simplification of this model as follows:
<!-- #endregion -->

<!-- #region id="XkNYX-dmLBh0" -->
<!-- #endregion -->

```python id="07C2M8shQlx2" colab={"base_uri": "https://localhost:8080/", "height": 598} executionInfo={"status": "ok", "timestamp": 1627932036891, "user_tz": -330, "elapsed": 1574, "user": {"displayName": "Sparsh Agarwal", "photoUrl": "", "userId": "13037694610922482904"}} outputId="40e20d59-d533-4ca4-bcfc-f290193845b9"
n_graphs = 20

def generate_radom():
    n = random.randint(6, 20)
    k = random.randint(5, n)
    p = random.uniform(0, 1)
    return nx.watts_strogatz_graph(n,k,p), [n,k,p]

Gs = [generate_radom() for x in range(n_graphs)]

model = Graph2Vec(dimensions=2, wl_iterations=10)
model.fit([x[0] for x in Gs])
embeddings = model.get_embedding()

fig, ax = plt.subplots(figsize=(10,10))

for i,vec in enumerate(embeddings):
    
    ax.scatter(vec[0],vec[1], s=1000)
    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)
```
