# Python code

## Clean filenames in a folder

```python
import re
import os
import pathlib
import glob

nbs = glob.glob('/content/docs/docs/tutorials/*.md')

def process(path):
    x = str(pathlib.Path(path).stem)
    x = x.lower()
    x = re.sub(r'[^a-z]','-',x)
    x = re.sub(r'-+','-',x)
    x = x.strip('-')
    x = os.path.join(str(pathlib.Path(path).parent), x+'.mdx')
    x = re.sub('/[a-z]\-','/',x)
    os.rename(path, x)

_ = [process(x) for x in nbs]
```

## Converting Jupyter notebooks into markdown

```python
!cd /content && git clone https://github.com/recohut/nbs.git
!pip install -q jupytext
!cd /content/nbs && jupytext *.ipynb --to markdown
```

```python
import glob
import os

nbs = glob.glob('/content/nbs/*.ipynb')

for x in nbs:
    mds = x[:-6]+'.md'
    if not os.path.exists(mds):
        try:
          !jupyter nbconvert --to markdown "{x}"
        except:
            print('error in {}'.format(x))
```

## Scraping

### BS4

<a href="https://nbviewer.org/github/recohut/nbs/blob/main/2020-06-20-simple-scraping-nlp-bs4-distilbert.ipynb" alt=""> <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>

```python
import bs4
import requests
import lxml.etree as xml

URLs = ["https://www.flexjobs.com/blog/post/job-search-strategies-for-success-v2/",
        "https://www.monster.com/career-advice/article/five-ps-of-job-search-progress"]

i = 0
web_page = bs4.BeautifulSoup(requests.get(URLs[i], {}).text, "lxml")
df.loc[i,'title'] = web_page.head.title.text
sub_web_page = web_page.find_all(name="article", attrs={"class": "single-post-page"})[0]
article = '. '.join([wp.text for wp in sub_web_page.find_all({"h2","p"})])
df.loc[i,'text'] = article
        
```

## Vector Search

### Faiss

<a href="https://nbviewer.org/github/recohut/nbs/blob/main/2021-01-23-vector-search.ipynb" alt=""> <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>
<a href="https://nbviewer.org/github/recohut/nbs/blob/main/2021-04-20-dl-retrieval.ipynb" alt=""> <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>


```python
import faiss
from vector_engine.utils import vector_search, id2details

# Step 1: Change data type
embeddings = np.array([embedding for embedding in embeddings]).astype("float32")

# Step 2: Instantiate the index
index = faiss.IndexFlatL2(embeddings.shape[1])

# Step 3: Pass the index to IndexIDMap
index = faiss.IndexIDMap(index)

# Step 4: Add vectors and their IDs
index.add_with_ids(embeddings, df.id.values)

print(f"Number of vectors in the Faiss index: {index.ntotal}")

# Retrieve the 10 nearest neighbours
D, I = index.search(np.array([embeddings[5415]]), k=10)
print(f'L2 distance: {D.flatten().tolist()}\n\nMAG paper IDs: {I.flatten().tolist()}')

# Wrap all steps in the vector_search function.
# It takes four arguments: 
# A query, the sentence-level transformer, the Faiss index and the number of requested results
D, I = vector_search([user_query], model, index, num_results=10)
print(f'L2 distance: {D.flatten().tolist()}\n\nMAG paper IDs: {I.flatten().tolist()}')

# Serialise index and store it as a pickle
with open(f"{project_dir}/models/faiss_index.pickle", "wb") as h:
    pickle.dump(faiss.serialize_index(index), h)
```

### Elasticsearch

<a href="https://nbviewer.org/github/recohut/nbs/blob/main/2021-04-20-dl-retrieval.ipynb" alt=""> <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>

```python
# download the latest elasticsearch version
!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.11.1-linux-x86_64.tar.gz
!tar -xzvf elasticsearch-7.11.1-linux-x86_64.tar.gz
!chown -R daemon:daemon elasticsearch-7.11.1

# prep the elasticsearch server
import os
from subprocess import Popen, PIPE, STDOUT
es_subprocess = Popen(['elasticsearch-7.11.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda : os.setuid(1))

# wait for a few minutes for the local host to start
!curl -X GET "localhost:9200/"

# install elasticsearch python api
!pip install -q elasticsearch

# check if elasticsearch server is properly running in the background
from elasticsearch import Elasticsearch, helpers
es_client = Elasticsearch(['localhost'])
es_client.info()
```