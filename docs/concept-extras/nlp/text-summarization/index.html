<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-react-helmet="true">Text Summarization | Recohut</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://docs.recohut.com/docs/concept-extras/nlp/text-summarization"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Text Summarization | Recohut"><meta data-react-helmet="true" name="description" content="/img/content-concepts-raw-nlp-text-summarization-untitled.png"><meta data-react-helmet="true" property="og:description" content="/img/content-concepts-raw-nlp-text-summarization-untitled.png"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://docs.recohut.com/docs/concept-extras/nlp/text-summarization"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/concept-extras/nlp/text-summarization" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/concept-extras/nlp/text-summarization" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.8d002642.css">
<link rel="preload" href="/assets/js/runtime~main.5f97d5c0.js" as="script">
<link rel="preload" href="/assets/js/main.a34fef53.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Recohut</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/concept-basics/challenges">Concept - Basics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_TwRn" href="/docs/concept-extras/bias-&amp;-fairness">Concept - Extras</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/bias-&amp;-fairness">Bias &amp; Fairness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/causal-inference">Causal Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/cold-start">Cold Start</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/data-science">Data Science</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/diversity">Diversity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/emerging-concepts-in-recommender-systems">Emerging Concepts in Recommender Systems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/graph-embeddings">Graph Embeddings</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/incremental-learning">Incremental Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/mlops">MLOps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/model-deployment">Model Deployment</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_TwRn" tabindex="0" href="/docs/concept-extras/nlp/chatbot">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/chatbot">Chatbot</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/language-modeling">Language Modeling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/named-entity-recognition">Named Entity Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/text-analysis">Text Analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/text-classification">Text Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/text-generation">Text Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/text-similarity">Text Similarity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/text-style-transfer">Text Style Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/concept-extras/nlp/text-summarization">Text Summarization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/topic-modeling">Topic Modeling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/nlp/transformers">Transformers</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" tabindex="0" href="/docs/concept-extras/success-stories/1mg-prod2vec">Success Stories</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" tabindex="0" href="/docs/concept-extras/vision/facial-analytics">Computer Vision</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/models/">Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Models&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/tutorials/">Tutorials</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorials&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/datasets">Datasets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/projects">Projects</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Text Summarization</h1></header><p><img alt="/img/content-concepts-raw-nlp-text-summarization-untitled.png" src="/assets/images/content-concepts-raw-nlp-text-summarization-untitled-bd99dd1e058e0a3b558b8d3ebb4be74c.png"></p><p>Automatic Text Summarization gained attention as early as the 1950’s. A <a href="http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf" target="_blank" rel="noopener noreferrer">research paper</a>, published by Hans Peter Luhn in the late 1950s, titled “The automatic creation of literature abstracts”, used features such as word frequency and phrase frequency to extract important sentences from the text for summarization purposes.</p><p>Another important <a href="http://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf" target="_blank" rel="noopener noreferrer">research</a>, done by Harold P Edmundson in the late 1960’s, used methods like the presence of cue words, words used in the title appearing in the text, and the location of sentences, to extract significant sentences for text summarization. Since then, many important and exciting studies have been published to address the challenge of automatic text summarization.</p><p>Text summarization can broadly be divided into two categories — <strong>Extractive Summarization</strong> and <strong>Abstractive Summarization</strong>.</p><ol><li><strong>Extractive Summarization:</strong> These methods rely on extracting several parts, such as phrases and sentences, from a piece of text and stack them together to create a summary. Therefore, identifying the right sentences for summarization is of utmost importance in an extractive method.</li><li><strong>Abstractive Summarization:</strong> These methods use advanced NLP techniques to generate an entirely new summary. Some parts of this summary may not even appear in the original text.</li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><ul><li><strong>Definition:</strong> <em>To take the appropriate action, we need the latest information, but on the contrary, the amount of information is more and more growing. Making an automatic &amp; accurate summaries feature will helps us to understand the topics and shorten the time to do it.</em></li><li><strong>Applications:</strong> News Summarization, Social media Summarization, Entity timelines, Storylines of event, Domain specific summaries, Sentence Compression, Event understanding, Summarization of user-generated content</li><li><strong>Scope:</strong> Extractive and Abstractive summary<ul><li>Single document summarization: summary = summarize(document)</li><li>Multi-document summarization: summary = summarize(document_1, document_2, ...)</li><li>Query focused summarization: summary = summarize(document, query)</li><li>Update summarization: summary = summarize(document, previous_document_or_summary)</li></ul></li><li><strong>Tools:</strong> HuggingFace Transformer Library</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="scheduled-sampling">Scheduled sampling<a class="hash-link" href="#scheduled-sampling" title="Direct link to heading">​</a></h2><p>In the inference (testing) phase, the model only depends on the previous step, which means that it totally depends on itself. The problem actually arises when the model results in a bad output in (t-1) (i.e. the previous time step results in a bad output). This would actually affect all the coming sequences. It would lead the model to an entirely different state space from where it has seen and trained on in the training phase, so it simply won’t be able to know what to do. A solution to this problem that has been suggested by <a href="https://arxiv.org/abs/1506.03099" target="_blank" rel="noopener noreferrer">bengio et ai</a> from google research, was to gradually change the reliance of the model from being totally dependent on the ground truth being supplied to it to depending on itself (i.e. depend on only its previous tokens generated from previous time steps in the decoder). The concept of making the learning path difficult through time (i.e. making the model depends on only itself) is called curriculum learning. Their technique to implement this was truly genius. They call it ‘<strong>scheduled sampling’.</strong></p><p><img alt="Landscape of seq2seq models for neural abstractive text summarization" src="/assets/images/content-concepts-raw-nlp-text-summarization-image_(4)-5f8409e2607f0b09a07bf3afd2bb425b.png"></p><p>Landscape of seq2seq models for neural abstractive text summarization</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="models">Models<a class="hash-link" href="#models" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="prophetnet">ProphetNet<a class="hash-link" href="#prophetnet" title="Direct link to heading">​</a></h3><p><em><a href="https://arxiv.org/abs/2001.04063" target="_blank" rel="noopener noreferrer">ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training. arXiv, 2020.</a></em></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pegasus">PEGASUS<a class="hash-link" href="#pegasus" title="Direct link to heading">​</a></h3><p><em><a href="https://arxiv.org/abs/1912.08777" target="_blank" rel="noopener noreferrer">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. arXiv, 2019.</a></em></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="bertsum">BERTSum<a class="hash-link" href="#bertsum" title="Direct link to heading">​</a></h3><p><em><a href="https://arxiv.org/pdf/1903.10318.pdf" target="_blank" rel="noopener noreferrer">Fine-tune BERT for Extractive Summarization. arXiv, 2019.</a></em></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="seq2seq-pointergenerator">Seq2Seq PointerGenerator<a class="hash-link" href="#seq2seq-pointergenerator" title="Direct link to heading">​</a></h3><p><em><a href="https://arxiv.org/abs/1704.04368v2" target="_blank" rel="noopener noreferrer">Get To The Point: Summarization with Pointer-Generator Networks. arXiv, 2017.</a></em></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="process-flow">Process flow<a class="hash-link" href="#process-flow" title="Direct link to heading">​</a></h2><p>Step 1: Collect Text Data</p><p>Fetch the raw text dataset into a directory.</p><p>Step 2: Create Labels</p><p>Step 3: Model Training &amp; Validation</p><p>Step 4: UAT Testing</p><p>Wrap the model inference engine in API for client testing. We will receive a text document from the user, encode it with our text encoder, find TopK similar vectors using Indexing object, and retrieve the text documents (and metadata) using dictionaries. We send these documents (and metadata) back to the user.</p><p>Step 5: Deployment</p><p>Deploy the model on cloud or edge as per the requirement.</p><p>Step 6: Documentation</p><p>Prepare the documentation and transfer all assets to the client.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="use-cases">Use Cases<a class="hash-link" href="#use-cases" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="enron-email-summarization">Enron Email Summarization<a class="hash-link" href="#enron-email-summarization" title="Direct link to heading">​</a></h3><p>Email overload can be a difficult problem to manage for both work and personal email inboxes. With the average office worker receiving between 40 to 90 emails a day, it has become difficult to extract the most important information in an optimal amount of time. A system that can create concise and coherent summaries of all emails received within a timeframe can reclaim a large amount of time. Check out <a href="https://www.notion.so/Enron-Email-Summarization-d137f618b4c5445fb595714fdc30c68d" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pdf-summarization-over-mail">PDF Summarization over mail<a class="hash-link" href="#pdf-summarization-over-mail" title="Direct link to heading">​</a></h3><p>Built a system that will receive a pdf over outlook mail and create a word cloud for this pdf. Then, send this word cloud back as an attachment to that email. Check out <a href="https://www.notion.so/PDF-to-Wordcloud-via-Mail-b7ae38d0e95e439eb68194b66bdcb889" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="bart-text-summarization">BART Text Summarization<a class="hash-link" href="#bart-text-summarization" title="Direct link to heading">​</a></h3><p>Document text summarization using the BART transformer model and visual API using the Plotly Dash app. Check out <a href="https://www.notion.so/BART-Text-Summarization-on-Plotly-Dash-f023de73b80c43bf9856a5ecbf24398b" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="transformers-summarization-experiment">Transformers Summarization Experiment<a class="hash-link" href="#transformers-summarization-experiment" title="Direct link to heading">​</a></h3><p>Experiment with various transformers for text summarization using HuggingFace library. Summarization of 4 books. Check out <a href="https://www.notion.so/HuggingFace-Transformers-based-Summarization-082d7af9c97944188f735c761cd27b9b" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cnn-dailymail-and-inshorts-news-summarization">CNN-DailyMail and InShorts News Summarization<a class="hash-link" href="#cnn-dailymail-and-inshorts-news-summarization" title="Direct link to heading">​</a></h3><p>Check out <a href="https://www.notion.so/CNN-DailyMail-News-Summarization-f831bc810195478985aa6356bd43465a" target="_blank" rel="noopener noreferrer">this</a> notion for CNN-DailyMail and <a href="https://www.notion.so/InShorts-News-Summarization-daeb6fe3aa924a24a88657937ab43145" target="_blank" rel="noopener noreferrer">this</a> one for InShorts.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="transformers-summarization-experiment-1">Transformers Summarization Experiment<a class="hash-link" href="#transformers-summarization-experiment-1" title="Direct link to heading">​</a></h3><p>Experiment with various transformers for text summarization using HuggingFace library. Summarization of 4 books. Check out <a href="https://www.notion.so/HuggingFace-Transformers-based-Summarization-082d7af9c97944188f735c761cd27b9b" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="covid-19-article-summarization">Covid-19 article summarization<a class="hash-link" href="#covid-19-article-summarization" title="Direct link to heading">​</a></h3><p>Used BERT and GPT-2 for article summarization related to covid-19. Check out <a href="https://www.notion.so/Text-Summarization-of-Articles-ec756ff596614f29b6896927e87609b1" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="common-applications">Common Applications<a class="hash-link" href="#common-applications" title="Direct link to heading">​</a></h2><ul><li>News Summarization: The summarization systems leverage the power of multi-document
summarization techniques in order to summarize the news coming from various sources.
It generates a compact summary that is informative and non-redundant. Methods
used can be extractive or abstractive. For example, the ’Inshort’ application generates a sixty-word summary of news articles.</li><li>Social media Summarization: deals with the summarization of social media text such as:
tweets, blogs, community forums, etc. These summarization systems are built keeping in
mind the needs of the user and are dependent on the genre of social media text. Tweets
summarization system will be different from blog summarization systems as tweets have
a short text (140 characters) and are often noisy. While the blog has considerably longer
length text with a different writing style.</li><li>Entity timelines: The system generates the summary of most salient events related to
an entity within a given timeline. The news collections from various news sources are
collected over a period of time. Each of these news collections define a main event. For
a given entity, these news collection are identified and ranked in order of importance.
Most salient sentences with respect to the given entity is selected from each of these news
cluster to finally generate an entity timeline.</li><li>Storylines of event: deals with identifying and summarization of events that leads
to event of interest. It helps in providing background information about an event and
structured timeline for an entity. The news collections from various news sources are
collected over a period of time. Each of these news collections define a main even. A
graph of events (news collection) is defined using similarity, then heaviest path ending in
a given event is identified and finally, the events on this path related to salient entities in
target event are summarized to obtain storylines of the event.</li><li>Domain specific summaries: Summarization systems are often used in generating
domain specific summaries. These systems are designed in accordance with the needs of
the user for a specific domain. For example: legal document summarization deals with
generating summary out of a legal/law documents, medical report summarization has aim
of generating a summary form a patient report history such that it includes all important
clinical events in order of timeline.</li><li>Sentence Compression: generates a short version of a longer sentence. The system
is trained using a parallel corpus containing headlines of news articles and first sentence
of the same article. The headline is assumed to be shorter version of the first sentence.
Recent works based on abstractive neural network approaches has proven to generate high
quality compressed sentences.</li><li>Event understanding: is understanding the way events are referred to in the text and
representing these event mentioning text in predicate argument structure. For example:
Michael marries Sara is represented as <!-- -->[actor]<!-- --> marries <!-- -->[actor]<!-- -->. It is very helpful for semiautomatically updation of knowledge graphs and also for the task of generating headlines
(abstractive summarization).</li><li>Summarization of usergenerated content: deals with summarizing user generated
contents like youtube comments, reviews of products, opinions etc. compact version of
reviews and comment are very helpful in identifying overall sentiment of mass towards
a particular product or topic. Which are often used by the consumers as well as the
platform itself in recommending products.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="variations">Variations<a class="hash-link" href="#variations" title="Direct link to heading">​</a></h2><ul><li>Single document summarization: <em>summary = summarize(document)</em></li><li>Multi-document summarization: <em>summary = summarize(document_1, document_2, ...)</em></li><li>Query focused summarization: <em>summary = summarize(document, query)</em></li><li>Update summarization: <em>summary = summarize(document, previous_document_or_summary)</em></li></ul><p>Basically, we can regard the &quot;summarization&quot; as the &quot;function&quot; its input is document and output is summary. And its input &amp; output type helps us to categorize the multiple summarization tasks.</p><ul><li>Single document summarization<ul><li><em>summary = summarize(document)</em></li></ul></li><li>Multi-document summarization<ul><li><em>summary = summarize(document_1, document_2, ...)</em></li></ul></li></ul><p>We can take the query to add the viewpoint of summarization.</p><ul><li>Query focused summarization<ul><li><em>summary = summarize(document, query)</em></li></ul></li></ul><p>This type of summarization is called &quot;Query focused summarization&quot; on the contrary to the &quot;Generic summarization&quot;. Especially, a type that set the viewpoint to the &quot;difference&quot; (update) is called &quot;Update summarization&quot;.</p><ul><li>Update summarization<ul><li><em>summary = summarize(document, previous_document_or_summary)</em></li></ul></li></ul><p>And the <em>&quot;summary&quot;</em> itself has some variety.</p><ul><li>Indicative summary<ul><li>It looks like a summary of the book. This summary describes what kinds of the story, but not tell all of the stories especially its ends (so indicative summary has only partial information).</li></ul></li><li>Informative summary<ul><li>In contrast to the indicative summary, the informative summary includes full information of the document.</li></ul></li><li>Keyword summary<ul><li>Not the text, but the words or phrases from the input document.</li></ul></li><li>Headline summary<ul><li>Only one line summary.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="summary-variations">Summary Variations<a class="hash-link" href="#summary-variations" title="Direct link to heading">​</a></h2><ul><li>Indicative summary: It looks like a summary of the book. This summary describes what kinds of the story, but not tell all of the stories especially its ends (so indicative summary has only partial information).</li><li>Informative summary: In contrast to the indicative summary, the informative summary includes full information of the document.</li><li>Keyword summary: Not the text, but the words or phrases from the input document.</li><li>Headline summary: Only one line summary.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="seq2seq">Seq2Seq<a class="hash-link" href="#seq2seq" title="Direct link to heading">​</a></h2><p><img alt="/img/content-concepts-raw-nlp-text-summarization-untitled-1.png" src="/assets/images/content-concepts-raw-nlp-text-summarization-untitled-1-78e03fba4d1e6dc753484cc3b2c2558b.png"></p><ul><li>Seq2seq - mainly bi-LSTM for encoder and attention mechanism for decoder network</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="pointer-generator">Pointer Generator<a class="hash-link" href="#pointer-generator" title="Direct link to heading">​</a></h2><p><img alt="/img/content-concepts-raw-nlp-text-summarization-image_(3).png" src="/assets/images/content-concepts-raw-nlp-text-summarization-image_(3)-acb7a882474e5ded1e7202a839f109aa.png"></p><p><img src="https://miro.medium.com/max/1400/0*vJgFcKRJpdN1sO2Z.png" alt="https://miro.medium.com/max/1400/0*vJgFcKRJpdN1sO2Z.png"></p><p>Researchers found 2 main problems with the seq2seq model, as discussed in this truly <a href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" target="_blank" rel="noopener noreferrer">amazing blog</a>, which is 1) the inability of the network to copy facts and 2) repetition of words. The pointer generator network (with coverage mechanism) tried to address these problems. </p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiments">Experiments<a class="hash-link" href="#experiments" title="Direct link to heading">​</a></h2><p><a href="https://github.com/theamrzaki/text_summurization_abstractive_methods" target="_blank" rel="noopener noreferrer">theamrzaki/text_summurization_abstractive_methods</a></p><p><a href="https://github.com/dongjun-Lee/text-summarization-tensorflow" target="_blank" rel="noopener noreferrer">dongjun-Lee/text-summarization-tensorflow</a></p><p><a href="https://github.com/nikhilcss97/Text-Summarization" target="_blank" rel="noopener noreferrer">nikhilcss97/Text-Summarization</a></p><p><a href="https://github.com/glopasso/text-summarization" target="_blank" rel="noopener noreferrer">glopasso/text-summarization</a></p><p><a href="https://www.dlology.com/blog/tutorial-summarizing-text-with-amazon-reviews/" target="_blank" rel="noopener noreferrer">How to Summarize Amazon Reviews with Tensorflow</a></p><p><a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener noreferrer">pytorch/fairseq</a></p><p><a href="https://github.com/tshi04/LeafNATS" target="_blank" rel="noopener noreferrer">tshi04/LeafNATS</a></p><ul><li><a href="https://colab.research.google.com/drive/1iAIFX1QQiFm1F01vMmnAgFh4oH1H-K8W" target="_blank" rel="noopener noreferrer">Text summarisation with BART &amp; T5 using HuggingFace</a></li><li><a href="https://colab.research.google.com/drive/1PlWjOQ9IV-MAtoZuAFfgccLKg_Uu-ufo#scrollTo=m85bgroRzV3_" target="_blank" rel="noopener noreferrer">TextRank, Summy and BERT Summarizer</a></li><li><a href="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/755dfc1a-4752-44a5-bc21-49aa5c80adb7/TED_Talk_Tag_Generator_and_Summarizer___by_Qi_Haodi___Jul_2020___Medium.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201014%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20201014T060921Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=82cab92ef127ef9a6122b243ef2f050e3db4282f524c9eaea9f8307af8699624&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=filename%20%3D%22TED%2520Talk%2520Tag%2520Generator%2520and%2520Summarizer%2520_%2520by%2520Qi%2520Haodi%2520_%2520Jul%252C%25202020%2520_%2520Medium.html%22" target="_blank" rel="noopener noreferrer">TED Talk Tag Generator and Summarizer</a></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="benchmark-datasets">Benchmark datasets<a class="hash-link" href="#benchmark-datasets" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="bigpatent">BigPatent<a class="hash-link" href="#bigpatent" title="Direct link to heading">​</a></h3><p><a href="https://arxiv.org/pdf/1906.03741.pdf" target="_blank" rel="noopener noreferrer"></a></p><p>Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries.</p><p>Compared to existing summarization datasets, BigPatent has the following properties:</p><ol><li>summaries contain richer discourse structure with more recurring entities,</li><li>salient content is evenly distributed in the input, and</li><li>lesser and shorter extractive fragments present in the summaries.</li></ol><p>We train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cnn-and-dailymail">CNN and DailyMail<a class="hash-link" href="#cnn-and-dailymail" title="Direct link to heading">​</a></h3><p><a href="https://cs.nyu.edu/~kcho/DMQA/" target="_blank" rel="noopener noreferrer">DMQA</a></p><p>CNN daily mail dataset consists of long news articles(an average of ~800 words). It consists of both articles and summaries of those articles. Some of the articles have multi line summaries also. We have used this dataset in our Pointer Generator model.</p><p>CNN dataset contains the documents from the news articles of CNN. There are approximately 90k documents/stories. DM dataset contains the documents from the news articles of Daily Mail. There are approximately 197k documents/stories. OTHR contains a few thousands stories scraped from 4 news websites.</p><ul><li>CNN/DM - the news body is used as the input for our model , while the header would be used as the summary target output.</li><li>Data can be acquired from this <a href="https://drive.google.com/drive/folders/1Izsbg_p1s52dFNh8NmSG5jmDtRgHcLUN" target="_blank" rel="noopener noreferrer">gdrive</a>.</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="amazon-fine-food-reviews">Amazon Fine Food Reviews<a class="hash-link" href="#amazon-fine-food-reviews" title="Direct link to heading">​</a></h3><p><a href="https://www.kaggle.com/snap/amazon-fine-food-reviews" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/snap/amazon-fine-food-reviews</a></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="duc-2014">DUC-2014<a class="hash-link" href="#duc-2014" title="Direct link to heading">​</a></h3><ul><li><a href="http://duc.nist.gov/data.html" target="_blank" rel="noopener noreferrer">DUC-2014 dataset</a> that involves generating approximately 14-word summaries for 500 news articles. The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes.</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="gigaword">Gigaword<a class="hash-link" href="#gigaword" title="Direct link to heading">​</a></h3><p><a href="https://catalog.ldc.upenn.edu/LDC2012T21" target="_blank" rel="noopener noreferrer">Annotated English Gigaword</a></p><p>It is popularly known as GIGAWORLD dataset and contains nearly ten million documents (over four billion words) of the original English Gigaword Fifth Edition. It consists of articles and their headlines. We have used this dataset to train our Abstractive summarization model.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cornell-newsroom">Cornell Newsroom<a class="hash-link" href="#cornell-newsroom" title="Direct link to heading">​</a></h3><p><a href="http://lil.nlp.cornell.edu/newsroom/" target="_blank" rel="noopener noreferrer">Cornell Newsroom Summarization Dataset</a></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="opinosis">Opinosis<a class="hash-link" href="#opinosis" title="Direct link to heading">​</a></h3><p><a href="http://kavita-ganesan.com/opinosis-opinion-dataset/#.Xs0trf8za01" target="_blank" rel="noopener noreferrer">Opinosis Dataset - Topic related review sentences | Kavita Ganesan</a></p><p>This dataset contains sentences extracted from user reviews on a given topic. Example topics are “performance of Toyota Camry” and “sound quality of ipod nano”, etc. The reviews were obtained from various sources — Tripadvisor (hotels), Edmunds.com (cars) and amazon.com (various electronics).Each article in the dataset has 5 manually written “gold” summaries. This dataset was used to score the results of the abstractive summarization model.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="evaluation-metrics">Evaluation metrics<a class="hash-link" href="#evaluation-metrics" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="rogue">ROGUE<a class="hash-link" href="#rogue" title="Direct link to heading">​</a></h3><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/was2004.pdf" target="_blank" rel="noopener noreferrer"></a></p><p><a href="https://arxiv.org/pdf/1803.01937v1.pdf" target="_blank" rel="noopener noreferrer"></a></p><p>Rouge-N is a word N-gram count that matche between the model and the gold summary. It is similart to the &quot;recall&quot; because it evaluates the covering rate of gold summary, and not consider the not included n-gram in it.</p><p>ROUGE-1 and ROUGE-2 is usually used. The ROUGE-1 means word base, so its order is not regarded. So &quot;apple pen&quot; and &quot;pen apple&quot; is same ROUGE-1 score. But if ROUGE-2, &quot;apple pen&quot; becomes single entity so &quot;apple pen&quot; and &quot;pen apple&quot; does not match. If you increase the ROUGE-&quot;N&quot; count, finally evaluates completely match or not.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="bleu">BLEU<a class="hash-link" href="#bleu" title="Direct link to heading">​</a></h3><p><a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener noreferrer"></a></p><p>BLEU is a modified form of &quot;precision&quot;, that used in machine translation evaluation usually. BLEU is basically calculated on the n-gram co-occerance between the generated summary and the gold (You don&#x27;t need to specify the &quot;n&quot; unlike ROUGE).</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="library">Library<a class="hash-link" href="#library" title="Direct link to heading">​</a></h2><p><a href="https://github.com/chakki-works/sumeval" target="_blank" rel="noopener noreferrer">chakki-works/sumeval</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="awesome-list">Awesome List<a class="hash-link" href="#awesome-list" title="Direct link to heading">​</a></h2><p><a href="https://github.com/mathsyouth/awesome-text-summarization" target="_blank" rel="noopener noreferrer">mathsyouth/awesome-text-summarization</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><p><a href="https://arxiv.org/abs/1812.02303v3" target="_blank" rel="noopener noreferrer">Neural Abstractive Text Summarization with Sequence-to-Sequence Models: A Survey</a></p><p><a href="https://arxiv.org/abs/1804.04589" target="_blank" rel="noopener noreferrer">A Survey on Neural Network-Based Summarization Methods</a></p><p><a href="https://paperswithcode.com/paper/a-neural-attention-model-for-abstractive" target="_blank" rel="noopener noreferrer">Papers with Code - A Neural Attention Model for Abstractive Sentence Summarization</a></p><p><a href="https://ai.googleblog.com/2016/08/text-summarization-with-tensorflow.html" target="_blank" rel="noopener noreferrer">Text summarization with TensorFlow</a></p><p><a href="https://paperswithcode.com/paper/get-to-the-point-summarization-with-pointer" target="_blank" rel="noopener noreferrer">Papers with Code - Get To The Point: Summarization with Pointer-Generator Networks</a></p><p><a href="https://arxiv.org/abs/1602.06023" target="_blank" rel="noopener noreferrer">Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</a></p><p><a href="https://arxiv.org/abs/1705.04304" target="_blank" rel="noopener noreferrer">A Deep Reinforced Model for Abstractive Summarization</a></p><p><a href="https://arxiv.org/abs/1803.11070v1" target="_blank" rel="noopener noreferrer">Actor-Critic based Training Framework for Abstractive Summarization</a></p><p><a href="https://paperswithcode.com/paper/unified-language-model-pre-training-for" target="_blank" rel="noopener noreferrer">Papers with Code - Unified Language Model Pre-training for Natural Language Understanding and Generation</a></p><p><a href="https://paperswithcode.com/paper/mass-masked-sequence-to-sequence-pre-training" target="_blank" rel="noopener noreferrer">Papers with Code - MASS: Masked Sequence to Sequence Pre-training for Language Generation</a></p><p><a href="https://paperswithcode.com/paper/deep-reinforcement-learning-for-sequence-to" target="_blank" rel="noopener noreferrer">Papers with Code - Deep Reinforcement Learning For Sequence to Sequence Models</a></p><p><a href="https://paperswithcode.com/paper/text-summarization-with-pretrained-encoders" target="_blank" rel="noopener noreferrer">Papers with Code - Text Summarization with Pretrained Encoders</a></p><p><a href="https://paperswithcode.com/paper/prophetnet-predicting-future-n-gram-for" target="_blank" rel="noopener noreferrer">Papers with Code - ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training</a></p><p><a href="https://arxiv.org/pdf/1911.02247v2.pdf" target="_blank" rel="noopener noreferrer"></a></p><ul><li><a href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" target="_blank" rel="noopener noreferrer">Taming Recurrent Neural Networks for Better Summarization</a> <!-- -->[⭐]</li><li><a href="https://github.com/mathsyouth/awesome-text-summarization" target="_blank" rel="noopener noreferrer">Awesome Text Summarization</a></li><li><a href="https://ai.googleblog.com/2016/08/text-summarization-with-tensorflow.html" target="_blank" rel="noopener noreferrer">Text summarization with TensorFlow, Google AI Blog 2016</a></li></ul><p><a href="https://github.com/sourcecode369/deep-natural-language-processing/blob/master/text%20summarization/summarization.md" target="_blank" rel="noopener noreferrer">sourcecode369/deep-natural-language-processing</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/recohut/docs/docs/docs/concept-extras/nlp/text-summarization.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/concept-extras/nlp/text-style-transfer"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Text Style Transfer</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/concept-extras/nlp/topic-modeling"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Topic Modeling</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#scheduled-sampling" class="table-of-contents__link toc-highlight">Scheduled sampling</a></li><li><a href="#models" class="table-of-contents__link toc-highlight">Models</a><ul><li><a href="#prophetnet" class="table-of-contents__link toc-highlight">ProphetNet</a></li><li><a href="#pegasus" class="table-of-contents__link toc-highlight">PEGASUS</a></li><li><a href="#bertsum" class="table-of-contents__link toc-highlight">BERTSum</a></li><li><a href="#seq2seq-pointergenerator" class="table-of-contents__link toc-highlight">Seq2Seq PointerGenerator</a></li></ul></li><li><a href="#process-flow" class="table-of-contents__link toc-highlight">Process flow</a></li><li><a href="#use-cases" class="table-of-contents__link toc-highlight">Use Cases</a><ul><li><a href="#enron-email-summarization" class="table-of-contents__link toc-highlight">Enron Email Summarization</a></li><li><a href="#pdf-summarization-over-mail" class="table-of-contents__link toc-highlight">PDF Summarization over mail</a></li><li><a href="#bart-text-summarization" class="table-of-contents__link toc-highlight">BART Text Summarization</a></li><li><a href="#transformers-summarization-experiment" class="table-of-contents__link toc-highlight">Transformers Summarization Experiment</a></li><li><a href="#cnn-dailymail-and-inshorts-news-summarization" class="table-of-contents__link toc-highlight">CNN-DailyMail and InShorts News Summarization</a></li><li><a href="#transformers-summarization-experiment-1" class="table-of-contents__link toc-highlight">Transformers Summarization Experiment</a></li><li><a href="#covid-19-article-summarization" class="table-of-contents__link toc-highlight">Covid-19 article summarization</a></li></ul></li><li><a href="#common-applications" class="table-of-contents__link toc-highlight">Common Applications</a></li><li><a href="#variations" class="table-of-contents__link toc-highlight">Variations</a></li><li><a href="#summary-variations" class="table-of-contents__link toc-highlight">Summary Variations</a></li><li><a href="#seq2seq" class="table-of-contents__link toc-highlight">Seq2Seq</a></li><li><a href="#pointer-generator" class="table-of-contents__link toc-highlight">Pointer Generator</a></li><li><a href="#experiments" class="table-of-contents__link toc-highlight">Experiments</a></li><li><a href="#benchmark-datasets" class="table-of-contents__link toc-highlight">Benchmark datasets</a><ul><li><a href="#bigpatent" class="table-of-contents__link toc-highlight">BigPatent</a></li><li><a href="#cnn-and-dailymail" class="table-of-contents__link toc-highlight">CNN and DailyMail</a></li><li><a href="#amazon-fine-food-reviews" class="table-of-contents__link toc-highlight">Amazon Fine Food Reviews</a></li><li><a href="#duc-2014" class="table-of-contents__link toc-highlight">DUC-2014</a></li><li><a href="#gigaword" class="table-of-contents__link toc-highlight">Gigaword</a></li><li><a href="#cornell-newsroom" class="table-of-contents__link toc-highlight">Cornell Newsroom</a></li><li><a href="#opinosis" class="table-of-contents__link toc-highlight">Opinosis</a></li></ul></li><li><a href="#evaluation-metrics" class="table-of-contents__link toc-highlight">Evaluation metrics</a><ul><li><a href="#rogue" class="table-of-contents__link toc-highlight">ROGUE</a></li><li><a href="#bleu" class="table-of-contents__link toc-highlight">BLEU</a></li></ul></li><li><a href="#library" class="table-of-contents__link toc-highlight">Library</a></li><li><a href="#awesome-list" class="table-of-contents__link toc-highlight">Awesome List</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Recohut Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.5f97d5c0.js"></script>
<script src="/assets/js/main.a34fef53.js"></script>
</body>
</html>