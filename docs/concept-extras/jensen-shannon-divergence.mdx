# Jensen–Shannon divergence

In [probability theory](https://en.wikipedia.org/wiki/Probability_theory) and [statistics](https://en.wikipedia.org/wiki/Statistics), the **[Jensen](https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician))–[Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) divergence** is a method of measuring the similarity between two [probability distributions](https://en.wikipedia.org/wiki/Probability_distribution). It is also known as **information radius** (**IRad**)[[1]](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence#cite_note-1) or **total divergence to the average**.[[2]](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence#cite_note-2) It is based on the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen–Shannon divergence is a [metric](https://en.wikipedia.org/wiki/Metric_(mathematics)) often referred to as Jensen-Shannon distance.